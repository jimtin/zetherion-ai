# Discord Bot Token - Required
# Get from: https://discord.com/developers/applications
DISCORD_TOKEN=

# Gemini API Key - Required for embeddings
# Get from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=

# Anthropic API Key - Optional, for Claude as LLM
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI API Key - Optional, alternative LLM
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Discord User IDs allowed to interact (comma-separated)
# Leave empty to allow all users (not recommended for production)
# Example: ALLOWED_USER_IDS=123456789,987654321
# To get your Discord User ID: Enable Developer Mode in Discord settings,
# right-click your username, and select "Copy User ID"
ALLOWED_USER_IDS=

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Qdrant Configuration
# For local development with Docker: use "localhost"
# For Docker Compose: use "qdrant" (service name)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# ======================================
# MODEL VERSIONS (Updated: 2026-02-05)
# ======================================
# Run 'python scripts/check-model-versions.py' to see latest available models

# Claude Model (for complex tasks)
# Options: claude-sonnet-4-5-20250929 (default), claude-opus-4-5-20251101 (best quality), claude-haiku-4-20250514 (fastest)
# Latest: https://docs.anthropic.com/en/docs/about-claude/models
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# OpenAI Model (alternative to Claude)
# Options: gpt-5.2 (default), gpt-5.2-2026-01-15 (pinned), gpt-4o (fallback)
# Latest: https://platform.openai.com/docs/models
OPENAI_MODEL=gpt-5.2

# Gemini Router Model (for message classification and simple queries)
# Options: gemini-2.5-flash (default, stable), gemini-3-flash-preview (latest preview)
# NOTE: Gemini 2.0 Flash was deprecated and shuts down March 31, 2026
# Latest: https://ai.google.dev/gemini-api/docs/models
ROUTER_MODEL=gemini-2.5-flash

# Gemini Embedding Model
# Options: text-embedding-004 (default)
EMBEDDING_MODEL=text-embedding-004

# ======================================
# ROUTER BACKEND (Updated: 2026-02-05)
# ======================================
# Router Backend: 'gemini' (default) or 'ollama'
# - gemini: Uses Google's cloud API (free tier, fast)
# - ollama: Uses local Ollama container (private, self-hosted)
ROUTER_BACKEND=gemini

# ======================================
# OLLAMA CONFIGURATION (Dual Container Architecture)
# ======================================
# Uses TWO Ollama containers:
# 1. ollama-router: Small, fast model for routing (always loaded)
# 2. ollama: Larger model for generation + embeddings (always loaded)
# This avoids model-swapping delays between routing and generation.

# --- Router Container (ollama-router) ---
# Dedicated container for fast message classification
# For Docker Compose: use "ollama-router" (service name)
# For local development: use "localhost" with port 11435
OLLAMA_ROUTER_HOST=ollama-router
OLLAMA_ROUTER_PORT=11434

# Router model: Small, fast model for classification
# Recommended: llama3.2:3b (default), phi3:mini (alternative)
OLLAMA_ROUTER_MODEL=llama3.2:3b

# --- Generation Container (ollama) ---
# Main container for complex queries and embeddings
# For Docker Compose: use "ollama" (service name)
# For local development: use "localhost" with port 11434
OLLAMA_HOST=ollama
OLLAMA_PORT=11434

# Generation model: Larger, more capable model
# Recommended: llama3.1:8b (default), llama3.1:70b (best quality)
# Browse: https://ollama.com/library
# Run 'python scripts/assess-system.py' for hardware-specific recommendations
OLLAMA_GENERATION_MODEL=llama3.1:8b

# Ollama Docker Memory Allocation (GB)
# Automatically set by assess-system.py based on selected model
# Router container: 1GB (small model)
# Generation container: 8GB (large model + embeddings)
OLLAMA_DOCKER_MEMORY=8

# Ollama API timeout (seconds)
OLLAMA_TIMEOUT=30

# Ollama Embedding Model (for local embeddings)
# Used when EMBEDDINGS_BACKEND=ollama
# Runs on generation container (ollama)
# nomic-embed-text produces 768-dimensional vectors (same as Gemini)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ======================================
# EMBEDDINGS CONFIGURATION
# ======================================
# Embeddings Backend: 'ollama' (default, local) or 'gemini' (cloud)
# - ollama: Uses local Ollama container (private, no data leaves your machine)
# - gemini: Uses Google's cloud API (requires GEMINI_API_KEY)
# For privacy-first operation, use 'ollama' (default)
EMBEDDINGS_BACKEND=ollama

# ======================================
# LOGGING CONFIGURATION
# ======================================
# Enable file-based logging (in addition to console)
LOG_TO_FILE=true

# Log directory (relative or absolute path)
# In Docker: /app/logs (mounted to ./logs on host)
LOG_DIRECTORY=logs

# Log file rotation settings
LOG_FILE_MAX_BYTES=52428800  # 50MB per file
LOG_FILE_BACKUP_COUNT=10      # Keep 10 rotated files (500MB total max)

# ======================================
# ENCRYPTION (Phase 5A) - ENABLED BY DEFAULT
# ======================================
# Enable field-level encryption for sensitive data in Qdrant
# When enabled, message and memory content is encrypted at rest
ENCRYPTION_ENABLED=true

# Master passphrase for encryption key derivation (min 16 characters)
# Generate a strong passphrase: openssl rand -base64 32
# ⚠️  WARNING: Keep this secret! Lost passphrase = unrecoverable data
ENCRYPTION_PASSPHRASE=

# Path to store the encryption salt file (created automatically)
ENCRYPTION_SALT_PATH=data/salt.bin

# ======================================
# QDRANT TLS (Phase 5A) - OPTIONAL
# ======================================
# Enable TLS for Qdrant connection (in-transit encryption)
# Requires running: scripts/generate-qdrant-certs.sh first
QDRANT_USE_TLS=false

# Path to Qdrant TLS certificate (for verification, optional)
# Leave empty to skip verification (acceptable for internal Docker network)
QDRANT_CERT_PATH=

# ======================================
# DISCORD E2E TESTING (OPTIONAL)
# ======================================
# Only needed for running Discord end-to-end integration tests
# These tests send real messages through Discord API

# Test bot token (separate from main DISCORD_TOKEN)
# Create a separate bot in Discord Developer Portal for testing
TEST_DISCORD_BOT_TOKEN=

# Test channel ID where tests will run
# Create a dedicated test server/channel for E2E tests
# Get channel ID: Enable Developer Mode, right-click channel → "Copy Channel ID"
TEST_DISCORD_CHANNEL_ID=

# (Optional) Explicit bot ID to test against
# If not set, tests will auto-detect bot by looking for "zetherion" in bot names
# Get bot ID: Enable Developer Mode, right-click bot → "Copy User ID"
TEST_DISCORD_TARGET_BOT_ID=

# Allow messages from other bots (required for Discord E2E tests)
# Set to 'true' to allow the test bot to send messages to the production bot
# ⚠️  WARNING: Keep this 'false' in production to prevent bot-to-bot spam
ALLOW_BOT_MESSAGES=false

# ======================================
# PROFILE SYSTEM (Phase 5C)
# ======================================
# Enable profile extraction from conversations
PROFILE_INFERENCE_ENABLED=true

# Only use Tier 1 (free regex-based) inference
# Set to 'true' to disable Ollama-based profile extraction (saves resources)
PROFILE_TIER1_ONLY=false

# Minimum confidence to auto-apply profile updates (0.0-1.0)
PROFILE_CONFIDENCE_THRESHOLD=0.6

# Profile cache TTL in seconds (default 5 minutes)
PROFILE_CACHE_TTL=300

# Path to SQLite database for profile operational data
PROFILE_DB_PATH=data/profiles.db

# Maximum pending confirmations per user
PROFILE_MAX_PENDING_CONFIRMATIONS=5

# Hours before pending confirmations expire (auto-decline)
PROFILE_CONFIRMATION_EXPIRY_HOURS=72

# ======================================
# EMPLOYMENT PROFILE DEFAULTS (Phase 5C.1)
# ======================================
# Initial communication style settings (0.0-1.0)
DEFAULT_FORMALITY=0.5   # 0=casual, 1=formal
DEFAULT_VERBOSITY=0.5   # 0=terse, 1=detailed
DEFAULT_PROACTIVITY=0.3 # 0=reactive, 1=proactive

# How fast trust builds per positive interaction
TRUST_EVOLUTION_RATE=0.05

# ======================================
# PUBLIC API (Phase 11 - Multi-Tenant)
# ======================================
# JWT secret for signing session tokens (required for public API)
# Generate: openssl rand -base64 32
API_JWT_SECRET=

# Public API bind host and port
API_HOST=0.0.0.0
API_PORT=8443

# ======================================
# CLOUDFLARE TUNNEL (Phase 11)
# ======================================
# Cloudflare Tunnel token for exposing the public API without opening ports
# Create at: https://one.dash.cloudflare.com → Zero Trust → Networks → Tunnels
# Configure the tunnel to route traffic to http://zetherion-ai-api:8443
CLOUDFLARE_TUNNEL_TOKEN=
