# Discord Bot Token - Required
# Get from: https://discord.com/developers/applications
DISCORD_TOKEN=

# Gemini API Key - Required for embeddings
# Get from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=

# Anthropic API Key - Optional, for Claude as LLM
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# OpenAI API Key - Optional, alternative LLM
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Discord User IDs allowed to interact (comma-separated)
# Leave empty to allow all users (not recommended for production)
# Example: ALLOWED_USER_IDS=123456789,987654321
# To get your Discord User ID: Enable Developer Mode in Discord settings,
# right-click your username, and select "Copy User ID"
ALLOWED_USER_IDS=

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Qdrant Configuration
# For local development with Docker: use "localhost"
# For Docker Compose: use "qdrant" (service name)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# ======================================
# MODEL VERSIONS (Updated: 2026-02-05)
# ======================================
# Run 'python scripts/check-model-versions.py' to see latest available models

# Claude Model (for complex tasks)
# Options: claude-sonnet-4-5-20250929 (default), claude-opus-4-5-20251101 (best quality), claude-haiku-4-20250514 (fastest)
# Latest: https://docs.anthropic.com/en/docs/about-claude/models
CLAUDE_MODEL=claude-sonnet-4-5-20250929

# OpenAI Model (alternative to Claude)
# Options: gpt-4o (default), gpt-4o-2024-11-20 (stable), gpt-4-turbo-2024-04-09
# Latest: https://platform.openai.com/docs/models
OPENAI_MODEL=gpt-4o

# Gemini Router Model (for message classification and simple queries)
# Options: gemini-2.5-flash (default, stable), gemini-3-flash-preview (latest preview)
# NOTE: Gemini 2.0 Flash was deprecated and shuts down March 31, 2026
# Latest: https://ai.google.dev/gemini-api/docs/models
ROUTER_MODEL=gemini-2.5-flash

# Gemini Embedding Model
# Options: text-embedding-004 (default)
EMBEDDING_MODEL=text-embedding-004

# ======================================
# ROUTER BACKEND (Updated: 2026-02-05)
# ======================================
# Router Backend: 'gemini' (default) or 'ollama'
# - gemini: Uses Google's cloud API (free tier, fast)
# - ollama: Uses local Ollama container (private, self-hosted)
ROUTER_BACKEND=gemini

# ======================================
# OLLAMA CONFIGURATION
# ======================================
# Only needed if ROUTER_BACKEND=ollama
# For Docker Compose: use "ollama" (service name)
# For local development with ./start.sh: use "localhost" (automatically set by start.sh)
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Ollama Model Selection
# Recommended: llama3.1:8b (best balance), mistral:7b (fastest), qwen2.5:7b (best quality)
# Browse: https://ollama.com/library
# Run 'python scripts/assess-system.py' for hardware-specific recommendations
OLLAMA_ROUTER_MODEL=llama3.1:8b

# Ollama Docker Memory Allocation (GB)
# Automatically set by assess-system.py based on selected model
# Default: 8GB for most 7B-8B models
OLLAMA_DOCKER_MEMORY=8

# Ollama API timeout (seconds)
OLLAMA_TIMEOUT=30

# ======================================
# LOGGING CONFIGURATION
# ======================================
# Enable file-based logging (in addition to console)
LOG_TO_FILE=true

# Log directory (relative or absolute path)
# In Docker: /app/logs (mounted to ./logs on host)
LOG_DIRECTORY=logs

# Log file rotation settings
LOG_FILE_MAX_BYTES=10485760  # 10MB per file
LOG_FILE_BACKUP_COUNT=5       # Keep 5 rotated files (60MB total max)
