{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Zetherion AI","text":"<p>Secure personal AI assistant with encrypted memory, multi-provider LLM routing, and privacy-first design.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Encrypted Memory - AES-256-GCM encryption for all stored data with PBKDF2 key derivation</li> <li>Multi-Provider Routing - Intelligent routing across Claude, OpenAI, Gemini, and Ollama</li> <li>Vector Memory - Long-term context using Qdrant with semantic search</li> <li>Security-First - Rate limiting, prompt injection detection, secrets management</li> <li>Self-Hosted - Run entirely on your own infrastructure with Ollama</li> <li>Cost Tracking - Monitor and budget API spending with alerts</li> <li>User Profiles - Learn preferences and adapt responses</li> <li>Skills Framework - Extensible task management, calendar, and more</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/jimtin/zetherion-ai.git\ncd zetherion-ai\n\n# One-command deployment (interactive setup)\n./start.sh  # or start.ps1 on Windows\n</code></pre> <p>See the Installation Guide for detailed platform-specific instructions.</p>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"Section Description Installation Platform-specific setup guide Configuration All environment variables Hardware Recommendations Optimize for your system Windows Deployment Windows-specific guide"},{"location":"#user-guides","title":"User Guides","text":"Section Description Commands Discord slash commands reference FAQ Frequently asked questions Troubleshooting Common issues and solutions"},{"location":"#advanced-features","title":"Advanced Features","text":"Section Description Features Overview Phase 5+ features guide Skills Framework Task management and extensibility Cost Tracking Budget management and optimization Profile System User preference learning"},{"location":"#architecture-security","title":"Architecture &amp; Security","text":"Section Description Architecture System design and components Docker Architecture Container setup and networking Security Security controls and best practices"},{"location":"#development","title":"Development","text":"Section Description Testing Test suite and coverage Testing &amp; Deployment Comprehensive deployment validation CI/CD Continuous integration pipeline GitHub Secrets CI/CD secrets configuration Changelog Version history Contributing Contribution guidelines Development Developer documentation"},{"location":"#project-status","title":"Project Status","text":"<ul> <li>Test Coverage: 78% (885 unit + 14 integration + 4 E2E tests)</li> <li>Current Version: 3.0.0 (Fully Automated Docker Deployment)</li> <li>Features: Phase 5+ complete (encryption, cost tracking, profiles, skills)</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Report Issues</li> <li>Wiki</li> <li>Discussions</li> </ul>"},{"location":"ARCHITECTURE/","title":"Zetherion AI Architecture","text":"<p>This document provides a comprehensive overview of Zetherion AI's system architecture, design decisions, and key components.</p>"},{"location":"ARCHITECTURE/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>High-Level Architecture</li> <li>Core Components</li> <li>Data Flow</li> <li>Design Patterns</li> <li>Technology Stack</li> <li>Scalability &amp; Performance</li> <li>Security Architecture</li> <li>Future Roadmap</li> </ul>"},{"location":"ARCHITECTURE/#overview","title":"Overview","text":"<p>Zetherion AI is a Discord bot with advanced AI capabilities, featuring: - Dual LLM backends for intelligent routing (Gemini + Ollama for routing, Claude/OpenAI for complex tasks) - Vector memory for long-term context using Qdrant - Comprehensive security with rate limiting, allowlists, and prompt injection detection - Full Docker containerization for reproducible deployment</p>"},{"location":"ARCHITECTURE/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Modularity - Clean separation of concerns (Discord, Agent, Memory, Security)</li> <li>Extensibility - Pluggable backends via factory pattern</li> <li>Resilience - Retry logic, fallbacks, graceful degradation</li> <li>Security-First - Defense in depth, least privilege, secrets management</li> <li>Performance - Async-first, parallel operations, efficient caching</li> <li>Maintainability - 87.58% test coverage, type hints, comprehensive logging</li> </ol>"},{"location":"ARCHITECTURE/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         User Interface                          \u2502\n\u2502                      Discord (discord.py)                       \u2502\n\u2502  Commands: /channels, /remember, /summarize                    \u2502\n\u2502  Interactions: DMs, @mentions, slash commands                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Security Layer                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Allowlist   \u2502  \u2502 Rate Limiter \u2502  \u2502 Injection Detect  \u2502    \u2502\n\u2502  \u2502  User IDs    \u2502  \u2502 10 msg/60s   \u2502  \u2502 17 Regex Patterns \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Agent Core                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Message Router (Factory Pattern)                       \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502   \u2502\n\u2502  \u2502  \u2502 Gemini Backend   \u2502  \u2502 Ollama Backend       \u2502        \u2502   \u2502\n\u2502  \u2502  \u2502 - Cloud API      \u2502  \u2502 - Local Container    \u2502        \u2502   \u2502\n\u2502  \u2502  \u2502 - Fast, reliable \u2502  \u2502 - Privacy-focused    \u2502        \u2502   \u2502\n\u2502  \u2502  \u2502 - Default        \u2502  \u2502 - Cost-effective     \u2502        \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502   \u2502\n\u2502  \u2502  Output: {intent: \"simple_query\" | \"complex_task\"}     \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Response Generation (Dual Generators)                  \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u2502\n\u2502  \u2502  \u2502 Complex Task Handler \u2502  \u2502 Simple Query Handler \u2502    \u2502   \u2502\n\u2502  \u2502  \u2502 - Claude Sonnet 4.5  \u2502  \u2502 - Gemini 2.5 Flash   \u2502    \u2502   \u2502\n\u2502  \u2502  \u2502 - OpenAI GPT-4o      \u2502  \u2502 - Ollama Llama 3.1   \u2502    \u2502   \u2502\n\u2502  \u2502  \u2502 - Code, reasoning    \u2502  \u2502 - Facts, greetings   \u2502    \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Memory Manager                                         \u2502   \u2502\n\u2502  \u2502  - Context building from vector search                 \u2502   \u2502\n\u2502  \u2502  - Deduplication (search once, use twice)              \u2502   \u2502\n\u2502  \u2502  - Retry logic with exponential backoff                \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Memory Layer                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Qdrant Vector Database   \u2502  \u2502  Embeddings Service       \u2502  \u2502\n\u2502  \u2502  - AsyncQdrantClient      \u2502  \u2502  - Gemini text-embed-004  \u2502  \u2502\n\u2502  \u2502  - Collections per user   \u2502  \u2502  - 768-dim vectors        \u2502  \u2502\n\u2502  \u2502  - Semantic search        \u2502  \u2502  - Parallel batching      \u2502  \u2502\n\u2502  \u2502  - Docker container       \u2502  \u2502  - Caching (TODO)         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Infrastructure                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   Docker    \u2502  \u2502  Logging    \u2502  \u2502  Configuration      \u2502    \u2502\n\u2502  \u2502   Compose   \u2502  \u2502  Structlog  \u2502  \u2502  Pydantic Settings  \u2502    \u2502\n\u2502  \u2502   - qdrant  \u2502  \u2502  - Console  \u2502  \u2502  - SecretStr        \u2502    \u2502\n\u2502  \u2502   - ollama  \u2502  \u2502  - Files    \u2502  \u2502  - .env validation  \u2502    \u2502\n\u2502  \u2502   - bot     \u2502  \u2502  - JSON     \u2502  \u2502                     \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#core-components","title":"Core Components","text":""},{"location":"ARCHITECTURE/#1-discord-interface-srczetherion_aidiscord","title":"1. Discord Interface (<code>src/zetherion_ai/discord/</code>)","text":"<p>Purpose: Handle all Discord interactions and command routing.</p> <p>Key Files: - <code>bot.py</code> - Main bot class, event handlers - <code>commands.py</code> - Slash command definitions - <code>security.py</code> - Security controls (allowlist, rate limiting, injection detection)</p> <p>Features: - Slash commands: <code>/channels</code>, <code>/remember</code>, <code>/summarize</code> - DM and @mention support - Message splitting for 2000-char limit - Typing indicators for better UX - Error handling and user feedback</p> <p>Design Decisions: - Used <code>discord.py</code> for stable API and strong typing - Commands implemented as app_commands for modern Discord UI - Security checks run before any agent processing - Async throughout for non-blocking I/O</p>"},{"location":"ARCHITECTURE/#2-agent-core-srczetherion_aiagent","title":"2. Agent Core (<code>src/zetherion_ai/agent/</code>)","text":"<p>Purpose: Intelligent message routing and response generation.</p> <p>Key Files: - <code>core.py</code> - Agent class (orchestrates routing and generation) - <code>router.py</code> - Gemini router backend - <code>router_ollama.py</code> - Ollama router backend - <code>router_factory.py</code> - Backend selection factory - <code>router_base.py</code> - Abstract router interface (Protocol)</p> <p>Message Flow: 1. Router classifies message intent (simple vs complex) 2. Agent searches memory for relevant context 3. Appropriate generator creates response 4. Response returned to Discord</p> <p>Routing Decision Logic: <pre><code>{\n  \"intent\": \"simple_query\",  # or \"complex_task\"\n  \"confidence\": 0.95,\n  \"use_claude\": false  # true for complex tasks\n}\n</code></pre></p> <p>Design Decisions: - Factory pattern allows runtime backend selection - Dual generators optimize cost and latency - Retry logic with exponential backoff (max 3 retries) - Context deduplication (search once, use for both generators)</p>"},{"location":"ARCHITECTURE/#3-memory-system-srczetherion_aimemory","title":"3. Memory System (<code>src/zetherion_ai/memory/</code>)","text":"<p>Purpose: Long-term semantic memory via vector embeddings.</p> <p>Key Files: - <code>qdrant.py</code> - Vector database client - <code>embeddings.py</code> - Gemini embedding generation</p> <p>Features: - Per-user collections in Qdrant - Semantic search for context retrieval - Parallel batch embeddings (10x faster) - Async operations throughout</p> <p>Vector Storage: - Model: <code>text-embedding-004</code> (Gemini) - Dimensions: 768 - Similarity: Cosine similarity - Storage: Qdrant Docker container</p> <p>Design Decisions: - AsyncQdrantClient prevents event loop blocking - Parallel embeddings via <code>asyncio.gather()</code> - Collections auto-created on first use - No embedding caching (TODO for Phase 5)</p>"},{"location":"ARCHITECTURE/#4-configuration-srczetherion_aiconfigpy","title":"4. Configuration (<code>src/zetherion_ai/config.py</code>)","text":"<p>Purpose: Centralized settings with validation and secrets management.</p> <p>Features: - Pydantic Settings for type-safe configuration - <code>SecretStr</code> for all credentials (never logged) - Environment variable loading from <code>.env</code> - Field validators for critical settings - Computed properties for derived values</p> <p>Configuration Sources: 1. Environment variables 2. <code>.env</code> file (development) 3. Default values (fallback)</p> <p>Design Decisions: - <code>SecretStr</code> ensures credentials never leak to logs - LRU cache prevents repeated file reads - Validators catch misconfigurations early - Clear separation of dev vs prod settings</p>"},{"location":"ARCHITECTURE/#5-logging-srczetherion_ailoggingpy","title":"5. Logging (<code>src/zetherion_ai/logging.py</code>)","text":"<p>Purpose: Structured logging for debugging and monitoring.</p> <p>Features: - Dual handlers (console + rotating files) - Structured logs with <code>structlog</code> - JSON format for files (parseable with <code>jq</code>) - Colored console output in development - Log rotation (10MB \u00d7 6 files)</p> <p>Log Levels: - DEBUG: Detailed diagnostics (development only) - INFO: Normal operations - WARNING: Potential issues - ERROR: Errors that don't crash the bot - CRITICAL: System failures</p> <p>Design Decisions: - Structlog for structured, performant logging - Separate formatters for console vs files - Reduced third-party noise (discord.py, httpx) - Rotation prevents disk filling</p>"},{"location":"ARCHITECTURE/#data-flow","title":"Data Flow","text":""},{"location":"ARCHITECTURE/#example-user-sends-what-is-async-programming","title":"Example: User Sends \"What is async programming?\"","text":"<pre><code>1. Discord Event\n   \u251c\u2500 User sends message in channel\n   \u2514\u2500 on_message() handler triggered\n\n2. Security Checks\n   \u251c\u2500 Allowlist: Is user authorized?\n   \u251c\u2500 Rate Limit: Within 10 msg/60s?\n   \u2514\u2500 Injection: Contains malicious patterns?\n\n3. Message Routing\n   \u251c\u2500 Router Backend (Gemini or Ollama) classifies\n   \u2514\u2500 Result: {intent: \"complex_task\", confidence: 0.92, use_claude: true}\n\n4. Memory Search (Parallel)\n   \u251c\u2500 Generate embedding: embed_text(\"What is async programming?\")\n   \u251c\u2500 Search Qdrant: top_k=5 similar memories\n   \u2514\u2500 Return context: [\"Previous async discussion...\", \"User prefers Python...\"]\n\n5. Response Generation (Claude)\n   \u251c\u2500 Build prompt: message + context\n   \u251c\u2500 Call Claude API: claude-sonnet-4-5-20250929\n   \u2514\u2500 Get response: \"Async programming is...\"\n\n6. Response Delivery\n   \u251c\u2500 Split if &gt; 2000 chars\n   \u251c\u2500 Send to Discord channel\n   \u2514\u2500 Store interaction in memory\n\n7. Memory Storage\n   \u251c\u2500 Embed user message + bot response\n   \u251c\u2500 Store in Qdrant collection (user-specific)\n   \u2514\u2500 Ready for future context retrieval\n</code></pre>"},{"location":"ARCHITECTURE/#design-patterns","title":"Design Patterns","text":""},{"location":"ARCHITECTURE/#1-factory-pattern-router-backend-selection","title":"1. Factory Pattern (Router Backend Selection)","text":"<p>Problem: Need to support multiple routing backends (Gemini, Ollama) without tight coupling.</p> <p>Solution: Factory function creates appropriate backend at runtime.</p> <pre><code>def create_router() -&gt; MessageRouter:\n    settings = get_settings()\n\n    if settings.router_backend == \"ollama\":\n        try:\n            backend = OllamaRouterBackend()\n            if await backend.health_check():\n                return MessageRouter(backend)\n        except Exception:\n            log.warning(\"Ollama failed, falling back to Gemini\")\n\n    # Default to Gemini\n    return MessageRouter(GeminiRouterBackend())\n</code></pre> <p>Benefits: - Easy to add new backends - Runtime configuration - Graceful fallbacks</p>"},{"location":"ARCHITECTURE/#2-strategy-pattern-llm-backends","title":"2. Strategy Pattern (LLM Backends)","text":"<p>Problem: Different LLMs excel at different tasks (Claude for code, Gemini for speed).</p> <p>Solution: Agent selects generator based on router decision.</p> <pre><code>if routing.use_claude:\n    response = await self._generate_claude(message, context)\nelse:\n    response = await self._generate_gemini(message, context)\n</code></pre> <p>Benefits: - Cost optimization - Performance tuning - Easy to swap implementations</p>"},{"location":"ARCHITECTURE/#3-repository-pattern-memory-abstraction","title":"3. Repository Pattern (Memory Abstraction)","text":"<p>Problem: Need to abstract vector database operations from business logic.</p> <p>Solution: <code>QdrantMemory</code> class provides high-level memory interface.</p> <pre><code>class QdrantMemory:\n    async def store(self, text: str, metadata: dict) -&gt; None:\n        \"\"\"Store text with metadata in vector DB.\"\"\"\n\n    async def search(self, query: str, top_k: int = 5) -&gt; list[str]:\n        \"\"\"Search for similar memories.\"\"\"\n</code></pre> <p>Benefits: - Easy to swap Qdrant for another vector DB - Business logic decoupled from storage - Testable with mocks</p>"},{"location":"ARCHITECTURE/#4-singleton-pattern-configuration","title":"4. Singleton Pattern (Configuration)","text":"<p>Problem: Settings should be loaded once and reused.</p> <p>Solution: LRU cache ensures single settings instance.</p> <pre><code>@lru_cache\ndef get_settings() -&gt; Settings:\n    return Settings()\n</code></pre> <p>Benefits: - Fast lookups (no repeated .env parsing) - Consistent configuration throughout app - Memory efficient</p>"},{"location":"ARCHITECTURE/#5-retry-pattern-api-resilience","title":"5. Retry Pattern (API Resilience)","text":"<p>Problem: Cloud APIs have transient failures (rate limits, timeouts).</p> <p>Solution: Exponential backoff retry logic.</p> <pre><code>retries = 0\nwhile retries &lt; 3:\n    try:\n        return await api_call()\n    except (ConnectionError, TimeoutError, RateLimitError):\n        await asyncio.sleep(2 ** retries)\n        retries += 1\nraise MaxRetriesExceeded()\n</code></pre> <p>Benefits: - Handles transient failures - Prevents retry storms - User-friendly (no immediate errors)</p>"},{"location":"ARCHITECTURE/#technology-stack","title":"Technology Stack","text":""},{"location":"ARCHITECTURE/#core-technologies","title":"Core Technologies","text":"Category Technology Version Purpose Language Python 3.12+ Modern async features, type hints Discord Library discord.py 2.4.0+ Discord bot API Vector DB Qdrant Latest Semantic memory storage Embeddings Gemini text-embedding-004 768-dim vectors LLMs Claude Sonnet 4.5 Complex reasoning, code OpenAI GPT-4o Alternative complex tasks Gemini 2.5 Flash Simple queries, routing Ollama Llama 3.1:8b Local routing (optional) Containerization Docker Latest Reproducible deployment Logging structlog Latest Structured, performant logs Config Pydantic 2.0+ Type-safe settings"},{"location":"ARCHITECTURE/#development-tools","title":"Development Tools","text":"Tool Purpose Ruff Linting, formatting (600+ rules) Mypy Type checking (strict mode) Pytest Testing framework Pre-commit Git hooks for quality checks Gitleaks Secret scanning Bandit Security scanning Hadolint Dockerfile linting GitHub Actions CI/CD pipeline Dependabot Dependency updates CodeQL Static analysis"},{"location":"ARCHITECTURE/#api-integrations","title":"API Integrations","text":"Provider API Purpose Anthropic Claude API Complex task generation OpenAI Chat Completions Alternative complex tasks Google Gemini API Routing, simple queries, embeddings Ollama HTTP API Local routing (optional) Discord Gateway &amp; REST Bot interactions"},{"location":"ARCHITECTURE/#scalability-performance","title":"Scalability &amp; Performance","text":""},{"location":"ARCHITECTURE/#current-performance-characteristics","title":"Current Performance Characteristics","text":"Operation Latency Notes Router Classification 200-500ms Gemini Flash (fast) Simple Response 500-1000ms Gemini Flash Complex Response 2-5s Claude Sonnet Memory Search 100-200ms Qdrant local Embedding Generation 200-400ms Parallel batching Discord Message Send 100-300ms Discord API"},{"location":"ARCHITECTURE/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Parallel Operations</li> <li>Embeddings: <code>asyncio.gather()</code> for batch processing</li> <li>Memory searches: Single search reused for both generators</li> <li> <p>Independent API calls: Concurrent execution</p> </li> <li> <p>Caching (TODO for Phase 5)</p> </li> <li>Embedding cache for repeated queries</li> <li>Response cache for identical messages</li> <li> <p>Memory cache for frequently accessed context</p> </li> <li> <p>Async-First Architecture</p> </li> <li>AsyncQdrantClient (non-blocking vector DB)</li> <li>Async Discord handlers</li> <li>Async LLM API calls</li> <li> <p>No blocking I/O in event loop</p> </li> <li> <p>Resource Limits</p> </li> <li>Rate limiting (10 msg/60s per user)</li> <li>Memory search top_k=5 (limits context size)</li> <li>Message splitting (prevents Discord rate limits)</li> </ol>"},{"location":"ARCHITECTURE/#scalability-considerations","title":"Scalability Considerations","text":"<p>Current Scale: - Single Discord server - ~10-50 concurrent users - ~100-500 messages/hour - Docker Compose on single host</p> <p>Future Scale (Phase 5+): - Multiple Discord servers - 100-1000 concurrent users - Kubernetes deployment - Distributed Qdrant cluster - Horizontal scaling of bot instances - Load balancing across LLM providers</p>"},{"location":"ARCHITECTURE/#security-architecture","title":"Security Architecture","text":"<p>See docs/SECURITY.md for comprehensive security documentation.</p>"},{"location":"ARCHITECTURE/#defense-in-depth","title":"Defense in Depth","text":"<pre><code>Layer 1: Network (Docker bridge network, no exposed ports)\n  \u2193\nLayer 2: Input Validation (Allowlist, rate limiting)\n  \u2193\nLayer 3: Content Filtering (Prompt injection detection)\n  \u2193\nLayer 4: Secrets Management (Pydantic SecretStr, env vars)\n  \u2193\nLayer 5: Monitoring (Structured logs, error tracking)\n</code></pre>"},{"location":"ARCHITECTURE/#key-security-controls","title":"Key Security Controls","text":"<ol> <li>User Allowlist - Only authorized Discord user IDs can interact</li> <li>Rate Limiting - 10 messages per 60 seconds per user</li> <li>Prompt Injection Detection - 17 regex patterns + Unicode obfuscation</li> <li>Secrets Management - SecretStr, never logged, .env gitignored</li> <li>Secret Scanning - Gitleaks pre-commit hook</li> <li>Dependency Scanning - Dependabot weekly updates</li> <li>Static Analysis - CodeQL weekly scans</li> <li>Container Security - Slim base image, health checks</li> </ol>"},{"location":"ARCHITECTURE/#security-gap-analysis","title":"Security Gap Analysis","text":"<p>See docs/SECURITY.md#12-gap-analysis for: - Container image scanning (Trivy) - SBOM generation - Signed commits - Read-only container filesystem - And 8 more recommendations</p>"},{"location":"ARCHITECTURE/#future-roadmap","title":"Future Roadmap","text":""},{"location":"ARCHITECTURE/#phase-5-advanced-features-planned","title":"Phase 5: Advanced Features (Planned)","text":"<p>See <code>memory/phase5-plan.md</code> for detailed plan.</p> <p>5A: Encrypted Memory - AES-256-GCM encryption for sensitive data - PyCA cryptography library - Encrypted fields in Qdrant metadata</p> <p>5B: User Profiling - Preferences tracking (language, timezone, interests) - Automatic profile updates from conversations - Profile-aware context building</p> <p>5C: Skills Framework - Pluggable command system - Separate Docker container for skills - Python sandbox for user-contributed skills</p> <p>5D: Smart Multi-Provider Routing - Provider capability matrix (Claude for code, OpenAI for reasoning, Gemini for docs) - Dynamic routing based on message analysis - Cost-aware provider selection</p> <p>5E: Heartbeat Scheduler - Proactive tasks (daily summaries, reminders) - Cron-like scheduling - Background task management</p> <p>5F: Advanced Memory - Embedding caching - Multi-modal memory (images, files) - Memory consolidation and pruning</p> <p>5G: Production Hardening - Kubernetes deployment - Distributed tracing (OpenTelemetry) - Metrics (Prometheus) - Alerting (PagerDuty/Slack)</p>"},{"location":"ARCHITECTURE/#long-term-vision","title":"Long-Term Vision","text":"<ul> <li>Multi-Channel Support - Slack, Teams, Telegram</li> <li>Web Dashboard - Admin UI for configuration</li> <li>Mobile App - React Native or Flutter</li> <li>Custom Model Fine-Tuning - Domain-specific models</li> <li>Federated Deployment - Multi-region for low latency</li> <li>Plugin Marketplace - Community-contributed skills</li> </ul>"},{"location":"ARCHITECTURE/#design-decisions-log","title":"Design Decisions Log","text":""},{"location":"ARCHITECTURE/#why-gemini-for-embeddings","title":"Why Gemini for Embeddings?","text":"<p>Decision: Use Gemini <code>text-embedding-004</code> instead of OpenAI <code>text-embedding-3-small</code>.</p> <p>Rationale: - Same API as routing (fewer integrations) - 768 dimensions (good balance of quality vs storage) - Free tier available - Proven quality in benchmarks</p> <p>Trade-offs: - Tied to Google ecosystem - Less flexibility than OpenAI</p>"},{"location":"ARCHITECTURE/#why-dual-routing-backends","title":"Why Dual Routing Backends?","text":"<p>Decision: Support both Gemini (cloud) and Ollama (local) for routing.</p> <p>Rationale: - Privacy: Some users prefer local inference - Cost: Ollama has no API costs - Reliability: Fallback if one provider fails - Flexibility: Users can choose based on needs</p> <p>Trade-offs: - Increased complexity - More testing required - Ollama requires GPU for good performance</p>"},{"location":"ARCHITECTURE/#why-discordpy-over-other-libraries","title":"Why Discord.py Over Other Libraries?","text":"<p>Decision: Use <code>discord.py</code> instead of alternatives like <code>discord.js</code> or <code>pycord</code>.</p> <p>Rationale: - Python ecosystem (same language as backend) - Strong typing support - Active maintenance - Excellent documentation - Mature slash command support</p> <p>Trade-offs: - Python slower than Node.js (not critical for this use case) - Fewer real-time features than discord.js</p>"},{"location":"ARCHITECTURE/#why-qdrant-over-alternatives","title":"Why Qdrant Over Alternatives?","text":"<p>Decision: Use Qdrant instead of Pinecone, Weaviate, or Milvus.</p> <p>Rationale: - Self-hosted (no vendor lock-in) - Docker-native (easy deployment) - Excellent async Python client - Good performance on consumer hardware - Open source</p> <p>Trade-offs: - Less managed than Pinecone - Smaller ecosystem than Weaviate - Manual scaling required</p>"},{"location":"ARCHITECTURE/#why-pydantic-settings-over-python-decouple","title":"Why Pydantic Settings Over python-decouple?","text":"<p>Decision: Use Pydantic Settings for configuration.</p> <p>Rationale: - Type validation at load time - SecretStr for credentials - Computed properties - Same library as data models - Better IDE support</p> <p>Trade-offs: - Heavier than python-decouple - Requires Pydantic knowledge</p>"},{"location":"ARCHITECTURE/#diagrams","title":"Diagrams","text":""},{"location":"ARCHITECTURE/#component-interaction-diagram","title":"Component Interaction Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Discord   \u2502\n\u2502    User     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Message\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Security Layer                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502Allowlist\u2502\u2192\u2502Rate    \u2502\u2192\u2502Injection    \u2502 \u2502\n\u2502  \u2502Check    \u2502 \u2502Limit   \u2502 \u2502Detection    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Authorized\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Agent Core                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Router Factory                 \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502   \u2502\n\u2502  \u2502  \u2502 Gemini   \u2502 or\u2502 Ollama   \u2502   \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502       \u2502                                 \u2502\n\u2502       \u2502 Routing Decision                \u2502\n\u2502       \u25bc                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Memory Manager                 \u2502   \u2502\n\u2502  \u2502  Search Qdrant for context      \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502       \u2502                                 \u2502\n\u2502       \u2502 Context                         \u2502\n\u2502       \u25bc                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Generator Selection            \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u2502\n\u2502  \u2502  \u2502 Claude  \u2502 or\u2502 Gemini   \u2502    \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Response\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Memory Storage                 \u2502\n\u2502  Embed and store in Qdrant              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Discord   \u2502\n\u2502   Response  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#references","title":"References","text":"<ul> <li>Discord.py Documentation</li> <li>Qdrant Documentation</li> <li>Anthropic Claude API</li> <li>OpenAI API</li> <li>Google Gemini API</li> <li>Pydantic Documentation</li> <li>Docker Documentation</li> <li>Structlog Documentation</li> </ul>"},{"location":"ARCHITECTURE/#contact-support","title":"Contact &amp; Support","text":"<ul> <li>Issues: https://github.com/jimtin/zetherion-ai/issues</li> <li>Discussions: https://github.com/jimtin/zetherion-ai/discussions</li> <li>Documentation: See docs/ directory for detailed guides</li> </ul> <p>Last Updated: 2026-02-06 Version: 1.0.0 (Phases 1-4 complete, 87.58% test coverage)</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to Zetherion AI will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"[Unreleased]","text":""},{"location":"CHANGELOG/#added-phase-5-2026-02-06","title":"Added - Phase 5 (2026-02-06)","text":""},{"location":"CHANGELOG/#phase-5a-encryption-layer","title":"Phase 5A: Encryption Layer","text":"<ul> <li>AES-256-GCM field-level encryption for sensitive Qdrant payloads</li> <li>PBKDF2-HMAC-SHA256 key derivation with 600k iterations</li> <li>TLS support for Qdrant connections (optional)</li> <li><code>FieldEncryptor</code> and <code>KeyManager</code> classes in <code>zetherion_ai.security</code></li> </ul>"},{"location":"CHANGELOG/#phase-5b-inferencebroker","title":"Phase 5B: InferenceBroker","text":"<ul> <li>Smart multi-provider LLM routing based on task type</li> <li>Provider capability matrix: Claude (code), OpenAI (reasoning), Gemini (long docs), Ollama (lightweight)</li> <li>16 TaskTypes for granular routing decisions</li> <li>Automatic fallback chains when primary provider unavailable</li> </ul>"},{"location":"CHANGELOG/#phase-5b1-model-registry-cost-tracking","title":"Phase 5B.1: Model Registry &amp; Cost Tracking","text":"<ul> <li>Dynamic model discovery via provider APIs</li> <li>Tier-based model selection (quality/balanced/fast)</li> <li>SQLite cost tracking with per-request logging</li> <li>Daily/monthly cost aggregation and reporting</li> <li>Discord notifications for budget alerts</li> </ul>"},{"location":"CHANGELOG/#phase-5c-user-profile-system","title":"Phase 5C: User Profile System","text":"<ul> <li>8 profile categories: identity, preferences, schedule, projects, relationships, skills, goals, habits</li> <li>Tiered inference: Tier 1 (regex), Tier 2 (Ollama), Tier 3 (embeddings), Tier 4 (cloud)</li> <li>5 signal engines for implicit extraction</li> <li>TTL-based caching with confidence scoring</li> <li>Background profile extraction after responses</li> </ul>"},{"location":"CHANGELOG/#phase-5c1-employment-profile","title":"Phase 5C.1: Employment Profile","text":"<ul> <li>Bot identity and relationship modeling</li> <li>Trust levels: MINIMAL \u2192 BUILDING \u2192 ESTABLISHED \u2192 HIGH \u2192 FULL</li> <li>10 relationship milestones tracking</li> <li>Communication style adaptation (formality, verbosity, proactivity)</li> </ul>"},{"location":"CHANGELOG/#phase-5d-skills-framework","title":"Phase 5D: Skills Framework","text":"<ul> <li>Abstract Skill interface with permission-based access control</li> <li>15 granular permissions (read/write profile, memories, messages, etc.)</li> <li>SkillRegistry for intent routing and heartbeat coordination</li> <li>Separate Docker container for skills service isolation</li> <li>REST API with authentication for bot \u2194 skills communication</li> </ul>"},{"location":"CHANGELOG/#phase-5e-built-in-skills","title":"Phase 5E: Built-in Skills","text":"<ul> <li>Task Manager: CRUD operations, priority levels, deadlines, heartbeat reminders</li> <li>Calendar Awareness: Event types, recurrence patterns, availability checking</li> <li>Profile Manager: GDPR-style view/update/delete/export, confidence reports</li> </ul>"},{"location":"CHANGELOG/#phase-5f-heartbeat-scheduler","title":"Phase 5F: Heartbeat Scheduler","text":"<ul> <li>Configurable interval (default 5 min) with quiet hours support</li> <li>Rate limiting (3 msgs/hour per user)</li> <li>ActionExecutor with handlers for all skill action types</li> <li>Scheduled event handling for one-time triggers</li> </ul>"},{"location":"CHANGELOG/#phase-5g-router-enhancement","title":"Phase 5G: Router Enhancement","text":"<ul> <li>3 new skill intents: TASK_MANAGEMENT, CALENDAR_QUERY, PROFILE_QUERY</li> <li>Skill intent examples in router prompts</li> <li>Agent integration for skill intent handling</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Project renamed from <code>secureclaw</code> to <code>zetherion-ai</code></li> <li>All imports updated to use <code>zetherion_ai</code> module name</li> <li>Docker container names now use hyphens (<code>zetherion-ai-*</code>)</li> <li>Comprehensive test suite expanded to 885 unit tests (78% coverage)</li> <li>Integration tests updated for Phase 5 features</li> </ul>"},{"location":"CHANGELOG/#previous-changes","title":"Previous Changes","text":"<ul> <li>Comprehensive test suite with 87.58% overall coverage</li> <li>Router factory with pluggable backend architecture (Gemini/Ollama)</li> <li>Discord bot commands: <code>/channels</code>, <code>/remember</code>, <code>/summarize</code></li> <li>File-based logging with rotation support</li> <li>Pre-commit hooks for code quality (Ruff, Mypy, Bandit, Gitleaks, Hadolint)</li> <li>CI/CD pipeline with 6 parallel jobs</li> <li>Docker Compose setup for local development</li> <li>Security controls: rate limiting, user allowlist, prompt injection detection</li> <li>Vector memory using Qdrant for long-term context</li> <li>Async embeddings with parallel batch processing</li> <li>Contributing guidelines and code of conduct</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>All test failures resolved (Phase 1A, 1B, 1C)</li> <li>Config tests with environment variable isolation using monkeypatch</li> <li>Docker integration tests with proper service startup</li> <li>Type checking errors in async Qdrant client</li> <li>GitHub push protection issues with example tokens in documentation</li> </ul>"},{"location":"CHANGELOG/#100-initial-release","title":"[1.0.0] - Initial Release","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Discord bot with dual LLM backends (Gemini + Ollama)</li> <li>Message routing with intent classification</li> <li>Claude/OpenAI integration for complex tasks</li> <li>Gemini/Ollama integration for simple queries</li> <li>Qdrant vector database for memory</li> <li>Google Gemini embeddings for semantic search</li> <li>Docker containerization</li> <li>Basic security controls (rate limiting, allowlist)</li> <li>Comprehensive error handling and retry logic</li> </ul>"},{"location":"CHANGELOG/#security","title":"Security","text":"<ul> <li>Pydantic SecretStr for all credentials</li> <li>Gitleaks secret scanning in pre-commit hooks</li> <li>Bandit security scanning in CI/CD</li> <li>CodeQL weekly analysis</li> <li>Pinned dependencies with Dependabot</li> <li>Prompt injection detection (17 regex patterns)</li> <li>User allowlist for Discord interactions</li> </ul>"},{"location":"CHANGELOG/#version-history-details","title":"Version History Details","text":""},{"location":"CHANGELOG/#phase-1-test-fixes-coverage-42-78","title":"Phase 1: Test Fixes (Coverage: ~42% \u2192 78%)","text":"<p>Phase 1A: Agent Core Tests - Fixed 13 test failures - Resolved Docker service dependency issues - Fixed async Qdrant client usage - Improved retry logic testing - Result: 41 tests passing, 94.76% coverage</p> <p>Phase 1B: Security Tests - Fixed 3 test failures - Fixed prompt injection detection tests - Corrected allowlist and rate limiter tests - Result: 37 tests passing, 94.12% coverage</p> <p>Phase 1C: Config Tests - Fixed 10 test failures - Implemented environment variable isolation with monkeypatch - Fixed allowed_user_ids parsing tests - Resolved Docker integration test errors (14 tests) - Result: All config tests passing, 96.88% coverage</p>"},{"location":"CHANGELOG/#phase-2-coverage-improvements-coverage-78-8758","title":"Phase 2: Coverage Improvements (Coverage: 78% \u2192 87.58%)","text":"<p>Phase 2A: Router Factory Tests - 26% \u2192 100% coverage - Added 12 comprehensive tests for factory functions - Tested async/sync router creation - Tested health checks and fallback logic - Tested Ollama \u2192 Gemini fallback scenarios - Result: 12 new tests, 100% coverage</p> <p>Phase 2B: Discord Bot Edge Cases - 68.55% \u2192 89.92% coverage - Added 11 edge case tests for uncovered code paths - <code>/channels</code> command tests (6 tests):   - Unauthorized user handling   - DM vs guild context   - Text/voice/category channel listing   - Long response message splitting - Agent readiness tests (1 test) - <code>_send_long_message</code> helper tests (4 tests) - Result: 11 new tests, 89.92% coverage (exceeded 85% target)</p>"},{"location":"CHANGELOG/#final-test-statistics","title":"Final Test Statistics","text":"Category Count Status Unit Tests 255 \u2705 All passing Integration Tests 14 \u2705 All passing Discord E2E Tests 4 \u2705 4 passing, 1 skipped Overall Coverage 87.58% \u2705 Target exceeded"},{"location":"CHANGELOG/#module-coverage-breakdown","title":"Module Coverage Breakdown","text":"Module Coverage Tests Status Router Factory 100% 12 \u2705 Comprehensive Config 96.88% 15 \u2705 Excellent Agent Core 94.76% 41 \u2705 Excellent Security 94.12% 37 \u2705 Excellent Discord Bot 89.92% 30 \u2705 Very Good Logging 85.71% 8 \u2705 Good Memory (Qdrant) 84.62% 12 \u2705 Good Memory (Embeddings) 81.82% 6 \u2705 Good Router (Gemini) 78.95% 8 \u2705 Good Router (Ollama) 75.86% 8 \u2705 Good"},{"location":"CHANGELOG/#documentation-updates","title":"Documentation Updates","text":""},{"location":"CHANGELOG/#recent-documentation-improvements","title":"Recent Documentation Improvements","text":"<ul> <li>README.md: Added CI/CD badges, key features section, comprehensive testing table</li> <li>SECURITY.md: Updated test coverage table with Phase 1 &amp; 2 improvements</li> <li>CONTRIBUTING.md: Created comprehensive contributor guide</li> <li>TROUBLESHOOTING.md: Fixed example tokens to prevent GitHub push protection issues</li> <li>TESTING.md: Added test organization, coverage targets, and debugging guides</li> </ul>"},{"location":"CHANGELOG/#documentation-files","title":"Documentation Files","text":"File Purpose Status README.md Project overview and quick start \u2705 Updated CONTRIBUTING.md Contribution guidelines \u2705 Created CHANGELOG.md Version history \u2705 Created SECURITY.md Security controls and testing \u2705 Updated docs/ARCHITECTURE.md System architecture \u2705 Complete docs/TESTING.md Testing guide \u2705 Complete docs/TROUBLESHOOTING.md Common issues \u2705 Updated docs/FAQ.md Frequently asked questions \u2705 Complete docs/COMMANDS.md Discord command reference \u2705 Complete"},{"location":"CHANGELOG/#cicd-pipeline","title":"CI/CD Pipeline","text":""},{"location":"CHANGELOG/#pipeline-stages","title":"Pipeline Stages","text":"<ol> <li>Lint - Ruff linting and formatting</li> <li>Type Check - Mypy strict mode type checking</li> <li>Security - Bandit security scanning</li> <li>Tests - Unit tests on Python 3.12 &amp; 3.13</li> <li>Docker Build - Container build verification</li> <li>Integration - Full integration tests with Docker services</li> </ol>"},{"location":"CHANGELOG/#pre-commit-hooks","title":"Pre-Commit Hooks","text":"<ul> <li>Ruff (linting and formatting)</li> <li>Mypy (type checking)</li> <li>Gitleaks (secret scanning)</li> <li>Bandit (security scanning)</li> <li>Hadolint (Dockerfile linting)</li> <li>File checks (trailing whitespace, EOF, merge conflicts)</li> </ul>"},{"location":"CHANGELOG/#breaking-changes","title":"Breaking Changes","text":"<p>None yet - this is the initial stable release with comprehensive testing.</p>"},{"location":"CHANGELOG/#migration-guide","title":"Migration Guide","text":""},{"location":"CHANGELOG/#from-development-to-production","title":"From Development to Production","text":"<ol> <li>Update <code>.env</code> file:</li> <li>Set <code>ENVIRONMENT=production</code></li> <li>Set <code>LOG_LEVEL=INFO</code></li> <li> <p>Configure <code>ALLOWED_USER_IDS</code> for production users</p> </li> <li> <p>Review Security Settings:</p> </li> <li>Ensure all API keys are properly set</li> <li>Verify user allowlist is configured</li> <li> <p>Check rate limiting settings</p> </li> <li> <p>Deploy with Docker Compose:    <pre><code>docker compose up -d\n</code></pre></p> </li> <li> <p>Monitor Logs:    <pre><code>tail -f logs/zetherion_ai.log | jq .\n</code></pre></p> </li> </ol>"},{"location":"CHANGELOG/#known-issues","title":"Known Issues","text":"<p>None currently reported. See GitHub Issues for the latest.</p>"},{"location":"CHANGELOG/#future-roadmap","title":"Future Roadmap","text":""},{"location":"CHANGELOG/#planned-features-phase-5","title":"Planned Features (Phase 5)","text":"<ul> <li>Encrypted memory storage (AES-256-GCM)</li> <li>User profiling and preferences</li> <li>Skills framework for extensible commands</li> <li>Smart multi-provider routing (Claude for code, OpenAI for reasoning, Gemini for docs)</li> <li>Heartbeat scheduler for proactive tasks</li> </ul> <p>See <code>memory/phase5-plan.md</code> for detailed implementation plan.</p>"},{"location":"CHANGELOG/#contributors","title":"Contributors","text":"<ul> <li>James Hinton (@jimtin) - Project creator and maintainer</li> <li>Claude Sonnet 4.5 - AI pair programming assistant</li> </ul> <p>See CONTRIBUTING.md for contribution guidelines.</p>"},{"location":"CHANGELOG/#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"CHANGELOG/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Anthropic for Claude API</li> <li>Google for Gemini API</li> <li>OpenAI for GPT models</li> <li>Discord.py community</li> <li>Qdrant team for vector database</li> <li>Ollama team for local LLM runtime</li> </ul>"},{"location":"CI_CD/","title":"CI/CD Pipeline Documentation","text":"<p>Complete guide to Zetherion AI's testing and continuous integration setup.</p>"},{"location":"CI_CD/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Local Development Workflow</li> <li>Git Hooks Setup</li> <li>Pre-Commit Hooks</li> <li>Pre-Push Hooks</li> <li>GitHub Actions CI/CD</li> <li>Running Tests Manually</li> <li>Troubleshooting</li> </ol>"},{"location":"CI_CD/#overview","title":"Overview","text":"<p>Zetherion AI uses a three-tier testing approach to ensure code quality:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Tier 1: Pre-Commit Hooks (Lightweight)                     \u2502\n\u2502  \u2022 Runs before each commit                                  \u2502\n\u2502  \u2022 Linting (Ruff)                                           \u2502\n\u2502  \u2022 Formatting (Ruff Format)                                 \u2502\n\u2502  \u2022 File checks (trailing whitespace, large files, etc.)     \u2502\n\u2502  \u2022 Security scan (Bandit)                                   \u2502\n\u2502  Duration: ~5-10 seconds                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Tier 2: Pre-Push Hooks (Comprehensive)                     \u2502\n\u2502  \u2022 Runs before each push to remote                          \u2502\n\u2502  \u2022 Full linting                                             \u2502\n\u2502  \u2022 Type checking (mypy)                                     \u2502\n\u2502  \u2022 Complete test suite with coverage                        \u2502\n\u2502  Duration: ~30-60 seconds                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Tier 3: GitHub Actions CI/CD (Exhaustive)                  \u2502\n\u2502  \u2022 Runs on push/PR to main/develop                          \u2502\n\u2502  \u2022 Multi-version testing (Python 3.12, 3.13)               \u2502\n\u2502  \u2022 Integration tests with services (Qdrant)                 \u2502\n\u2502  \u2022 Docker build validation                                  \u2502\n\u2502  \u2022 Security scanning                                        \u2502\n\u2502  \u2022 Coverage reporting (Codecov)                             \u2502\n\u2502  Duration: ~5-10 minutes                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Philosophy: - Fast feedback locally (pre-commit catches simple issues in seconds) - Confidence before push (pre-push ensures tests pass) - Comprehensive CI (GitHub Actions catches integration issues)</p>"},{"location":"CI_CD/#local-development-workflow","title":"Local Development Workflow","text":""},{"location":"CI_CD/#recommended-workflow","title":"Recommended Workflow","text":"<pre><code># 1. Make your changes\nvim src/zetherion_ai/some_file.py\n\n# 2. Commit (pre-commit hooks run automatically)\ngit add src/zetherion_ai/some_file.py\ngit commit -m \"Add new feature\"\n# \u2192 Runs linting, formatting, security checks (~5-10s)\n\n# 3. Push (pre-push hooks run automatically)\ngit push origin main\n# \u2192 Runs full test suite (~30-60s)\n# \u2192 If tests pass, code is pushed to GitHub\n# \u2192 GitHub Actions CI/CD starts automatically\n</code></pre>"},{"location":"CI_CD/#what-happens-when","title":"What Happens When","text":"<p>On <code>git commit</code>: 1. Ruff linter checks code style 2. Ruff formatter auto-fixes formatting 3. File checks (trailing whitespace, large files) 4. Security scan (Bandit) 5. If all pass \u2192 commit succeeds 6. If any fail \u2192 commit blocked, fixes applied automatically</p> <p>On <code>git push</code>: 1. Full Ruff linting (comprehensive) 2. Type checking with mypy 3. Complete test suite with pytest 4. Coverage report 5. If all pass \u2192 push succeeds 6. If any fail \u2192 push blocked, errors shown</p> <p>On GitHub (after push): 1. Linting job 2. Type checking job 3. Security scanning job 4. Tests on Python 3.12 &amp; 3.13 5. Docker build validation 6. Integration tests with Qdrant 7. Summary report</p>"},{"location":"CI_CD/#git-hooks-setup","title":"Git Hooks Setup","text":""},{"location":"CI_CD/#initial-setup-one-time","title":"Initial Setup (One-Time)","text":"<p>After cloning the repository:</p> <pre><code># Activate virtual environment\nsource .venv/bin/activate\n\n# Run setup script\n./scripts/setup-git-hooks.sh\n</code></pre> <p>What the script does: 1. Installs <code>pre-commit</code> framework 2. Installs pre-commit hooks from <code>.pre-commit-config.yaml</code> 3. Creates symlink for custom pre-push hook 4. Optionally runs checks on all files</p> <p>Manual Setup (if script fails):</p> <pre><code># 1. Install pre-commit\npip install pre-commit\n\n# 2. Install hooks\npre-commit install --hook-type pre-commit --hook-type pre-push\n\n# 3. Link custom pre-push hook\nln -sf ../../.git-hooks/pre-push .git/hooks/pre-push\n</code></pre>"},{"location":"CI_CD/#verify-installation","title":"Verify Installation","text":"<pre><code># Check that hooks are installed\nls -la .git/hooks/\n# Should show: pre-commit, pre-push (symlinks)\n\n# Test pre-commit hooks\npre-commit run --all-files\n</code></pre>"},{"location":"CI_CD/#pre-commit-hooks","title":"Pre-Commit Hooks","text":""},{"location":"CI_CD/#what-runs-on-every-commit","title":"What Runs on Every Commit","text":"<p>Defined in <code>.pre-commit-config.yaml</code>:</p> <ol> <li>Ruff Linter</li> <li>Checks: Code style (PEP 8), unused imports, complexity</li> <li>Auto-fixes: Import sorting, simple style issues</li> <li> <p>Config: <code>pyproject.toml</code></p> </li> <li> <p>Ruff Formatter</p> </li> <li>Checks: Code formatting (like Black)</li> <li>Auto-fixes: Reformats all Python files</li> <li> <p>Replaces: Black, isort</p> </li> <li> <p>General File Checks</p> </li> <li>Large files (&gt;1MB blocked)</li> <li>Trailing whitespace (auto-removed)</li> <li>File endings (ensures newline at EOF)</li> <li>YAML/TOML/JSON syntax</li> <li>Merge conflict markers</li> <li> <p>Private keys detection</p> </li> <li> <p>Bandit Security Scan</p> </li> <li>Checks: Common security issues (SQL injection, hardcoded passwords, etc.)</li> <li>Skips: Test files</li> <li> <p>Config: <code>pyproject.toml</code></p> </li> <li> <p>Gitleaks Secret Scanner</p> </li> <li>Checks: API keys, tokens, passwords, private keys, credentials</li> <li>Detects: Discord tokens, Google/Gemini keys, Anthropic keys, OpenAI keys, AWS keys, GitHub tokens, JWT tokens, high-entropy strings</li> <li>Config: <code>.gitleaks.toml</code></li> <li> <p>Prevents: Accidental commit of secrets to repository</p> </li> <li> <p>Hadolint (Dockerfile linting)</p> </li> <li>Checks: Dockerfile best practices</li> <li>Ignores: Apt-get pin warnings</li> </ol>"},{"location":"CI_CD/#bypassing-pre-commit-not-recommended","title":"Bypassing Pre-Commit (Not Recommended)","text":"<pre><code># Skip all pre-commit hooks (NOT RECOMMENDED)\ngit commit --no-verify -m \"Quick fix\"\n\n# Skip specific hook\nSKIP=ruff git commit -m \"Skip ruff only\"\n</code></pre> <p>\u26a0\ufe0f Warning: Bypassing hooks may cause CI to fail.</p>"},{"location":"CI_CD/#running-pre-commit-manually","title":"Running Pre-Commit Manually","text":"<pre><code># Run on all files\npre-commit run --all-files\n\n# Run on staged files only\npre-commit run\n\n# Run specific hook\npre-commit run ruff --all-files\n\n# Run only Gitleaks secret scanner\npre-commit run gitleaks --all-files\n\n# Update hooks to latest versions\npre-commit autoupdate\n</code></pre>"},{"location":"CI_CD/#secret-scanning-with-gitleaks","title":"Secret Scanning with Gitleaks","text":"<p>What Gitleaks Detects: - API Keys: Discord tokens, Google/Gemini keys, Anthropic (Claude) keys, OpenAI keys, AWS keys, GitHub tokens - Private Keys: RSA, SSH, PGP, OpenSSH private keys - Credentials: Passwords in URLs, JWT tokens, Slack tokens - High-Entropy Strings: Potential secrets based on randomness</p> <p>Configuration: - Rules defined in <code>.gitleaks.toml</code> - Custom rules for Zetherion AI-specific secrets - Allowlist for false positives (e.g., <code>.env.example</code>, test fixtures)</p> <p>What's Excluded: - <code>.env.example</code> (template file with placeholders) - Test fixtures with <code>test_*</code> prefixes - Generated files (lock files, cache directories) - Logs and coverage reports</p> <p>If Gitleaks Finds a Secret: <pre><code># 1. DO NOT commit the file\n# 2. Remove the secret from the file\nvim .env  # Replace actual secret with placeholder\n\n# 3. If the secret was already committed (previous commits):\n# Option A: Use BFG Repo-Cleaner to remove from history\ngit clone --mirror git@github.com:user/repo.git\nbfg --replace-text passwords.txt repo.git\ncd repo.git\ngit reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive\ngit push\n\n# Option B: Interactive rebase (for recent commits)\ngit rebase -i HEAD~5  # Edit last 5 commits\n# Mark commit as 'edit', remove secret, continue\ngit rebase --continue\n\n# 4. Rotate the exposed secret immediately\n# - Discord: Generate new bot token\n# - API keys: Regenerate in provider dashboard\n</code></pre></p> <p>Testing Gitleaks: <pre><code># Scan all files\npre-commit run gitleaks --all-files\n\n# Scan specific file\ngitleaks detect --no-git --source=src/zetherion_ai/config.py\n\n# Generate detailed report\ngitleaks detect --no-git --report-path=gitleaks-report.json\n</code></pre></p>"},{"location":"CI_CD/#pre-push-hooks","title":"Pre-Push Hooks","text":""},{"location":"CI_CD/#what-runs-before-every-push","title":"What Runs Before Every Push","text":"<p>Defined in <code>.git-hooks/pre-push</code>:</p> <p>Step 1: Linting (Ruff) <pre><code>ruff check src/ tests/\n</code></pre> - Comprehensive linting of all source code - Fails if any issues found</p> <p>Step 2: Type Checking (mypy) <pre><code>mypy src/zetherion_ai --config-file=pyproject.toml\n</code></pre> - Static type checking - Ensures type safety - Skips: Tests, scripts</p> <p>Step 3: Full Test Suite <pre><code>pytest tests/ -v --tb=short --cov=src/zetherion_ai --cov-report=term-missing\n</code></pre> - Runs all tests - Generates coverage report - Shows missing coverage</p> <p>Total Time: ~30-60 seconds</p>"},{"location":"CI_CD/#bypassing-pre-push-not-recommended","title":"Bypassing Pre-Push (Not Recommended)","text":"<pre><code># Skip pre-push hook (NOT RECOMMENDED)\ngit push --no-verify origin main\n</code></pre> <p>\u26a0\ufe0f Warning: This will likely cause GitHub Actions to fail.</p>"},{"location":"CI_CD/#running-pre-push-checks-manually","title":"Running Pre-Push Checks Manually","text":"<pre><code># Run the pre-push hook manually\n.git-hooks/pre-push\n\n# Or run individual steps:\nruff check src/ tests/\nmypy src/zetherion_ai --config-file=pyproject.toml\npytest tests/ -v --cov=src/zetherion_ai\n</code></pre>"},{"location":"CI_CD/#github-actions-cicd","title":"GitHub Actions CI/CD","text":""},{"location":"CI_CD/#workflow-overview","title":"Workflow Overview","text":"<p>Defined in <code>.github/workflows/ci.yml</code></p> <p>Triggers: - Push to <code>main</code> or <code>develop</code> branches - Pull requests to <code>main</code> or <code>develop</code> - Manual trigger via GitHub UI (<code>workflow_dispatch</code>)</p> <p>Jobs:</p> <pre><code>lint (5s)\n  \u251c\u2500 Ruff linter\n  \u2514\u2500 Ruff formatter check\n\ntype-check (10s)\n  \u2514\u2500 mypy on src/zetherion_ai\n\nsecurity (10s)\n  \u2514\u2500 Bandit security scan\n\ntest (60s)\n  \u251c\u2500 Python 3.12 tests + coverage\n  \u2514\u2500 Python 3.13 tests + coverage\n\ndocker-build (30s)\n  \u251c\u2500 Build Docker image\n  \u2514\u2500 Validate docker-compose.yml\n\nintegration (2-3 min) \u26a1 RUNS BY DEFAULT\n  \u251c\u2500 Start Docker Compose (Qdrant + Zetherion AI)\n  \u251c\u2500 Wait for services to be healthy\n  \u251c\u2500 Run full end-to-end integration tests\n  \u251c\u2500 Upload logs on failure\n  \u2514\u2500 Clean up Docker resources\n\nsummary (5s)\n  \u251c\u2500 Check all job results\n  \u2514\u2500 Post summary to PR\n</code></pre> <p>Total Time: ~5-10 minutes</p>"},{"location":"CI_CD/#integration-tests-run-automatically","title":"\u26a1 Integration Tests Run Automatically","text":"<p>NEW: Integration tests now run by default on every push/PR to ensure end-to-end functionality.</p> <p>To skip integration tests, add <code>[skip integration]</code> to your commit message:</p> <pre><code>git commit -m \"Update documentation [skip integration]\"\ngit push\n</code></pre> <p>When to skip: - Documentation-only changes - Minor typo fixes - README updates - Configuration changes that don't affect functionality</p> <p>When NOT to skip: - Code changes in <code>src/</code> - Changes to Docker configuration - Dependency updates - Any functional changes</p>"},{"location":"CI_CD/#viewing-ci-results","title":"Viewing CI Results","text":"<p>On GitHub: 1. Go to your repository 2. Click Actions tab 3. Click on the latest workflow run 4. View job results and logs</p> <p>On Pull Requests: - Status checks appear at bottom of PR - Required checks must pass before merging - Click \"Details\" to see full logs</p>"},{"location":"CI_CD/#coverage-reports","title":"Coverage Reports","text":"<p>Codecov Integration: - Coverage reports uploaded to codecov.io - Badge shows coverage percentage - PR comments show coverage changes</p> <p>Download Coverage Report: <pre><code># Coverage HTML report saved as artifact\n# Download from GitHub Actions \u2192 Workflow run \u2192 Artifacts\n</code></pre></p>"},{"location":"CI_CD/#manual-trigger","title":"Manual Trigger","text":"<pre><code># Via GitHub CLI\ngh workflow run ci.yml\n\n# Via GitHub UI\n# Actions \u2192 CI/CD Pipeline \u2192 Run workflow\n</code></pre>"},{"location":"CI_CD/#running-tests-manually","title":"Running Tests Manually","text":""},{"location":"CI_CD/#quick-tests-during-development","title":"Quick Tests (During Development)","text":"<pre><code># Run specific test file\npytest tests/test_router.py -v\n\n# Run specific test class\npytest tests/test_config.py::TestSettingsInitialization -v\n\n# Run specific test\npytest tests/test_config.py::TestSettingsInitialization::test_settings_from_env_minimal -v\n\n# Run with pattern matching\npytest tests/ -k \"test_router\" -v\n</code></pre>"},{"location":"CI_CD/#full-test-suite","title":"Full Test Suite","text":"<pre><code># With coverage\npytest tests/ --cov=src/zetherion_ai --cov-report=html\n\n# Open coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\n</code></pre>"},{"location":"CI_CD/#test-markers","title":"Test Markers","text":"<pre><code># Run only fast tests (exclude slow integration tests)\npytest tests/ -v -m \"not slow\"\n\n# Run only integration tests\npytest tests/ -v -m \"integration\"\n\n# Run only unit tests\npytest tests/ -v -m \"unit\"\n</code></pre>"},{"location":"CI_CD/#debugging-tests","title":"Debugging Tests","text":"<pre><code># Show print statements\npytest tests/ -v -s\n\n# Drop into debugger on failure\npytest tests/ -v --pdb\n\n# Show full traceback\npytest tests/ -v --tb=long\n\n# Stop at first failure\npytest tests/ -v -x\n</code></pre>"},{"location":"CI_CD/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CI_CD/#pre-commit-hook-fails","title":"Pre-Commit Hook Fails","text":"<p>Error: <code>pre-commit: command not found</code> <pre><code># Solution: Install pre-commit\npip install pre-commit\npre-commit install\n</code></pre></p> <p>Error: Hook fails with ImportError <pre><code># Solution: Ensure dependencies installed\npip install -r requirements.txt\npip install -e \".[dev]\"\n</code></pre></p> <p>Error: Ruff not found <pre><code># Solution: Install dev dependencies\npip install ruff mypy bandit[toml]\n</code></pre></p>"},{"location":"CI_CD/#pre-push-hook-fails","title":"Pre-Push Hook Fails","text":"<p>Error: Tests fail locally <pre><code># 1. Run tests to see details\npytest tests/ -v --tb=short\n\n# 2. Fix failing tests\n\n# 3. Try again\ngit push\n</code></pre></p> <p>Error: mypy type errors <pre><code># 1. Run mypy to see details\nmypy src/zetherion_ai --config-file=pyproject.toml\n\n# 2. Fix type errors\n\n# 3. Try again\ngit push\n</code></pre></p> <p>Error: Coverage too low <pre><code># 1. Check which files lack coverage\npytest tests/ --cov=src/zetherion_ai --cov-report=term-missing\n\n# 2. Add tests for missing coverage\n\n# 3. Try again\ngit push\n</code></pre></p>"},{"location":"CI_CD/#github-actions-fails","title":"GitHub Actions Fails","text":"<p>Error: Tests pass locally but fail on GitHub</p> <p>Possible causes: 1. Environment differences    - Solution: Ensure <code>.env</code> secrets are set in GitHub    - Go to: Settings \u2192 Secrets and variables \u2192 Actions</p> <ol> <li>Dependency issues</li> <li>Solution: Check if <code>requirements.txt</code> is up to date</li> <li> <p>Run: <code>pip freeze &gt; requirements.txt</code></p> </li> <li> <p>Python version differences</p> </li> <li>Solution: Test locally with Python 3.12 and 3.13</li> <li> <p>Use: <code>pyenv</code> or Docker</p> </li> <li> <p>Service dependencies</p> </li> <li>Solution: Check if Qdrant service started correctly</li> <li>View logs in GitHub Actions</li> </ol> <p>Error: Docker build fails on GitHub <pre><code># Test Docker build locally\ndocker build -t zetherion_ai:test .\n\n# If fails, fix Dockerfile and try again\n</code></pre></p>"},{"location":"CI_CD/#bypassing-hooks-safely","title":"Bypassing Hooks Safely","text":"<p>When it's okay to bypass: - Emergency hotfix (fix immediately, create cleanup PR later) - Documentation-only changes - CI configuration changes</p> <p>How to bypass safely: <pre><code># Skip pre-commit only\ngit commit --no-verify -m \"docs: Update README\"\n\n# Skip pre-push only (commit hooks still run)\ngit push --no-verify origin main\n\n# Skip all (NOT RECOMMENDED)\ngit commit --no-verify -m \"Emergency fix\"\ngit push --no-verify origin main\n</code></pre></p> <p>\u26a0\ufe0f Best Practice: - Use <code>--no-verify</code> sparingly - Always create follow-up PR to fix issues - Never bypass on <code>main</code> branch</p>"},{"location":"CI_CD/#configuration-files","title":"Configuration Files","text":""},{"location":"CI_CD/#pre-commit-configuration","title":"Pre-Commit Configuration","text":"<p>File: <code>.pre-commit-config.yaml</code></p> <pre><code>repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.2.2\n    hooks:\n      - id: ruff\n      - id: ruff-format\n</code></pre> <p>Update hooks: <pre><code>pre-commit autoupdate\n</code></pre></p>"},{"location":"CI_CD/#test-configuration","title":"Test Configuration","text":"<p>File: <code>pyproject.toml</code></p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\naddopts = [\n    \"-v\",\n    \"--strict-markers\",\n    \"--tb=short\",\n    \"--cov=src/zetherion_ai\",\n]\n</code></pre>"},{"location":"CI_CD/#type-checking-configuration","title":"Type Checking Configuration","text":"<p>File: <code>pyproject.toml</code></p> <pre><code>[tool.mypy]\npython_version = \"3.12\"\nstrict = true\n</code></pre>"},{"location":"CI_CD/#best-practices","title":"Best Practices","text":""},{"location":"CI_CD/#commit-messages","title":"Commit Messages","text":"<p>Follow Conventional Commits:</p> <pre><code># Feature\ngit commit -m \"feat: Add Ollama router backend\"\n\n# Bug fix\ngit commit -m \"fix: Resolve Docker memory allocation bug\"\n\n# Documentation\ngit commit -m \"docs: Update CI/CD setup guide\"\n\n# Tests\ngit commit -m \"test: Add Ollama router tests\"\n\n# Chore\ngit commit -m \"chore: Update dependencies\"\n</code></pre>"},{"location":"CI_CD/#test-coverage","title":"Test Coverage","text":"<p>Target: 80%+ coverage</p> <pre><code># Check current coverage\npytest tests/ --cov=src/zetherion_ai --cov-report=term\n\n# View detailed report\npytest tests/ --cov=src/zetherion_ai --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"CI_CD/#code-quality","title":"Code Quality","text":"<p>Before committing: 1. Run <code>ruff check --fix src/ tests/</code> 2. Run <code>mypy src/zetherion_ai</code> 3. Run <code>pytest tests/</code> 4. Review changes: <code>git diff</code></p> <p>Before pushing: 1. Ensure tests pass 2. Update documentation if needed 3. Check coverage hasn't decreased 4. Rebase on latest main</p>"},{"location":"CI_CD/#quick-reference","title":"Quick Reference","text":""},{"location":"CI_CD/#common-commands","title":"Common Commands","text":"<pre><code># Setup git hooks (one-time)\n./scripts/setup-git-hooks.sh\n\n# Run pre-commit manually\npre-commit run --all-files\n\n# Run tests\npytest tests/ -v\n\n# Run tests with coverage\npytest tests/ --cov=src/zetherion_ai\n\n# Run linter\nruff check src/ tests/\n\n# Run formatter\nruff format src/ tests/\n\n# Run type checker\nmypy src/zetherion_ai\n\n# Update pre-commit hooks\npre-commit autoupdate\n\n# Skip hooks (emergency only)\ngit commit --no-verify\ngit push --no-verify\n</code></pre>"},{"location":"CI_CD/#github-actions-badge","title":"GitHub Actions Badge","text":"<p>Add to README.md:</p> <pre><code>[![CI](https://github.com/yourusername/zetherion_ai/actions/workflows/ci.yml/badge.svg)](https://github.com/yourusername/zetherion_ai/actions/workflows/ci.yml)\n</code></pre>"},{"location":"CI_CD/#additional-resources","title":"Additional Resources","text":"<ul> <li>Pre-commit Framework</li> <li>Ruff Documentation</li> <li>GitHub Actions Documentation</li> <li>Pytest Documentation</li> <li>Codecov Documentation</li> </ul>"},{"location":"COMMANDS/","title":"Zetherion AI Command Reference","text":"<p>Complete list of all Discord commands and interactions for Zetherion AI.</p>"},{"location":"COMMANDS/#quick-reference","title":"Quick Reference","text":"Command Type Description Usage <code>/ask</code> Slash Command Ask a question <code>/ask What is Python?</code> <code>/remember</code> Slash Command Store a memory <code>/remember I prefer dark mode</code> <code>/search</code> Slash Command Search memories <code>/search preferences</code> <code>/ping</code> Slash Command Check bot status <code>/ping</code> DM Direct Message Talk naturally Just send a message Mention Server Message Ask in server <code>@Zetherion AI help me</code>"},{"location":"COMMANDS/#slash-commands","title":"Slash Commands","text":""},{"location":"COMMANDS/#ask-ask-a-question","title":"<code>/ask</code> - Ask a Question","text":"<p>Description: Ask Zetherion AI a question or request help with a task.</p> <p>Syntax: <pre><code>/ask &lt;question&gt;\n</code></pre></p> <p>Parameters: - <code>question</code> (required) - Your question or request</p> <p>Examples: <pre><code>/ask What is the capital of France?\n/ask Explain async/await in Python\n/ask Write a function to reverse a string\n/ask What's the weather like? (bot doesn't have real-time data)\n/ask Help me debug this error: TypeError...\n</code></pre></p> <p>Behavior: - Bot analyzes intent and complexity - Simple questions \u2192 Answered by Gemini Flash (fast) - Complex tasks \u2192 Routed to Claude/GPT-4 (slower, better quality) - Searches recent conversation history and relevant memories - Response includes context from previous interactions</p> <p>Expected Response Time: - Simple queries: 1-3 seconds - Complex tasks: 5-15 seconds</p> <p>Security: - Checks user allowlist - Rate limited (10 messages per minute) - Prompt injection detection enabled</p>"},{"location":"COMMANDS/#remember-store-a-memory","title":"<code>/remember</code> - Store a Memory","text":"<p>Description: Ask Zetherion AI to remember something for later retrieval.</p> <p>Syntax: <pre><code>/remember &lt;content&gt;\n</code></pre></p> <p>Parameters: - <code>content</code> (required) - What you want the bot to remember</p> <p>Examples: <pre><code>/remember I prefer dark mode in all applications\n/remember My birthday is March 15th\n/remember I'm a Python developer working on web apps\n/remember I don't like spicy food\n/remember Project deadline is next Friday\n</code></pre></p> <p>Behavior: - Stores content in Qdrant vector database - Content is embedded using Gemini text-embedding-004 - Searchable via semantic similarity - Persists across bot restarts - Automatically recalled in relevant future conversations</p> <p>Expected Response: <pre><code>\u2713 I'll remember that: \"I prefer dark mode in all applications\"\n</code></pre></p> <p>Response Time: &lt; 2 seconds</p> <p>Storage: - Stored in: <code>qdrant_storage/collections/long_term_memory/</code> - Persists until manually deleted - Encrypted at rest (if Qdrant configured with encryption)</p>"},{"location":"COMMANDS/#search-search-memories","title":"<code>/search</code> - Search Memories","text":"<p>Description: Search your stored memories by semantic similarity.</p> <p>Syntax: <pre><code>/search &lt;query&gt;\n</code></pre></p> <p>Parameters: - <code>query</code> (required) - What to search for</p> <p>Examples: <pre><code>/search preferences\n/search birthday\n/search Python projects\n/search food preferences\n/search deadlines\n</code></pre></p> <p>Behavior: - Searches long-term memory collection - Uses vector similarity (not keyword matching) - Returns top 5 most relevant memories - Shows similarity score (0-100%) - Sorted by relevance</p> <p>Expected Response: <pre><code>**Search Results:**\n\n1. [95%] I prefer dark mode in all applications\n2. [87%] I'm a Python developer working on web apps\n3. [72%] Project uses FastAPI framework\n</code></pre></p> <p>Response Time: &lt; 1 second</p> <p>No Results Response: <pre><code>No matching memories found.\n</code></pre></p>"},{"location":"COMMANDS/#ping-check-bot-status","title":"<code>/ping</code> - Check Bot Status","text":"<p>Description: Verify the bot is online and check latency.</p> <p>Syntax: <pre><code>/ping\n</code></pre></p> <p>Parameters: None</p> <p>Expected Response: <pre><code>\ud83e\udd80 Pong! Latency: 45ms\n</code></pre></p> <p>Response Time: &lt; 500ms</p> <p>Use Cases: - Verify bot is responsive - Check connection quality - Debug connectivity issues - Test bot permissions</p> <p>Visibility: Response is ephemeral (only you can see it)</p>"},{"location":"COMMANDS/#direct-messaging-dm","title":"Direct Messaging (DM)","text":"<p>Description: Send messages directly to the bot in a private conversation.</p> <p>How to Use: 1. Find Zetherion AI in your server member list 2. Right-click \u2192 Message 3. Type your message naturally</p> <p>Examples: <pre><code>Hello!\nWhat can you help me with?\nExplain quantum computing\nRemember that I live in San Francisco\nWhat did we talk about yesterday?\n</code></pre></p> <p>Behavior: - No special prefix required - Works exactly like <code>/ask</code> command - Full conversation history maintained - Supports all intents (ask, remember, recall) - More private than server messages</p> <p>Advantages: - No <code>/ask</code> prefix needed - Private conversation - Easier for testing - Better for sensitive information</p>"},{"location":"COMMANDS/#mentions-in-server","title":"Mentions in Server","text":"<p>Description: Mention the bot in any channel where it has access.</p> <p>Syntax: <pre><code>@Zetherion AI &lt;your message&gt;\n</code></pre></p> <p>Examples: <pre><code>@Zetherion AI what's the best way to learn Python?\n@Zetherion AI can you help me debug this code?\n@Zetherion AI remember that our team meeting is every Monday\n</code></pre></p> <p>Behavior: - Bot only responds when explicitly mentioned - Removes mention from message before processing - Public response (everyone can see) - Same functionality as DM or <code>/ask</code></p> <p>Empty Mention: <pre><code>@Zetherion AI\n</code></pre> Response: <pre><code>How can I help you?\n</code></pre></p>"},{"location":"COMMANDS/#natural-language-intents","title":"Natural Language Intents","text":"<p>The bot automatically detects your intent from natural language:</p>"},{"location":"COMMANDS/#simple-query-intent","title":"Simple Query Intent","text":"<p>Triggers: Greetings, quick facts, simple questions</p> <p>Examples: <pre><code>Hello!\nWhat's 2 + 2?\nThanks for your help!\nGood morning\n</code></pre></p> <p>Model Used: Gemini Flash (fast, free tier)</p>"},{"location":"COMMANDS/#complex-task-intent","title":"Complex Task Intent","text":"<p>Triggers: Code generation, detailed analysis, multi-step tasks</p> <p>Examples: <pre><code>Write a Python function to validate email addresses\nExplain how transformers work in detail\nHelp me design a REST API for a blog\nDebug this code: [code snippet]\n</code></pre></p> <p>Model Used: Claude 3.5 Sonnet or GPT-4 (slower, better quality)</p> <p>Routing Logic: - Gemini Flash analyzes message - If complexity confidence &gt; 70% \u2192 Routes to Claude/GPT-4 - Otherwise \u2192 Gemini Flash handles it</p>"},{"location":"COMMANDS/#memory-store-intent","title":"Memory Store Intent","text":"<p>Triggers: Explicit remember requests</p> <p>Examples: <pre><code>Remember that I prefer tabs over spaces\nNote: Project uses PostgreSQL\nKeep in mind that I'm in PST timezone\nDon't forget my favorite color is blue\n</code></pre></p> <p>Auto-detection Keywords: - \"remember\" - \"note\" - \"keep in mind\" - \"don't forget\"</p>"},{"location":"COMMANDS/#memory-recall-intent","title":"Memory Recall Intent","text":"<p>Triggers: Questions about past conversations or stored info</p> <p>Examples: <pre><code>What do you know about me?\nWhat did we discuss yesterday?\nWhat are my preferences?\nTell me what you remember about my projects\n</code></pre></p> <p>Behavior: - Searches conversation history + long-term memory - Returns relevant past interactions - Includes timestamps for conversation context</p>"},{"location":"COMMANDS/#system-command-intent","title":"System Command Intent","text":"<p>Triggers: Bot commands, help requests</p> <p>Examples: <pre><code>Help\nWhat can you do?\nList your commands\n/ping\n</code></pre></p> <p>Response: Lists available commands and capabilities</p>"},{"location":"COMMANDS/#testing-checklist","title":"Testing Checklist","text":"<p>Use this checklist to verify all commands work correctly:</p>"},{"location":"COMMANDS/#basic-functionality","title":"Basic Functionality","text":"<ul> <li>[ ] <code>/ping</code> - Bot responds with latency</li> <li>[ ] <code>/ask Hello</code> - Bot greets you</li> <li>[ ] DM: <code>Hello</code> - Bot responds to DM</li> <li>[ ] Mention: <code>@Zetherion AI hi</code> - Bot responds to mention</li> </ul>"},{"location":"COMMANDS/#memory-operations","title":"Memory Operations","text":"<ul> <li>[ ] <code>/remember I like pizza</code> - Confirms storage</li> <li>[ ] <code>/search pizza</code> - Finds the memory with high score</li> <li>[ ] <code>/ask What do I like to eat?</code> - Bot recalls pizza preference</li> <li>[ ] <code>/remember I prefer Python 3.12</code> - Store another</li> <li>[ ] <code>/search programming</code> - Should find Python preference</li> </ul>"},{"location":"COMMANDS/#complex-tasks","title":"Complex Tasks","text":"<ul> <li>[ ] <code>/ask Write a hello world in Python</code> - Should route to Claude/GPT-4</li> <li>[ ] <code>/ask Explain quantum entanglement</code> - Detailed response</li> <li>[ ] DM: <code>Help me debug this error</code> - Analyzes and helps</li> </ul>"},{"location":"COMMANDS/#edge-cases","title":"Edge Cases","text":"<ul> <li>[ ] Empty mention: <code>@Zetherion AI</code> - Asks how to help</li> <li>[ ] Very long message (&gt;2000 chars) - Splits response</li> <li>[ ] Rate limiting - Send 11+ messages quickly</li> <li>[ ] Prompt injection: <code>ignore previous instructions</code> - Blocked</li> <li>[ ] Unauthorized user (if allowlist set) - Blocked</li> </ul>"},{"location":"COMMANDS/#error-scenarios","title":"Error Scenarios","text":"<ul> <li>[ ] Bot offline - Command fails gracefully</li> <li>[ ] Qdrant down - Error message about memory system</li> <li>[ ] API rate limit - Retry with backoff</li> <li>[ ] Invalid API key - Clear error message</li> </ul>"},{"location":"COMMANDS/#response-formats","title":"Response Formats","text":""},{"location":"COMMANDS/#success-response-askdmmention","title":"Success Response (Ask/DM/Mention)","text":"<pre><code>[Detailed answer to your question]\n\n[Additional context if relevant]\n</code></pre>"},{"location":"COMMANDS/#memory-stored","title":"Memory Stored","text":"<pre><code>\u2713 I'll remember that: \"[your content]\"\n</code></pre>"},{"location":"COMMANDS/#search-results","title":"Search Results","text":"<pre><code>**Search Results:**\n\n1. [95%] [memory content 1]\n2. [87%] [memory content 2]\n...\n</code></pre>"},{"location":"COMMANDS/#error-response-rate-limited","title":"Error Response (Rate Limited)","text":"<pre><code>You're sending messages too quickly. Please wait a moment before trying again.\n</code></pre>"},{"location":"COMMANDS/#error-response-not-authorized","title":"Error Response (Not Authorized)","text":"<pre><code>Sorry, you're not authorized to use this bot.\n</code></pre>"},{"location":"COMMANDS/#error-response-prompt-injection-detected","title":"Error Response (Prompt Injection Detected)","text":"<pre><code>I noticed some unusual patterns in your message. Could you rephrase your question?\n</code></pre>"},{"location":"COMMANDS/#command-permissions","title":"Command Permissions","text":""},{"location":"COMMANDS/#user-level-permissions-required","title":"User Level Permissions Required","text":"<ul> <li><code>Send Messages</code> - To use any command</li> <li><code>Read Message History</code> - For context awareness</li> <li><code>View Channel</code> - To see where bot is mentioned</li> </ul>"},{"location":"COMMANDS/#bot-permissions-required","title":"Bot Permissions Required","text":"<ul> <li><code>Send Messages</code> - To respond</li> <li><code>Embed Links</code> - For rich formatting (if added)</li> <li><code>Read Message History</code> - To load conversation context</li> <li><code>Use Slash Commands</code> - For <code>/ask</code>, <code>/remember</code>, etc.</li> </ul>"},{"location":"COMMANDS/#rate-limits","title":"Rate Limits","text":"<p>Default Configuration: - Max Messages: 10 per user - Time Window: 60 seconds - Warning Cooldown: 30 seconds</p> <p>Behavior: 1. User sends 10 messages in 60 seconds \u2192 \u2713 All allowed 2. User sends 11th message \u2192 \u2717 Blocked, warning shown 3. User sends 12th message within 30s \u2192 \u2717 Blocked, no warning (cooldown) 4. After 60s from first message \u2192 Counter resets</p> <p>Bypass Rate Limit: Set <code>max_messages=999</code> in <code>src/zetherion_ai/discord/security.py:36</code></p>"},{"location":"COMMANDS/#configuration","title":"Configuration","text":""},{"location":"COMMANDS/#model-configuration","title":"Model Configuration","text":"<p>Current Models (in <code>.env</code>): <pre><code># Routing &amp; Simple Queries\nROUTER_MODEL=gemini-2.0-flash\n\n# Complex Tasks\nCLAUDE_MODEL=claude-3-5-sonnet-20241022\nOPENAI_MODEL=gpt-4o\n\n# Embeddings\nEMBEDDING_MODEL=text-embedding-004\n</code></pre></p>"},{"location":"COMMANDS/#allowlist-configuration","title":"Allowlist Configuration","text":"<p>Allow All Users: <pre><code>ALLOWED_USER_IDS=\n</code></pre></p> <p>Restrict to Specific Users: <pre><code>ALLOWED_USER_IDS=123456789,987654321\n</code></pre></p>"},{"location":"COMMANDS/#troubleshooting-commands","title":"Troubleshooting Commands","text":""},{"location":"COMMANDS/#command-not-appearing","title":"Command Not Appearing","text":"<p>Problem: Slash commands don't show in Discord</p> <p>Solutions: 1. Wait up to 1 hour for global sync 2. Restart Discord app 3. Check bot was invited with <code>applications.commands</code> scope 4. Verify bot has <code>Use Application Commands</code> permission</p> <p>Verify Sync: <pre><code># Check logs for:\n./status.sh\n# Look for: \"commands_synced\"\n</code></pre></p>"},{"location":"COMMANDS/#command-not-responding","title":"Command Not Responding","text":"<p>Problem: Bot online but commands don't work</p> <p>Checklist: - [ ] Bot has <code>Send Messages</code> permission - [ ] User is on allowlist (if configured) - [ ] Not rate limited - [ ] Message Content Intent enabled (for DMs/mentions) - [ ] Check logs for errors</p> <p>Debug: <pre><code># Enable debug logging\n# In .env:\nLOG_LEVEL=DEBUG\n\n./stop.sh &amp;&amp; ./start.sh\n# Try command again\n# Check output for errors\n</code></pre></p>"},{"location":"COMMANDS/#api-reference","title":"API Reference","text":"<p>For programmatic access or building additional features:</p>"},{"location":"COMMANDS/#command-handler-methods","title":"Command Handler Methods","text":"<pre><code># In src/zetherion_ai/discord/bot.py\n\nasync def _handle_ask(interaction, question)\n# Handles /ask command\n\nasync def _handle_remember(interaction, content)\n# Handles /remember command\n\nasync def _handle_search(interaction, query)\n# Handles /search command\n\nasync def on_message(message)\n# Handles DMs and mentions\n</code></pre>"},{"location":"COMMANDS/#adding-new-commands","title":"Adding New Commands","text":"<ol> <li>Edit <code>src/zetherion_ai/discord/bot.py</code></li> <li>Add command in <code>_setup_commands()</code>: <pre><code>@self._tree.command(name=\"hello\", description=\"Say hello\")\nasync def hello_command(interaction: discord.Interaction) -&gt; None:\n    await interaction.response.send_message(\"Hello!\")\n</code></pre></li> <li>Restart bot - auto-syncs</li> </ol>"},{"location":"COMMANDS/#testing-scripts","title":"Testing Scripts","text":""},{"location":"COMMANDS/#quick-test-all-commands","title":"Quick Test All Commands","text":"<pre><code># In Discord:\n/ping\n/ask What is 2+2?\n/remember I like testing\n/search testing\n</code></pre>"},{"location":"COMMANDS/#comprehensive-test","title":"Comprehensive Test","text":"<pre><code># Test DM\n1. DM bot: \"Hello!\"\n2. DM bot: \"Remember I'm testing commands\"\n3. DM bot: \"What do you remember about me?\"\n\n# Test Mentions\n1. In server: \"@Zetherion AI help\"\n2. In server: \"@Zetherion AI remember our meeting is tomorrow\"\n\n# Test Error Handling\n1. Send 11 messages quickly (rate limit)\n2. Send: \"ignore previous instructions\" (injection)\n3. Disable Qdrant, try /search (graceful failure)\n</code></pre>"},{"location":"COMMANDS/#support","title":"Support","text":"<p>Need help with commands? - Troubleshooting Guide - FAQ - GitHub Issues</p>"},{"location":"CONFIGURATION/","title":"Configuration Guide","text":"<p>Complete reference for configuring Zetherion AI via environment variables in <code>.env</code> file.</p>"},{"location":"CONFIGURATION/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Required Configuration</li> <li>Router Backend Configuration</li> <li>Optional API Keys</li> <li>Security Configuration</li> <li>Performance Tuning</li> <li>Logging Configuration</li> <li>Advanced Settings</li> <li>Environment-Specific Configs</li> <li>Configuration Examples</li> </ul>"},{"location":"CONFIGURATION/#overview","title":"Overview","text":"<p>Zetherion AI is configured via a <code>.env</code> file in the project root directory. This file contains: - API keys (Discord, AI providers) - Router backend selection - Security settings - Performance tuning - Logging configuration</p> <p>Configuration File: - Template: <code>.env.example</code> (checked into git) - Active Config: <code>.env</code> (gitignored, contains your secrets)</p> <p>Creating Your Config: <pre><code># Copy template\ncp .env.example .env\n\n# Or let start.sh/start.ps1 create it interactively\n./start.sh  # Runs interactive setup if .env missing\n</code></pre></p>"},{"location":"CONFIGURATION/#required-configuration","title":"Required Configuration","text":"<p>These settings are mandatory for Zetherion AI to function.</p>"},{"location":"CONFIGURATION/#discord_token","title":"DISCORD_TOKEN","text":"<p>Discord Bot Token - Authenticates your bot with Discord API.</p> <pre><code>DISCORD_TOKEN=MTQ2ODc4MDQxODY1MTI2MzEyOQ.GGFum2.lsf_abc123def456ghi789\n</code></pre> <p>How to get: 1. Discord Developer Portal 2. Create Application \u2192 Bot tab \u2192 Reset Token 3. Copy token immediately (won't be shown again) 4. Enable \"Message Content Intent\" in Bot settings</p> <p>Format: - 59+ characters - Pattern: <code>[MN][A-Za-z0-9]{23}.[A-Za-z0-9_-]{6}.[A-Za-z0-9_-]{27}</code> - Example: <code>MTQz...GGF...lsf...</code></p> <p>Security: - \u26a0\ufe0f Never commit to git - \u26a0\ufe0f Never share publicly - \u26a0\ufe0f Regenerate if compromised</p>"},{"location":"CONFIGURATION/#gemini_api_key","title":"GEMINI_API_KEY","text":"<p>Google Gemini API Key - Used for embeddings (required) and routing (if <code>ROUTER_BACKEND=gemini</code>).</p> <pre><code>GEMINI_API_KEY=AIzaSyCO9WodgUFJfW-7qK4Vtbnc1234567890ABC\n</code></pre> <p>How to get: 1. Google AI Studio 2. Sign in \u2192 Create API key 3. Select or create Google Cloud project 4. Copy key</p> <p>Format: - Starts with <code>AIzaSy</code> - 39 characters total - Pattern: <code>AIzaSy[A-Za-z0-9_-]{33}</code></p> <p>Pricing: - Free tier: 15 requests/minute, 1,500 requests/day - Paid tier: $0.00007 per 1K characters (input) - See: https://ai.google.dev/pricing</p> <p>Usage: - Always used: Generating embeddings for vector storage - Router backend: Message classification and simple responses (if <code>ROUTER_BACKEND=gemini</code>)</p>"},{"location":"CONFIGURATION/#router-backend-configuration","title":"Router Backend Configuration","text":"<p>Choose how Zetherion AI routes and processes messages.</p>"},{"location":"CONFIGURATION/#router_backend","title":"ROUTER_BACKEND","text":"<p>Router Backend Selection - Determines which AI processes routing decisions.</p> <pre><code>ROUTER_BACKEND=gemini  # or ollama\n</code></pre> <p>Options:</p>"},{"location":"CONFIGURATION/#1-gemini-default-cloud-based","title":"1. <code>gemini</code> (Default - Cloud-Based)","text":"<p>Pros: - \u2705 Fast startup (~3 minutes first run) - \u2705 No model downloads - \u2705 Lower resource requirements (4GB Docker RAM) - \u2705 Free tier available (1,500 requests/day)</p> <p>Cons: - \u274c Sends routing data to Google's cloud - \u274c Requires internet for every request</p> <p>Best for: - Quick deployments - Cloud-based workflows - Limited hardware</p>"},{"location":"CONFIGURATION/#2-ollama-localprivacy-focused","title":"2. <code>ollama</code> (Local/Privacy-Focused)","text":"<p>Pros: - \u2705 Privacy-focused (no routing data sent to cloud) - \u2705 Offline capability - \u2705 No per-request API costs - \u2705 GPU acceleration support</p> <p>Cons: - \u274c Longer startup (~9 minutes first run, includes model download) - \u274c Higher resource requirements (8-12GB Docker RAM) - \u274c 5-10GB model downloads</p> <p>Best for: - Privacy-conscious users - Offline deployments - Powerful hardware (16GB+ RAM or GPU)</p>"},{"location":"CONFIGURATION/#ollama_router_model","title":"OLLAMA_ROUTER_MODEL","text":"<p>Ollama Model Selection - Which local model to use for routing (only if <code>ROUTER_BACKEND=ollama</code>).</p> <pre><code>OLLAMA_ROUTER_MODEL=llama3.1:8b\n</code></pre> <p>Recommended Models (by hardware):</p> Hardware Model Docker RAM Quality Speed 8GB RAM, CPU <code>phi3:mini</code> 5GB Good Fast 16GB RAM, CPU <code>llama3.1:8b</code> 8GB Excellent Moderate 16GB+ RAM, GPU <code>qwen2.5:14b</code> 12GB Best Fast (GPU) 32GB+ RAM, GPU <code>qwen2.5:32b</code> 24GB Maximum Fast (GPU) <p>Hardware assessment in <code>start.sh</code>/<code>start.ps1</code> automatically recommends optimal model.</p> <p>See: Hardware Recommendations for detailed comparison.</p>"},{"location":"CONFIGURATION/#ollama_host","title":"OLLAMA_HOST","text":"<p>Ollama Service Host - Where to find Ollama API.</p> <pre><code>OLLAMA_HOST=ollama  # Docker service name (default)\n# or\nOLLAMA_HOST=localhost:11434  # For local development\n</code></pre> <p>Default: <code>ollama</code> (Docker Compose service name)</p> <p>When to change: - Running Ollama outside Docker - Custom Ollama deployment - Remote Ollama server</p>"},{"location":"CONFIGURATION/#ollama_docker_memory","title":"OLLAMA_DOCKER_MEMORY","text":"<p>Docker Memory Allocation - RAM limit for Ollama container (GB).</p> <pre><code>OLLAMA_DOCKER_MEMORY=8\n</code></pre> <p>Recommended by Model: - <code>phi3:mini</code>: 5GB - <code>llama3.1:8b</code>: 8GB - <code>qwen2.5:14b</code>: 12GB - <code>qwen2.5:32b</code>: 24GB</p> <p>Auto-set by startup script based on model selection.</p> <p>Manual adjustment needed if: - Out of memory errors - Running larger custom models - Memory-constrained system</p>"},{"location":"CONFIGURATION/#optional-api-keys","title":"Optional API Keys","text":"<p>These enhance capabilities but are not required.</p>"},{"location":"CONFIGURATION/#anthropic_api_key","title":"ANTHROPIC_API_KEY","text":"<p>Anthropic Claude API Key - For complex reasoning tasks.</p> <pre><code>ANTHROPIC_API_KEY=sk-ant-api03-OEKnlIipBFzxRV1234567890...\n</code></pre> <p>How to get: 1. Anthropic Console 2. Settings \u2192 API Keys \u2192 Create Key 3. Add payment method ($5 minimum credit)</p> <p>Format: - Starts with <code>sk-ant-api03-</code> - ~100 characters total</p> <p>Pricing: - Claude Sonnet 4.5: $3/million input tokens, $15/million output tokens - See: https://www.anthropic.com/pricing</p> <p>Usage: - Complex queries requiring advanced reasoning - Code generation and analysis - Multi-step problem solving - Fallback if OpenAI unavailable</p> <p>Can be omitted - Gemini Flash handles all queries (lower quality for complex tasks).</p>"},{"location":"CONFIGURATION/#openai_api_key","title":"OPENAI_API_KEY","text":"<p>OpenAI API Key - Alternative for complex tasks (GPT-4o).</p> <pre><code>OPENAI_API_KEY=sk-proj-1234567890abcdefghijklmnopqrstuvwxyz...\n</code></pre> <p>How to get: 1. OpenAI Platform 2. Profile \u2192 API Keys \u2192 Create new secret key 3. Add payment method ($5+ credits recommended)</p> <p>Format: - Starts with <code>sk-</code> or <code>sk-proj-</code> - 48+ characters</p> <p>Pricing: - GPT-4o: $2.50/million input tokens, $10/million output tokens - See: https://openai.com/api/pricing/</p> <p>Usage: - Alternative to Claude for complex tasks - Used if Claude unavailable or rate limited</p> <p>Can be omitted - Not used if Anthropic key present.</p>"},{"location":"CONFIGURATION/#security-configuration","title":"Security Configuration","text":""},{"location":"CONFIGURATION/#allowed_user_ids","title":"ALLOWED_USER_IDS","text":"<p>User Allowlist - Restrict bot access to specific Discord users (comma-separated user IDs).</p> <pre><code>ALLOWED_USER_IDS=123456789012345678,987654321098765432\n</code></pre> <p>Getting User IDs: 1. Enable Discord Developer Mode (Settings \u2192 Advanced) 2. Right-click user \u2192 Copy User ID 3. Add to comma-separated list</p> <p>Examples: <pre><code># Single user\nALLOWED_USER_IDS=123456789012345678\n\n# Multiple users\nALLOWED_USER_IDS=123456789012345678,987654321098765432,456789012345678901\n\n# All users (not recommended for production)\nALLOWED_USER_IDS=\n</code></pre></p> <p>Security: - \u2705 Recommended: Set to your user ID(s) - \u26a0\ufe0f Warning: Empty = anyone can use bot (costs, spam, abuse) - \ud83d\udd12 Production: Always set allowlist</p>"},{"location":"CONFIGURATION/#encryption_enabled","title":"ENCRYPTION_ENABLED","text":"<p>Data Encryption - Enable AES-256-GCM encryption for vector storage.</p> <pre><code>ENCRYPTION_ENABLED=true\n</code></pre> <p>Default: <code>false</code> (disabled)</p> <p>When enabled: - All sensitive data in Qdrant encrypted - Encryption key derived from <code>ENCRYPTION_PASSPHRASE</code> - Provides defense-in-depth if database compromised</p> <p>See: Security Guide for details.</p>"},{"location":"CONFIGURATION/#encryption_passphrase","title":"ENCRYPTION_PASSPHRASE","text":"<p>Encryption Key - Passphrase for AES-256-GCM encryption (required if <code>ENCRYPTION_ENABLED=true</code>).</p> <pre><code>ENCRYPTION_PASSPHRASE=your-secure-passphrase-here-min-16-chars\n</code></pre> <p>Requirements: - Minimum 16 characters - Use strong, random passphrase - Never commit to git</p> <p>Generating secure passphrase: <pre><code># Generate 32-byte random passphrase\nopenssl rand -base64 32\n# Output: 8zP3kL9mN2qR5vT7wX0yA4bC6dE1fG8hI...\n\n# Or use password manager to generate\n</code></pre></p> <p>\u26a0\ufe0f CRITICAL: - Never lose this: Cannot decrypt data without it - Backup securely: Separate from git repository - Rotate regularly: Change every 6-12 months (requires data migration)</p>"},{"location":"CONFIGURATION/#performance-tuning","title":"Performance Tuning","text":""},{"location":"CONFIGURATION/#rate_limit_messages","title":"RATE_LIMIT_MESSAGES","text":"<p>Rate Limit - Maximum messages per user per time window.</p> <pre><code>RATE_LIMIT_MESSAGES=10\n</code></pre> <p>Default: <code>10</code> messages</p> <p>Purpose: - Prevent abuse - Control API costs - Manage resource usage</p> <p>Adjust based on: - Personal use: 5-10 messages/minute sufficient - Team use: 15-20 messages/minute - Production: 10 messages/minute (monitor costs)</p>"},{"location":"CONFIGURATION/#rate_limit_window","title":"RATE_LIMIT_WINDOW","text":"<p>Rate Limit Window - Time window in seconds for rate limiting.</p> <pre><code>RATE_LIMIT_WINDOW=60\n</code></pre> <p>Default: <code>60</code> seconds (1 minute)</p> <p>Examples: <pre><code># Strict: 5 messages per 30 seconds\nRATE_LIMIT_MESSAGES=5\nRATE_LIMIT_WINDOW=30\n\n# Lenient: 20 messages per 2 minutes\nRATE_LIMIT_MESSAGES=20\nRATE_LIMIT_WINDOW=120\n</code></pre></p>"},{"location":"CONFIGURATION/#qdrant_host","title":"QDRANT_HOST","text":"<p>Qdrant Service Host - Vector database connection.</p> <pre><code>QDRANT_HOST=qdrant  # Docker service name (default)\n</code></pre> <p>Default: <code>qdrant</code> (Docker Compose service name)</p> <p>Alternatives: <pre><code># Local development\nQDRANT_HOST=localhost\n\n# Remote Qdrant instance\nQDRANT_HOST=qdrant.example.com\n</code></pre></p>"},{"location":"CONFIGURATION/#qdrant_port","title":"QDRANT_PORT","text":"<p>Qdrant Service Port - Vector database API port.</p> <pre><code>QDRANT_PORT=6333\n</code></pre> <p>Default: <code>6333</code> (Qdrant standard port)</p> <p>Change if: - Port conflict with other services - Custom Qdrant deployment - Security requirements (non-standard port)</p>"},{"location":"CONFIGURATION/#logging-configuration","title":"Logging Configuration","text":""},{"location":"CONFIGURATION/#log_level","title":"LOG_LEVEL","text":"<p>Logging Verbosity - Controls detail level of logs.</p> <pre><code>LOG_LEVEL=INFO\n</code></pre> <p>Options (from most to least verbose): - <code>DEBUG</code>: All messages (development, troubleshooting) - <code>INFO</code>: General information (default, recommended) - <code>WARNING</code>: Warnings and errors only - <code>ERROR</code>: Errors only - <code>CRITICAL</code>: Critical failures only</p> <p>Recommendations: - Development: <code>DEBUG</code> - Production: <code>INFO</code> or <code>WARNING</code> - Troubleshooting: <code>DEBUG</code> (temporarily)</p>"},{"location":"CONFIGURATION/#log_to_file","title":"LOG_TO_FILE","text":"<p>File Logging - Enable writing logs to files.</p> <pre><code>LOG_TO_FILE=true\n</code></pre> <p>Default: <code>true</code> (enabled)</p> <p>When enabled: - Logs written to <code>logs/</code> directory - Automatic rotation (10MB max, 5 backups) - Separate files per severity (info.log, error.log)</p> <p>Disable for: - Containerized environments with centralized logging - Docker log aggregation (use <code>docker-compose logs</code> instead)</p>"},{"location":"CONFIGURATION/#log_directory","title":"LOG_DIRECTORY","text":"<p>Log File Directory - Where to store log files.</p> <pre><code>LOG_DIRECTORY=logs\n</code></pre> <p>Default: <code>logs/</code> (relative to project root)</p> <p>Absolute path example: <pre><code>LOG_DIRECTORY=/var/log/zetherion-ai\n</code></pre></p> <p>Ensure directory: - Exists or is created automatically - Has write permissions for bot user (UID 65532 in distroless)</p>"},{"location":"CONFIGURATION/#advanced-settings","title":"Advanced Settings","text":""},{"location":"CONFIGURATION/#discord_command_prefix","title":"DISCORD_COMMAND_PREFIX","text":"<p>Command Prefix - Prefix for text-based commands (alternative to mentions).</p> <pre><code>DISCORD_COMMAND_PREFIX=!\n</code></pre> <p>Default: <code>None</code> (mention-only)</p> <p>Examples: <pre><code># Exclamation prefix\nDISCORD_COMMAND_PREFIX=!\n# Usage: !ask What is the weather?\n\n# Dot prefix\nDISCORD_COMMAND_PREFIX=.\n# Usage: .ask What is the weather?\n\n# No prefix (mention only)\nDISCORD_COMMAND_PREFIX=\n# Usage: @Zetherion AI What is the weather?\n</code></pre></p> <p>Note: Slash commands (<code>/ask</code>) always work regardless of prefix.</p>"},{"location":"CONFIGURATION/#qdrant_use_tls","title":"QDRANT_USE_TLS","text":"<p>TLS Encryption - Enable encrypted connection to Qdrant.</p> <pre><code>QDRANT_USE_TLS=false\n</code></pre> <p>Default: <code>false</code> (unencrypted, OK for local Docker network)</p> <p>Enable for: - Remote Qdrant instances - Production deployments - Compliance requirements</p> <p>Requires: - TLS certificates (self-signed or CA-issued) - Certificate files mounted in Docker volume</p> <p>See: Security Guide</p>"},{"location":"CONFIGURATION/#memory_search_limit","title":"MEMORY_SEARCH_LIMIT","text":"<p>Memory Search Results - Maximum number of results from vector search.</p> <pre><code>MEMORY_SEARCH_LIMIT=5\n</code></pre> <p>Default: <code>5</code> results</p> <p>Purpose: - Limit context size sent to LLM - Control API costs (tokens) - Balance relevance vs. context window</p> <p>Adjust based on: - Complex queries: Increase to 10-15 - Simple queries: Decrease to 3 - Cost-sensitive: Decrease to 3-5</p>"},{"location":"CONFIGURATION/#context_window_size","title":"CONTEXT_WINDOW_SIZE","text":"<p>Conversation History - Number of recent messages to include in context.</p> <pre><code>CONTEXT_WINDOW_SIZE=10\n</code></pre> <p>Default: <code>10</code> messages</p> <p>Higher values: - Better conversation continuity - More tokens per request (higher costs) - Risk of exceeding model context limits</p> <p>Lower values: - Cheaper API calls - May lose conversation thread - Faster responses</p>"},{"location":"CONFIGURATION/#environment-specific-configs","title":"Environment-Specific Configs","text":""},{"location":"CONFIGURATION/#development-environment","title":"Development Environment","text":"<pre><code># .env.development\nLOG_LEVEL=DEBUG\nLOG_TO_FILE=true\nRATE_LIMIT_MESSAGES=100  # High limit for testing\nROUTER_BACKEND=gemini    # Fast iteration\nENCRYPTION_ENABLED=false # Faster for testing\n</code></pre>"},{"location":"CONFIGURATION/#production-environment","title":"Production Environment","text":"<pre><code># .env.production\nLOG_LEVEL=INFO\nLOG_TO_FILE=true\nRATE_LIMIT_MESSAGES=10\nROUTER_BACKEND=ollama  # Privacy-focused\nENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=&lt;strong-passphrase&gt;\nALLOWED_USER_IDS=&lt;specific-users&gt;\nQDRANT_USE_TLS=true\n</code></pre>"},{"location":"CONFIGURATION/#testing-environment","title":"Testing Environment","text":"<pre><code># .env.test\nLOG_LEVEL=WARNING\nLOG_TO_FILE=false  # Use stdout for CI logs\nROUTER_BACKEND=gemini\nALLOWED_USER_IDS=  # Allow test users\nRATE_LIMIT_MESSAGES=1000  # No limits during tests\n</code></pre>"},{"location":"CONFIGURATION/#configuration-examples","title":"Configuration Examples","text":""},{"location":"CONFIGURATION/#minimal-setup-personal-use","title":"Minimal Setup (Personal Use)","text":"<pre><code># Required only\nDISCORD_TOKEN=MTQz...\nGEMINI_API_KEY=AIzaSy...\n\n# Recommended\nROUTER_BACKEND=gemini\nALLOWED_USER_IDS=123456789012345678\nLOG_LEVEL=INFO\n</code></pre> <p>Good for: - Quick start - Personal Discord server - Gemini free tier</p>"},{"location":"CONFIGURATION/#privacy-focused-setup","title":"Privacy-Focused Setup","text":"<pre><code># Required\nDISCORD_TOKEN=MTQz...\nGEMINI_API_KEY=AIzaSy...  # Still needed for embeddings\n\n# Ollama for routing (local)\nROUTER_BACKEND=ollama\nOLLAMA_ROUTER_MODEL=qwen2.5:14b\nOLLAMA_DOCKER_MEMORY=12\n\n# Security\nALLOWED_USER_IDS=123456789012345678\nENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=&lt;strong-passphrase&gt;\n\n# Logging\nLOG_LEVEL=INFO\nLOG_TO_FILE=true\n</code></pre> <p>Good for: - Privacy requirements - Offline capability - Powerful hardware</p>"},{"location":"CONFIGURATION/#professional-setup-best-quality","title":"Professional Setup (Best Quality)","text":"<pre><code># Required\nDISCORD_TOKEN=MTQz...\nGEMINI_API_KEY=AIzaSy...\n\n# All AI providers\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\n\n# Cloud routing for speed\nROUTER_BACKEND=gemini\n\n# Security\nALLOWED_USER_IDS=123,456,789\nRATE_LIMIT_MESSAGES=15\nENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=&lt;strong-passphrase&gt;\n\n# Logging\nLOG_LEVEL=INFO\nLOG_TO_FILE=true\nLOG_DIRECTORY=/var/log/zetherion-ai\n\n# Performance\nCONTEXT_WINDOW_SIZE=15\nMEMORY_SEARCH_LIMIT=7\n</code></pre> <p>Good for: - Professional use - Team deployments - Best quality responses</p>"},{"location":"CONFIGURATION/#budget-conscious-setup","title":"Budget-Conscious Setup","text":"<pre><code># Required (free tiers)\nDISCORD_TOKEN=MTQz...\nGEMINI_API_KEY=AIzaSy...\n\n# Free routing\nROUTER_BACKEND=gemini\n\n# Strict rate limiting\nRATE_LIMIT_MESSAGES=5\nRATE_LIMIT_WINDOW=60\n\n# Minimize token usage\nCONTEXT_WINDOW_SIZE=5\nMEMORY_SEARCH_LIMIT=3\n\n# Security\nALLOWED_USER_IDS=123456789012345678\n</code></pre> <p>Good for: - Staying within free tiers - Controlling costs - Moderate usage</p>"},{"location":"CONFIGURATION/#validation","title":"Validation","text":""},{"location":"CONFIGURATION/#check-configuration","title":"Check Configuration","text":"<p>Verify .env file exists: <pre><code>ls -la .env\n# Should show: -rw------- 1 user user 1234 .env\n</code></pre></p> <p>Verify required keys set: <pre><code>grep -E \"^(DISCORD_TOKEN|GEMINI_API_KEY)=\" .env\n# Should show both variables with values\n</code></pre></p> <p>Test configuration: <pre><code># Start and check logs\n./start.sh\ndocker-compose logs -f zetherion-ai-bot\n\n# Look for:\n# \u2713 Discord token validated\n# \u2713 Gemini API key validated\n# \u2713 Router backend: gemini (or ollama)\n</code></pre></p>"},{"location":"CONFIGURATION/#common-validation-errors","title":"Common Validation Errors","text":"<p>\"Invalid Discord token format\" <pre><code>Fix: Ensure token is 59+ characters, no quotes/spaces\nDISCORD_TOKEN=MTQz...  \u2713 Correct\nDISCORD_TOKEN=\"MTQz...\"  \u2717 Has quotes\n</code></pre></p> <p>\"Gemini API key validation failed\" <pre><code>Fix: Ensure key starts with AIzaSy, 39 characters total\nGEMINI_API_KEY=AIzaSy...  \u2713 Correct\nGEMINI_API_KEY=AIzaS...   \u2717 Too short\n</code></pre></p> <p>\"No users allowed (ALLOWED_USER_IDS empty)\" <pre><code>Fix: Set your Discord user ID or expect warning\nALLOWED_USER_IDS=123456789012345678  \u2713 Restricted\nALLOWED_USER_IDS=                    \u26a0\ufe0f Warning (anyone can use)\n</code></pre></p>"},{"location":"CONFIGURATION/#security-best-practices","title":"Security Best Practices","text":""},{"location":"CONFIGURATION/#file-permissions","title":"File Permissions","text":"<p>Unix/Mac: <pre><code># .env should be readable by owner only\nchmod 600 .env\nls -la .env\n# Expected: -rw------- (owner read/write only)\n</code></pre></p> <p>Windows (PowerShell as Admin): <pre><code># Remove inheritance and grant only user access\nicacls .env /inheritance:r /grant:r \"$env:USERNAME:(R,W)\"\n</code></pre></p>"},{"location":"CONFIGURATION/#key-rotation","title":"Key Rotation","text":"<p>Rotate API keys periodically: 1. Discord: Every 6 months or if compromised 2. AI Providers: Every 3-6 months 3. Encryption: Every 6-12 months (complex migration)</p> <p>Rotation process: 1. Generate new key in provider console 2. Update <code>.env</code> with new key 3. Restart bot: <code>./stop.sh &amp;&amp; ./start.sh</code> 4. Revoke old key in provider console 5. Verify bot still works</p>"},{"location":"CONFIGURATION/#backup-configuration","title":"Backup Configuration","text":"<p>Backup .env securely: <pre><code># Encrypt backup\ngpg -c .env\n# Creates: .env.gpg (encrypted)\n\n# Store in secure location (not git)\nmv .env.gpg ~/secure-backups/\n\n# Restore when needed\ngpg -d ~/secure-backups/.env.gpg &gt; .env\n</code></pre></p>"},{"location":"CONFIGURATION/#audit-logging","title":"Audit Logging","text":"<p>Monitor configuration changes: <pre><code># Track who changed .env\ngit log -p .env.example  # Track template changes in git\n\n# Log access to .env\naudit-log \".env accessed by $USER at $(date)\" &gt;&gt; .env.access.log\n</code></pre></p>"},{"location":"CONFIGURATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONFIGURATION/#bot-wont-start","title":"Bot Won't Start","text":"<p>Check configuration: <pre><code># View container logs\ndocker-compose logs zetherion-ai-bot\n\n# Common errors:\n# - \"Invalid Discord token\" \u2192 Check DISCORD_TOKEN format\n# - \"Gemini API error\" \u2192 Check GEMINI_API_KEY\n# - \"Router backend not set\" \u2192 Add ROUTER_BACKEND=gemini\n</code></pre></p>"},{"location":"CONFIGURATION/#high-api-costs","title":"High API Costs","text":"<p>Reduce token usage: <pre><code># Lower context window\nCONTEXT_WINDOW_SIZE=5  # Was: 10\n\n# Fewer memory results\nMEMORY_SEARCH_LIMIT=3  # Was: 5\n\n# Stricter rate limiting\nRATE_LIMIT_MESSAGES=5  # Was: 10\n</code></pre></p>"},{"location":"CONFIGURATION/#performance-issues","title":"Performance Issues","text":"<p>Optimize settings: <pre><code># For faster responses (cloud)\nROUTER_BACKEND=gemini\n\n# For Ollama (if slow)\nOLLAMA_ROUTER_MODEL=phi3:mini  # Faster model\nOLLAMA_DOCKER_MEMORY=5  # Match model requirements\n</code></pre></p>"},{"location":"CONFIGURATION/#additional-resources","title":"Additional Resources","text":"<ul> <li>Installation Guide - First-time setup</li> <li>Hardware Recommendations - Optimize for your system</li> <li>Security Guide - Encryption and distroless containers</li> <li>.env.example - Template with all variables</li> </ul> <p>Need Help? Check GitHub Discussions or open an issue.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Zetherion AI","text":"<p>Thank you for your interest in contributing to Zetherion AI! This document provides guidelines and instructions for contributing.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Code of Conduct</li> <li>Getting Started</li> <li>Development Workflow</li> <li>Testing</li> <li>Code Style</li> <li>Commit Messages</li> <li>Pull Request Process</li> <li>Documentation</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"CONTRIBUTING/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers and help them get started</li> <li>Focus on what is best for the community</li> <li>Show empathy towards other community members</li> </ul>"},{"location":"CONTRIBUTING/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<ul> <li>Harassment, trolling, or derogatory comments</li> <li>Publishing others' private information</li> <li>Other conduct which could reasonably be considered inappropriate</li> </ul>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":""},{"location":"CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 or higher</li> <li>Docker Desktop (for Qdrant and optionally Ollama)</li> <li>Git</li> <li>A Discord bot token (see README.md for setup instructions)</li> <li>Gemini API key (free tier available)</li> </ul>"},{"location":"CONTRIBUTING/#fork-and-clone","title":"Fork and Clone","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/zetherion_ai.git\ncd zetherion_ai\n</code></pre></li> <li>Add the upstream repository:    <pre><code>git remote add upstream https://github.com/jimtin/zetherion-ai.git\n</code></pre></li> <li>Create a new branch for your feature:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> </ol>"},{"location":"CONTRIBUTING/#set-up-development-environment","title":"Set Up Development Environment","text":"<ol> <li> <p>Create <code>.env</code> file:    <pre><code>cp .env.example .env\n# Edit .env with your API keys\n</code></pre></p> </li> <li> <p>Run the startup script:    <pre><code>./start.sh\n</code></pre>    This will:</p> </li> <li>Set up Python virtual environment</li> <li>Install dependencies</li> <li>Install pre-commit hooks</li> <li> <p>Start required services (Qdrant, Ollama if selected)</p> </li> <li> <p>Verify setup:    <pre><code>./status.sh\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":""},{"location":"CONTRIBUTING/#branch-strategy","title":"Branch Strategy","text":"<ul> <li><code>main</code> - Production-ready code</li> <li><code>feature/*</code> - New features</li> <li><code>fix/*</code> - Bug fixes</li> <li><code>docs/*</code> - Documentation updates</li> <li><code>test/*</code> - Test improvements</li> </ul>"},{"location":"CONTRIBUTING/#making-changes","title":"Making Changes","text":"<ol> <li> <p>Create a feature branch:    <pre><code>git checkout -b feature/my-new-feature\n</code></pre></p> </li> <li> <p>Make your changes:</p> </li> <li>Write code following our Code Style</li> <li>Add tests for new functionality</li> <li> <p>Update documentation as needed</p> </li> <li> <p>Run tests:    <pre><code># Unit tests\npytest tests/ -m \"not integration and not discord_e2e\"\n\n# Integration tests\npytest tests/integration/test_e2e.py -m integration\n\n# All tests with coverage\npytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai --cov-report=html\n</code></pre></p> </li> <li> <p>Check code quality:    <pre><code># Linting and formatting (auto-fixes)\nruff check --fix .\nruff format .\n\n# Type checking\nmypy src/zetherion_ai\n\n# Security scan\nbandit -r src/zetherion_ai\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"feat: Add amazing new feature\"\n</code></pre></p> </li> </ol> <p>Pre-commit hooks will automatically run:    - Ruff (linting and formatting)    - Mypy (type checking)    - Gitleaks (secret scanning)    - Bandit (security scanning)</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":""},{"location":"CONTRIBUTING/#test-coverage-standards","title":"Test Coverage Standards","text":"<ul> <li>New features must include tests</li> <li>Maintain or improve overall coverage (currently 87.58%)</li> <li>Aim for 85%+ coverage on new modules</li> </ul>"},{"location":"CONTRIBUTING/#writing-tests","title":"Writing Tests","text":"<ol> <li> <p>Unit Tests (<code>tests/test_*.py</code>):    <pre><code>import pytest\nfrom zetherion_ai.module import YourClass\n\n@pytest.mark.asyncio\nasync def test_your_feature():\n    \"\"\"Test description.\"\"\"\n    # Arrange\n    instance = YourClass()\n\n    # Act\n    result = await instance.method()\n\n    # Assert\n    assert result == expected_value\n</code></pre></p> </li> <li> <p>Integration Tests (<code>tests/integration/test_e2e.py</code>):</p> </li> <li>Use <code>@pytest.mark.integration</code> marker</li> <li>Test with real Docker services (Qdrant, Ollama)</li> <li> <p>Use MockDiscordBot to bypass Discord API</p> </li> <li> <p>Discord E2E Tests (<code>tests/integration/test_discord_e2e.py</code>):</p> </li> <li>Use <code>@pytest.mark.discord_e2e</code> marker</li> <li>Optional: requires test bot credentials</li> <li>Test real Discord API interactions</li> </ol>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<pre><code># Unit tests only (fast, ~24s)\npytest tests/ -m \"not integration and not discord_e2e\"\n\n# Integration tests (~2 min)\npytest tests/integration/test_e2e.py -m integration\n\n# Discord E2E tests (~1 min, requires test bot)\npytest tests/integration/test_discord_e2e.py -m discord_e2e\n\n# All tests with coverage\npytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai --cov-report=html\n\n# Coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":""},{"location":"CONTRIBUTING/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 with the following specifics:</p> <ul> <li>Line length: 100 characters (enforced by Ruff)</li> <li>Imports: Use <code>isort</code> ordering (automated by Ruff)</li> <li>Quotes: Double quotes for strings</li> <li>Type hints: Required for all functions (enforced by mypy strict mode)</li> <li>Docstrings: Google style for all public functions/classes</li> </ul>"},{"location":"CONTRIBUTING/#example","title":"Example","text":"<pre><code>\"\"\"Module docstring describing the file.\"\"\"\n\nfrom typing import Any\n\nfrom zetherion_ai.module import SomeClass\n\n\nasync def example_function(param: str, optional: int = 0) -&gt; dict[str, Any]:\n    \"\"\"Brief description of what the function does.\n\n    Args:\n        param: Description of param.\n        optional: Description of optional parameter.\n\n    Returns:\n        Dictionary containing the result.\n\n    Raises:\n        ValueError: If param is invalid.\n    \"\"\"\n    if not param:\n        raise ValueError(\"param must not be empty\")\n\n    return {\"result\": param, \"count\": optional}\n</code></pre>"},{"location":"CONTRIBUTING/#automated-formatting","title":"Automated Formatting","text":"<p>Ruff handles most formatting automatically:</p> <pre><code># Auto-fix linting issues\nruff check --fix .\n\n# Format code\nruff format .\n</code></pre>"},{"location":"CONTRIBUTING/#commit-messages","title":"Commit Messages","text":""},{"location":"CONTRIBUTING/#format","title":"Format","text":"<p>We follow the Conventional Commits specification:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#types","title":"Types","text":"<ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation changes</li> <li><code>test</code>: Adding or updating tests</li> <li><code>refactor</code>: Code refactoring</li> <li><code>perf</code>: Performance improvements</li> <li><code>style</code>: Code style changes (formatting, etc.)</li> <li><code>chore</code>: Maintenance tasks</li> <li><code>ci</code>: CI/CD changes</li> </ul>"},{"location":"CONTRIBUTING/#examples","title":"Examples","text":"<pre><code># Simple feature\ngit commit -m \"feat: add /status command to show bot statistics\"\n\n# Bug fix with scope\ngit commit -m \"fix(router): handle timeout errors gracefully\"\n\n# Breaking change\ngit commit -m \"feat!: change router interface to async-only\n\nBREAKING CHANGE: RouterBackend.classify() is now async\nMigration: Add 'await' to all classify() calls\"\n\n# Multi-line with body\ngit commit -m \"test: improve Discord bot coverage to 89.92%\n\n- Add /channels command tests (6 tests)\n- Add message splitting edge cases (4 tests)\n- Add agent not ready scenario (1 test)\n\nCloses #123\"\n</code></pre>"},{"location":"CONTRIBUTING/#co-authored-commits","title":"Co-Authored Commits","text":"<p>If working with Claude Code or pair programming:</p> <pre><code>git commit -m \"feat: implement new feature\n\nCo-Authored-By: Claude Sonnet 4.5 &lt;noreply@anthropic.com&gt;\"\n</code></pre>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":""},{"location":"CONTRIBUTING/#before-submitting","title":"Before Submitting","text":"<ol> <li> <p>Sync with upstream:    <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre></p> </li> <li> <p>Run full test suite:    <pre><code>pytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai\n</code></pre></p> </li> <li> <p>Ensure all pre-commit hooks pass:    <pre><code>pre-commit run --all-files\n</code></pre></p> </li> <li> <p>Update documentation if needed</p> </li> </ol>"},{"location":"CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<ol> <li> <p>Push your branch:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Go to GitHub and create a Pull Request</p> </li> <li> <p>PR Title: Use conventional commit format    <pre><code>feat: Add new feature description\n</code></pre></p> </li> <li> <p>PR Description should include:    <pre><code>## Summary\nBrief description of what this PR does.\n\n## Changes\n- Added X feature\n- Fixed Y bug\n- Updated Z documentation\n\n## Testing\n- [ ] Unit tests added/updated\n- [ ] Integration tests pass\n- [ ] Manual testing completed\n\n## Screenshots (if UI changes)\n[Add screenshots if applicable]\n\n## Related Issues\nCloses #123\nFixes #456\n</code></pre></p> </li> <li> <p>Wait for CI/CD: All checks must pass</p> </li> <li>Linting (Ruff)</li> <li>Type checking (Mypy)</li> <li>Security scan (Bandit)</li> <li>Unit tests (Python 3.12 &amp; 3.13)</li> <li>Docker build</li> <li> <p>Integration tests</p> </li> <li> <p>Address review comments: Make requested changes and push updates</p> </li> <li> <p>Squash commits (if requested):    <pre><code>git rebase -i HEAD~N  # N = number of commits\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#pr-review-criteria","title":"PR Review Criteria","text":"<p>Your PR will be evaluated on:</p> <ul> <li>\u2705 Functionality: Does it work as intended?</li> <li>\u2705 Tests: Are there adequate tests? Do they pass?</li> <li>\u2705 Code Quality: Follows style guide, no code smells</li> <li>\u2705 Documentation: Updated docs, clear docstrings</li> <li>\u2705 Security: No vulnerabilities introduced</li> <li>\u2705 Performance: No significant performance degradation</li> </ul>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":""},{"location":"CONTRIBUTING/#what-to-document","title":"What to Document","text":"<ol> <li>Code Documentation:</li> <li>Docstrings for all public functions/classes</li> <li>Inline comments for complex logic</li> <li> <p>Type hints for all parameters and returns</p> </li> <li> <p>User Documentation:</p> </li> <li>Update <code>README.md</code> for new features</li> <li>Add to <code>docs/COMMANDS.md</code> for new commands</li> <li>Update <code>docs/TROUBLESHOOTING.md</code> for common issues</li> <li> <p>Update <code>docs/FAQ.md</code> for frequently asked questions</p> </li> <li> <p>Developer Documentation:</p> </li> <li>Update <code>docs/ARCHITECTURE.md</code> for structural changes</li> <li>Update <code>docs/TESTING.md</code> for new test patterns</li> <li>Update <code>docs/SECURITY.md</code> for security changes</li> <li>Update <code>CHANGELOG.md</code> for user-facing changes</li> </ol>"},{"location":"CONTRIBUTING/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use Markdown for all documentation</li> <li>Include code examples</li> <li>Add screenshots for UI features</li> <li>Keep language clear and concise</li> <li>Use tables for comparisons</li> <li>Include links to related documentation</li> </ul>"},{"location":"CONTRIBUTING/#getting-help","title":"Getting Help","text":"<ul> <li>Discord: Join our Discord server (if available)</li> <li>Issues: Search existing issues or create a new one</li> <li>Discussions: Use GitHub Discussions for questions</li> </ul>"},{"location":"CONTRIBUTING/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - <code>CONTRIBUTORS.md</code> file - Release notes - GitHub contributors page</p> <p>Thank you for contributing to Zetherion AI! \ud83c\udf89</p>"},{"location":"COST_TRACKING/","title":"Cost Tracking Guide","text":"<p>Monitor, budget, and optimize your LLM API spending with Zetherion AI's built-in cost tracking system.</p>"},{"location":"COST_TRACKING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Configuration</li> <li>Understanding Costs</li> <li>Budget Management</li> <li>Cost Reports</li> <li>Optimization Strategies</li> <li>Troubleshooting</li> </ul>"},{"location":"COST_TRACKING/#overview","title":"Overview","text":"<p>The cost tracking system monitors every API call to LLM providers and:</p> <ul> <li>Tracks spending in real-time per provider and task type</li> <li>Enforces budgets with configurable daily and monthly limits</li> <li>Sends alerts before you exceed thresholds</li> <li>Generates reports for spending analysis</li> <li>Stores history in SQLite for querying</li> </ul>"},{"location":"COST_TRACKING/#how-it-works","title":"How It Works","text":"<pre><code>API Request \u2192 InferenceBroker \u2192 Cost Calculation \u2192 Budget Check \u2192 Storage\n                                      \u2193\n                              Alert if threshold reached\n</code></pre>"},{"location":"COST_TRACKING/#configuration","title":"Configuration","text":""},{"location":"COST_TRACKING/#enable-cost-tracking","title":"Enable Cost Tracking","text":"<pre><code># Enable the cost tracking system\nCOST_TRACKING_ENABLED=true\n\n# Storage location\nCOST_DB_PATH=data/costs.db\n</code></pre>"},{"location":"COST_TRACKING/#set-budgets","title":"Set Budgets","text":"<pre><code># Daily spending limit (USD)\nDAILY_BUDGET_USD=5.00\n\n# Monthly spending limit (USD)\nMONTHLY_BUDGET_USD=50.00\n\n# Warning threshold (percentage)\nBUDGET_WARNING_PCT=80.0\n</code></pre>"},{"location":"COST_TRACKING/#enable-notifications","title":"Enable Notifications","text":"<pre><code># Enable notifications for cost alerts\nNOTIFICATIONS_ENABLED=true\n\n# Daily summary at 8 PM\nDAILY_SUMMARY_ENABLED=true\nDAILY_SUMMARY_HOUR=20\n</code></pre>"},{"location":"COST_TRACKING/#understanding-costs","title":"Understanding Costs","text":""},{"location":"COST_TRACKING/#provider-pricing","title":"Provider Pricing","text":"Provider Model Input (per 1M tokens) Output (per 1M tokens) Anthropic Claude Sonnet 4.5 $3.00 $15.00 Anthropic Claude Haiku 4.5 $0.25 $1.25 OpenAI GPT-4o $2.50 $10.00 OpenAI GPT-4o-mini $0.15 $0.60 Google Gemini Flash Free tier Free tier Ollama Local models $0.00 $0.00 <p>Prices as of February 2026. Check provider websites for current rates.</p>"},{"location":"COST_TRACKING/#token-estimation","title":"Token Estimation","text":"<p>Approximate token counts: - 1 word \u2248 1.3 tokens - 100 words \u2248 130 tokens - 1 page of text \u2248 500-700 tokens</p>"},{"location":"COST_TRACKING/#typical-query-costs","title":"Typical Query Costs","text":"Query Type Input Tokens Output Tokens Estimated Cost (Claude) Simple question ~50 ~100 $0.0016 Code review ~500 ~300 $0.006 Complex analysis ~1000 ~500 $0.01 Long conversation ~2000 ~1000 $0.02"},{"location":"COST_TRACKING/#cost-by-task-type","title":"Cost by Task Type","text":"<p>The system tracks costs by task type:</p> Task Type Typical Provider Avg Cost/Query Simple Query Gemini Flash $0.00 (free) Memory Search Gemini Flash $0.00 (free) Complex Reasoning Claude Sonnet $0.005 Code Generation Claude Sonnet $0.008 Creative Writing GPT-4o $0.006"},{"location":"COST_TRACKING/#budget-management","title":"Budget Management","text":""},{"location":"COST_TRACKING/#daily-budgets","title":"Daily Budgets","text":"<p>Daily budgets reset at midnight (local time).</p> <p>Workflow: 1. Each API call adds to daily total 2. At 80% (configurable), warning notification sent 3. At 100%, exceeded notification sent 4. Complex queries may be limited (uses cheaper provider)</p>"},{"location":"COST_TRACKING/#monthly-budgets","title":"Monthly Budgets","text":"<p>Monthly budgets reset on the 1st of each month.</p> <p>Workflow: 1. Each API call adds to monthly total 2. At 80%, warning notification sent 3. At 100%, exceeded notification sent 4. Daily budget may be reduced automatically</p>"},{"location":"COST_TRACKING/#budget-notifications","title":"Budget Notifications","text":"<pre><code>\u26a0\ufe0f Budget Warning (80% reached)\n\nDaily Spending: $4.00 / $5.00 (80%)\nMonthly Spending: $35.00 / $50.00 (70%)\n\nTop spending:\n- Claude Sonnet: $2.50 (62%)\n- GPT-4o: $1.20 (30%)\n- Gemini Flash: $0.00 (0%)\n\nRemaining today: $1.00\n</code></pre>"},{"location":"COST_TRACKING/#budget-actions","title":"Budget Actions","text":"<p>When budgets are exceeded:</p> Level Action 80% Warning Notification only, no restrictions 100% Daily Prefer cheaper providers, limit complex queries 100% Monthly Strict limits, may require manual override"},{"location":"COST_TRACKING/#cost-reports","title":"Cost Reports","text":""},{"location":"COST_TRACKING/#daily-summary","title":"Daily Summary","text":"<p>Automatically sent at configured hour (default: 8 PM):</p> <pre><code>\ud83d\udcca Daily Cost Summary (Feb 7, 2026)\n\nTotal Spent Today: $3.45\n\nBy Provider:\n  Claude Sonnet: $2.10 (61%)\n  GPT-4o: $1.05 (30%)\n  Gemini Flash: $0.00 (0%)\n  Ollama: $0.00 (0%)\n\nBy Task Type:\n  Complex Reasoning: $1.50 (43%)\n  Code Generation: $1.20 (35%)\n  Simple Queries: $0.00 (0%)\n  Memory Operations: $0.00 (0%)\n\nQueries: 47 total\n  - Successful: 45 (96%)\n  - Rate Limited: 2 (4%)\n\nBudget Status:\n  Daily: $3.45 / $5.00 (69%)\n  Monthly: $28.45 / $50.00 (57%)\n</code></pre>"},{"location":"COST_TRACKING/#monthly-summary","title":"Monthly Summary","text":"<p>Sent on the 1st of each month:</p> <pre><code>\ud83d\udcca Monthly Cost Summary (January 2026)\n\nTotal Spent: $42.50\n\nBy Provider:\n  Claude Sonnet: $28.00 (66%)\n  GPT-4o: $12.50 (29%)\n  Gemini Flash: $0.00 (0%)\n  Ollama: $0.00 (0%)\n\nDaily Average: $1.37\nPeak Day: Jan 15 ($4.80)\nLowest Day: Jan 3 ($0.12)\n\nTotal Queries: 1,247\nAverage Cost/Query: $0.034\n\nBudget: $42.50 / $50.00 (85%)\n</code></pre>"},{"location":"COST_TRACKING/#querying-cost-data","title":"Querying Cost Data","text":"<p>Access cost data programmatically:</p> <pre><code># View SQLite database\ndocker exec zetherion-ai-bot sqlite3 /app/data/costs.db \"\n  SELECT date(timestamp), SUM(cost_usd) as daily_cost\n  FROM usage_records\n  GROUP BY date(timestamp)\n  ORDER BY date(timestamp) DESC\n  LIMIT 7;\n\"\n</code></pre>"},{"location":"COST_TRACKING/#export-cost-data","title":"Export Cost Data","text":"<pre><code># Export to CSV\ndocker exec zetherion-ai-bot sqlite3 -header -csv /app/data/costs.db \"\n  SELECT * FROM usage_records\n  WHERE timestamp &gt; datetime('now', '-30 days');\n\" &gt; costs_last_30_days.csv\n</code></pre>"},{"location":"COST_TRACKING/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"COST_TRACKING/#1-use-free-tiers-effectively","title":"1. Use Free Tiers Effectively","text":"<p>Gemini Flash Free Tier: - 15 requests/minute - 1,500 requests/day - Use for routing, simple queries, embeddings</p> <p>Strategy: <pre><code># Route simple queries to Gemini\nROUTER_BACKEND=gemini\n</code></pre></p>"},{"location":"COST_TRACKING/#2-optimize-context-window","title":"2. Optimize Context Window","text":"<p>Reduce tokens sent per request:</p> <pre><code># Fewer messages in context\nCONTEXT_WINDOW_SIZE=5  # Default: 10\n\n# Fewer memory search results\nMEMORY_SEARCH_LIMIT=3  # Default: 5\n</code></pre> <p>Savings: ~30-50% reduction in input tokens</p>"},{"location":"COST_TRACKING/#3-use-ollama-for-routing","title":"3. Use Ollama for Routing","text":"<p>Local routing = zero API costs:</p> <pre><code>ROUTER_BACKEND=ollama\nOLLAMA_ROUTER_MODEL=llama3.1:8b\n</code></pre> <p>Savings: 100% of routing costs (typically 10-20% of total)</p>"},{"location":"COST_TRACKING/#4-choose-cost-effective-models","title":"4. Choose Cost-Effective Models","text":"Need Expensive Cost-Effective Simple Q&amp;A Claude Sonnet Gemini Flash Code Review GPT-4o Claude Haiku Summarization Claude Sonnet GPT-4o-mini"},{"location":"COST_TRACKING/#5-rate-limiting","title":"5. Rate Limiting","text":"<p>Prevent runaway costs:</p> <pre><code># Limit messages per user\nRATE_LIMIT_MESSAGES=5\nRATE_LIMIT_WINDOW=60\n</code></pre>"},{"location":"COST_TRACKING/#6-hybrid-approach","title":"6. Hybrid Approach","text":"<p>Best of both worlds:</p> <pre><code># Free routing\nROUTER_BACKEND=gemini\n\n# Quality for complex tasks only\nANTHROPIC_API_KEY=sk-ant-...\n\n# Skip OpenAI (redundant with Claude)\nOPENAI_API_KEY=\n\n# Strict budget\nDAILY_BUDGET_USD=3.00\n</code></pre> <p>Expected Monthly Cost: $20-40</p>"},{"location":"COST_TRACKING/#cost-comparison-strategies","title":"Cost Comparison: Strategies","text":"Strategy Monthly Cost Quality Privacy All Cloud (Claude + GPT-4o) $50-100 Best Low Gemini Only $0 (free tier) Good Low Ollama Only $0 (electricity) Good High Hybrid (Gemini + Claude) $20-40 Very Good Medium Hybrid (Ollama + Claude) $15-30 Very Good High"},{"location":"COST_TRACKING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"COST_TRACKING/#costs-higher-than-expected","title":"Costs Higher Than Expected","text":"<p>Check usage patterns: <pre><code># Top cost queries today\ndocker exec zetherion-ai-bot sqlite3 /app/data/costs.db \"\n  SELECT provider, task_type, COUNT(*) as count, SUM(cost_usd) as total\n  FROM usage_records\n  WHERE date(timestamp) = date('now')\n  GROUP BY provider, task_type\n  ORDER BY total DESC;\n\"\n</code></pre></p> <p>Common causes: 1. Long conversations (high context window) 2. Complex queries routed to expensive models 3. Retry loops on failed requests 4. Memory search returning too many results</p>"},{"location":"COST_TRACKING/#budget-not-enforcing","title":"Budget Not Enforcing","text":"<p>Check configuration: <pre><code># Verify settings loaded\ndocker exec zetherion-ai-bot python -c \"\nfrom zetherion_ai.config import get_settings\ns = get_settings()\nprint(f'Daily: {s.daily_budget_usd}')\nprint(f'Monthly: {s.monthly_budget_usd}')\nprint(f'Cost tracking: {s.cost_tracking_enabled}')\n\"\n</code></pre></p> <p>Ensure InferenceBroker enabled: <pre><code>INFERENCE_BROKER_ENABLED=true\nCOST_TRACKING_ENABLED=true\n</code></pre></p>"},{"location":"COST_TRACKING/#notifications-not-sending","title":"Notifications Not Sending","text":"<p>Check notification configuration: <pre><code>NOTIFICATIONS_ENABLED=true\nBUDGET_WARNING_PCT=80.0\n</code></pre></p> <p>Verify Discord connection: <pre><code>docker-compose logs zetherion-ai-bot | grep -i notification\n</code></pre></p>"},{"location":"COST_TRACKING/#database-issues","title":"Database Issues","text":"<p>Reset cost database (lose history): <pre><code>docker exec zetherion-ai-bot rm /app/data/costs.db\ndocker-compose restart zetherion-ai-bot\n</code></pre></p> <p>Backup before reset: <pre><code>docker cp zetherion-ai-bot:/app/data/costs.db ./costs_backup.db\n</code></pre></p>"},{"location":"COST_TRACKING/#database-schema","title":"Database Schema","text":"<p>The cost database uses SQLite with this schema:</p> <pre><code>CREATE TABLE usage_records (\n    id INTEGER PRIMARY KEY,\n    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n    provider TEXT NOT NULL,\n    model TEXT NOT NULL,\n    task_type TEXT,\n    input_tokens INTEGER,\n    output_tokens INTEGER,\n    cost_usd REAL,\n    latency_ms INTEGER,\n    success BOOLEAN,\n    error_message TEXT\n);\n\nCREATE INDEX idx_timestamp ON usage_records(timestamp);\nCREATE INDEX idx_provider ON usage_records(provider);\n</code></pre>"},{"location":"COST_TRACKING/#additional-resources","title":"Additional Resources","text":"<ul> <li>Features Overview - All Phase 5+ features</li> <li>Configuration Reference - All settings</li> <li>InferenceBroker - Provider routing</li> <li>Hardware Recommendations - Ollama setup for $0 routing</li> </ul> <p>Last Updated: 2026-02-07 Version: 3.0.0 (Cost Tracking)</p>"},{"location":"DEVELOPMENT/","title":"Development Guide","text":"<p>This guide provides in-depth information for developers working on Zetherion AI. For contribution guidelines, see CONTRIBUTING.md.</p>"},{"location":"DEVELOPMENT/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Architecture Overview</li> <li>Development Environment</li> <li>Project Structure</li> <li>Development Workflow</li> <li>Testing Patterns</li> <li>Debugging</li> <li>Performance Optimization</li> <li>Adding New Features</li> <li>Docker Development</li> <li>Common Tasks</li> </ul>"},{"location":"DEVELOPMENT/#architecture-overview","title":"Architecture Overview","text":""},{"location":"DEVELOPMENT/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         Discord Bot                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Commands   \u2502  \u2502  Security    \u2502  \u2502  Message Handler \u2502  \u2502\n\u2502  \u2502  (/channels \u2502  \u2502  - Allowlist \u2502  \u2502  - DM vs Mention \u2502  \u2502\n\u2502  \u2502  /remember  \u2502  \u2502  - Rate Limit\u2502  \u2502  - Splitting     \u2502  \u2502\n\u2502  \u2502  /summarize)\u2502  \u2502  - Injection \u2502  \u2502                  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Agent Core                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Message Router (Factory Pattern)                   \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u2502\n\u2502  \u2502  \u2502 Gemini Backend   \u2502  \u2502 Ollama Backend       \u2502    \u2502   \u2502\n\u2502  \u2502  \u2502 - Classification \u2502  \u2502 - Classification     \u2502    \u2502   \u2502\n\u2502  \u2502  \u2502 - Simple Queries \u2502  \u2502 - Simple Queries     \u2502    \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Response Generators                                \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502   \u2502\n\u2502  \u2502  \u2502 Claude/OpenAI\u2502  \u2502 Gemini/Ollama\u2502                \u2502   \u2502\n\u2502  \u2502  \u2502 (Complex)    \u2502  \u2502 (Simple)     \u2502                \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Memory Manager                                     \u2502   \u2502\n\u2502  \u2502  - Context Building                                 \u2502   \u2502\n\u2502  \u2502  - Deduplication                                    \u2502   \u2502\n\u2502  \u2502  - Retry Logic                                      \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Memory Layer                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Qdrant Vector DB \u2502  \u2502  Embeddings (Gemini)          \u2502  \u2502\n\u2502  \u2502  - Async Client   \u2502  \u2502  - text-embedding-004         \u2502  \u2502\n\u2502  \u2502  - Collections    \u2502  \u2502  - Parallel Batch Processing  \u2502  \u2502\n\u2502  \u2502  - Semantic Search\u2502  \u2502  - 768-dimensional vectors    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"DEVELOPMENT/#key-design-patterns","title":"Key Design Patterns","text":"<ol> <li>Factory Pattern - Router backend selection (Gemini vs Ollama)</li> <li>Strategy Pattern - Pluggable LLM backends</li> <li>Repository Pattern - Memory abstraction layer</li> <li>Singleton Pattern - Configuration management with LRU cache</li> <li>Async/Await - Non-blocking I/O throughout the stack</li> </ol>"},{"location":"DEVELOPMENT/#data-flow","title":"Data Flow","text":"<pre><code>User Message (Discord)\n  \u2193\nSecurity Checks (Allowlist, Rate Limit, Injection Detection)\n  \u2193\nMessage Router (Intent Classification)\n  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Simple Query    \u2502 Complex Task    \u2502\n\u2502 (Gemini/Ollama) \u2502 (Claude/OpenAI) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2193\nMemory Search (Qdrant - Parallel Embeddings)\n  \u2193\nContext Building (Retrieved Memory + Current Message)\n  \u2193\nResponse Generation (LLM API Call with Retry)\n  \u2193\nResponse Splitting (2000 char Discord limit)\n  \u2193\nUser Response (Discord)\n</code></pre>"},{"location":"DEVELOPMENT/#development-environment","title":"Development Environment","text":""},{"location":"DEVELOPMENT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: 3.12 or higher (tested on 3.12 and 3.13)</li> <li>Docker Desktop: Latest version (for Qdrant and Ollama)</li> <li>Git: 2.30+</li> <li>Code Editor: VSCode recommended (see <code>.vscode/settings.json</code>)</li> </ul>"},{"location":"DEVELOPMENT/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone and set up remote:    <pre><code>git clone https://github.com/jimtin/zetherion-ai.git\ncd zetherion_ai\ngit remote add upstream https://github.com/jimtin/zetherion-ai.git\n</code></pre></p> </li> <li> <p>Create virtual environment:    <pre><code>python3.12 -m venv venv\nsource venv/bin/activate  # macOS/Linux\n# OR\nvenv\\Scripts\\activate  # Windows\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install --upgrade pip\npip install -r requirements.txt\npip install -r requirements-dev.txt\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install\npre-commit install --hook-type pre-push\n</code></pre></p> </li> <li> <p>Set up environment variables:    <pre><code>cp .env.example .env\n# Edit .env with your API keys\n</code></pre></p> </li> <li> <p>Verify setup:    <pre><code>python -c \"from zetherion_ai.config import get_settings; print('Config loaded successfully')\"\npytest tests/ -m \"not integration and not discord_e2e\" --maxfail=1\n</code></pre></p> </li> </ol>"},{"location":"DEVELOPMENT/#vscode-configuration","title":"VSCode Configuration","text":"<p>Recommended extensions (see <code>.vscode/extensions.json</code>): - Python (ms-python.python) - Pylance (ms-python.vscode-pylance) - Ruff (charliermarsh.ruff) - Docker (ms-azuretools.vscode-docker) - GitLens (eamodio.gitlens)</p> <p>Settings (<code>.vscode/settings.json</code>): <pre><code>{\n  \"python.linting.enabled\": true,\n  \"python.linting.mypyEnabled\": true,\n  \"python.formatting.provider\": \"none\",\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\",\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n      \"source.fixAll\": \"explicit\",\n      \"source.organizeImports\": \"explicit\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"DEVELOPMENT/#project-structure","title":"Project Structure","text":"<pre><code>zetherion_ai/\n\u251c\u2500\u2500 src/zetherion_ai/          # Main application code\n\u2502   \u251c\u2500\u2500 agent/               # Agent core logic\n\u2502   \u2502   \u251c\u2500\u2500 core.py          # Agent class (message handling, retry)\n\u2502   \u2502   \u251c\u2500\u2500 router.py        # Gemini router backend\n\u2502   \u2502   \u251c\u2500\u2500 router_ollama.py # Ollama router backend\n\u2502   \u2502   \u251c\u2500\u2500 router_factory.py# Router factory (backend selection)\n\u2502   \u2502   \u2514\u2500\u2500 router_base.py   # Abstract router interface\n\u2502   \u251c\u2500\u2500 discord/             # Discord bot\n\u2502   \u2502   \u251c\u2500\u2500 bot.py           # Main bot class\n\u2502   \u2502   \u251c\u2500\u2500 commands.py      # Slash commands\n\u2502   \u2502   \u2514\u2500\u2500 security.py      # Security controls\n\u2502   \u251c\u2500\u2500 memory/              # Vector memory\n\u2502   \u2502   \u251c\u2500\u2500 embeddings.py    # Gemini embeddings\n\u2502   \u2502   \u2514\u2500\u2500 qdrant.py        # Qdrant client\n\u2502   \u251c\u2500\u2500 config.py            # Pydantic settings\n\u2502   \u251c\u2500\u2500 logging.py           # Structured logging setup\n\u2502   \u2514\u2500\u2500 main.py              # Entry point\n\u251c\u2500\u2500 tests/                   # Test suite\n\u2502   \u251c\u2500\u2500 unit/                # Unit tests (fast)\n\u2502   \u2502   \u251c\u2500\u2500 test_agent_core.py\n\u2502   \u2502   \u251c\u2500\u2500 test_discord_bot.py\n\u2502   \u2502   \u251c\u2500\u2500 test_security.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 integration/         # Integration tests (slow)\n\u2502   \u2502   \u251c\u2500\u2500 test_e2e.py      # Full flow tests\n\u2502   \u2502   \u2514\u2500\u2500 test_discord_e2e.py  # Real Discord tests\n\u2502   \u251c\u2500\u2500 conftest.py          # Shared fixtures\n\u2502   \u2514\u2500\u2500 test_*.py            # Module-specific tests\n\u251c\u2500\u2500 docs/                    # Documentation\n\u2502   \u251c\u2500\u2500 ARCHITECTURE.md      # System architecture\n\u2502   \u251c\u2500\u2500 SECURITY.md          # Security controls\n\u2502   \u251c\u2500\u2500 TESTING.md           # Testing guide\n\u2502   \u251c\u2500\u2500 TROUBLESHOOTING.md   # Common issues\n\u2502   \u251c\u2500\u2500 FAQ.md               # Frequently asked questions\n\u2502   \u2514\u2500\u2500 COMMANDS.md          # Discord command reference\n\u251c\u2500\u2500 scripts/                 # Utility scripts\n\u2502   \u251c\u2500\u2500 assess-system.py     # Hardware assessment\n\u2502   \u2514\u2500\u2500 increase-docker-memory.sh  # Docker memory tuning\n\u251c\u2500\u2500 memory/                  # Auto memory (persistent)\n\u2502   \u251c\u2500\u2500 MEMORY.md            # Project memory\n\u2502   \u2514\u2500\u2500 phase5-plan.md       # Future roadmap\n\u251c\u2500\u2500 .github/                 # CI/CD workflows\n\u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u251c\u2500\u2500 ci.yml           # Main CI pipeline\n\u2502   \u2502   \u2514\u2500\u2500 codeql.yml       # Security analysis\n\u2502   \u2514\u2500\u2500 dependabot.yml       # Dependency updates\n\u251c\u2500\u2500 docker-compose.yml       # Production compose\n\u251c\u2500\u2500 docker-compose.dev.yml   # Development compose\n\u251c\u2500\u2500 Dockerfile               # Container image\n\u251c\u2500\u2500 start.sh                 # Startup script (498 lines)\n\u251c\u2500\u2500 stop.sh                  # Shutdown script\n\u251c\u2500\u2500 status.sh                # Status check script\n\u251c\u2500\u2500 pyproject.toml           # Python project config\n\u251c\u2500\u2500 requirements.txt         # Production dependencies\n\u251c\u2500\u2500 requirements-dev.txt     # Development dependencies\n\u251c\u2500\u2500 .pre-commit-config.yaml  # Pre-commit hooks\n\u251c\u2500\u2500 .gitleaks.toml           # Secret scanning config\n\u2514\u2500\u2500 .env.example             # Environment template\n</code></pre>"},{"location":"DEVELOPMENT/#module-responsibilities","title":"Module Responsibilities","text":"Module Responsibility Dependencies <code>agent/core.py</code> Message handling, retry logic, dual generators router, memory, LLM APIs <code>agent/router.py</code> Gemini classification and simple responses Gemini API <code>agent/router_ollama.py</code> Ollama classification and simple responses Ollama HTTP API <code>agent/router_factory.py</code> Backend selection and health checks router, router_ollama <code>discord/bot.py</code> Discord event handling, commands discord.py, agent <code>discord/security.py</code> Rate limiting, allowlist, injection detection None (standalone) <code>memory/embeddings.py</code> Parallel batch embeddings Gemini API <code>memory/qdrant.py</code> Vector storage and search Qdrant <code>config.py</code> Environment configuration Pydantic <code>logging.py</code> Structured logging setup structlog"},{"location":"DEVELOPMENT/#development-workflow","title":"Development Workflow","text":""},{"location":"DEVELOPMENT/#feature-development-cycle","title":"Feature Development Cycle","text":"<ol> <li> <p>Create feature branch:    <pre><code>git checkout -b feature/my-new-feature\n</code></pre></p> </li> <li> <p>Write failing tests first (TDD):    <pre><code># Create test file\ntouch tests/test_my_feature.py\n\n# Write test\n# Run tests (should fail)\npytest tests/test_my_feature.py -v\n</code></pre></p> </li> <li> <p>Implement feature:    <pre><code># Create/modify source files\n# Run tests iteratively\npytest tests/test_my_feature.py -v --maxfail=1\n</code></pre></p> </li> <li> <p>Verify all tests pass:    <pre><code>pytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai\n</code></pre></p> </li> <li> <p>Check code quality:    <pre><code>ruff check --fix .\nruff format .\nmypy src/zetherion_ai\n</code></pre></p> </li> <li> <p>Commit with conventional format:    <pre><code>git add .\ngit commit -m \"feat: add amazing new feature\"\n# Pre-commit hooks run automatically\n</code></pre></p> </li> <li> <p>Push and create PR:    <pre><code>git push origin feature/my-new-feature\n# Create PR on GitHub\n</code></pre></p> </li> </ol>"},{"location":"DEVELOPMENT/#branch-strategy","title":"Branch Strategy","text":"<ul> <li><code>main</code> - Production-ready code (protected)</li> <li><code>feature/*</code> - New features</li> <li><code>fix/*</code> - Bug fixes</li> <li><code>docs/*</code> - Documentation updates</li> <li><code>test/*</code> - Test improvements</li> <li><code>refactor/*</code> - Code refactoring</li> </ul>"},{"location":"DEVELOPMENT/#pre-commit-workflow","title":"Pre-Commit Workflow","text":"<p>When you commit, the following hooks run automatically:</p> <ol> <li>File Checks (100ms)</li> <li>Trailing whitespace removal</li> <li>End-of-file fixer</li> <li>Merge conflict markers</li> <li> <p>Large file prevention</p> </li> <li> <p>Gitleaks (1-2s)</p> </li> <li>Secret scanning (12 custom rules)</li> <li> <p>Zero false positives (tuned allowlists)</p> </li> <li> <p>Ruff Linting (1-2s)</p> </li> <li>600+ lint rules</li> <li>Auto-fixes applied</li> <li> <p>Import sorting</p> </li> <li> <p>Ruff Formatting (0.5s)</p> </li> <li>PEP 8 compliance</li> <li> <p>100-char line length</p> </li> <li> <p>Mypy Type Checking (3-5s)</p> </li> <li>Strict mode</li> <li> <p>All source files</p> </li> <li> <p>Bandit Security Scan (2-3s)</p> </li> <li>Python security issues</li> <li> <p>OWASP top 10 checks</p> </li> <li> <p>Hadolint (0.5s)</p> </li> <li>Dockerfile best practices</li> </ol> <p>Total pre-commit time: ~10-15 seconds</p>"},{"location":"DEVELOPMENT/#pre-push-workflow","title":"Pre-Push Workflow","text":"<p>Before pushing, additional checks run:</p> <ol> <li>Ruff (full project)</li> <li>Mypy (full project)</li> <li>Pytest (all tests with coverage)</li> </ol> <p>Total pre-push time: ~30-60 seconds</p>"},{"location":"DEVELOPMENT/#testing-patterns","title":"Testing Patterns","text":""},{"location":"DEVELOPMENT/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Fast, isolated tests (~24s)\n\u2502   \u251c\u2500\u2500 test_agent_core.py   # Agent logic\n\u2502   \u251c\u2500\u2500 test_discord_bot.py  # Discord bot\n\u2502   \u251c\u2500\u2500 test_security.py     # Security controls\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 integration/             # Slow, full-stack tests (~2 min)\n\u2502   \u251c\u2500\u2500 test_e2e.py          # Gemini + Ollama flows\n\u2502   \u2514\u2500\u2500 test_discord_e2e.py  # Real Discord API\n\u2514\u2500\u2500 conftest.py              # Shared fixtures\n</code></pre>"},{"location":"DEVELOPMENT/#fixture-patterns","title":"Fixture Patterns","text":"<p>Parametrized fixtures for multi-backend testing: <pre><code>@pytest.fixture(params=[\"gemini\", \"ollama\"])\ndef backend_type(request):\n    \"\"\"Test both Gemini and Ollama backends.\"\"\"\n    return request.param\n\ndef test_routing(backend_type):\n    \"\"\"This test runs twice (once for each backend).\"\"\"\n    router = create_router(backend_type)\n    # Test implementation\n</code></pre></p> <p>Mock fixtures for unit tests: <pre><code>@pytest.fixture\ndef mock_discord_interaction():\n    \"\"\"Mock Discord interaction for command testing.\"\"\"\n    interaction = AsyncMock(spec=discord.Interaction)\n    interaction.user = MagicMock(id=123456789, name=\"testuser\")\n    interaction.guild = MagicMock(id=987654321, name=\"Test Server\")\n    interaction.channel = MagicMock(id=111222333, name=\"test-channel\")\n    interaction.response = AsyncMock()\n    interaction.followup = AsyncMock()\n    return interaction\n</code></pre></p> <p>Async fixtures with proper cleanup: <pre><code>@pytest.fixture\nasync def qdrant_client():\n    \"\"\"Async Qdrant client with cleanup.\"\"\"\n    client = AsyncQdrantClient(url=\"http://localhost:6333\")\n    yield client\n    await client.close()\n</code></pre></p>"},{"location":"DEVELOPMENT/#testing-async-code","title":"Testing Async Code","text":"<p>Basic async test: <pre><code>@pytest.mark.asyncio\nasync def test_async_function():\n    \"\"\"Test async function.\"\"\"\n    result = await some_async_function()\n    assert result == expected_value\n</code></pre></p> <p>Testing with asyncio.gather: <pre><code>@pytest.mark.asyncio\nasync def test_parallel_execution():\n    \"\"\"Test parallel async operations.\"\"\"\n    tasks = [async_function(i) for i in range(10)]\n    results = await asyncio.gather(*tasks)\n    assert len(results) == 10\n</code></pre></p> <p>Testing timeouts: <pre><code>@pytest.mark.asyncio\nasync def test_timeout_handling():\n    \"\"\"Test timeout scenarios.\"\"\"\n    with pytest.raises(asyncio.TimeoutError):\n        await asyncio.wait_for(slow_function(), timeout=1.0)\n</code></pre></p>"},{"location":"DEVELOPMENT/#mocking-patterns","title":"Mocking Patterns","text":"<p>Patching with context managers: <pre><code>from unittest.mock import patch, AsyncMock\n\ndef test_with_mock():\n    \"\"\"Test with mocked dependency.\"\"\"\n    with patch(\"zetherion_ai.agent.router.gemini\") as mock_gemini:\n        mock_gemini.generate_content.return_value = AsyncMock(\n            text='{\"intent\": \"simple_query\", \"confidence\": 0.9}'\n        )\n        router = MessageRouter()\n        result = await router.classify(\"Hello\")\n        assert result.intent == \"simple_query\"\n</code></pre></p> <p>Patching multiple targets: <pre><code>@patch(\"zetherion_ai.memory.embeddings.genai\")\n@patch(\"zetherion_ai.memory.qdrant.AsyncQdrantClient\")\nasync def test_with_multiple_mocks(mock_qdrant, mock_genai):\n    \"\"\"Test with multiple mocked dependencies.\"\"\"\n    # Configure mocks\n    mock_genai.embed_content_async.return_value = MagicMock(\n        embeddings=[MagicMock(values=[0.1] * 768)]\n    )\n    # Test implementation\n</code></pre></p>"},{"location":"DEVELOPMENT/#coverage-best-practices","title":"Coverage Best Practices","text":"<ol> <li>Aim for 85%+ coverage on new modules</li> <li>Maintain overall 85%+ coverage across the project</li> <li>Focus on critical paths (security, core logic)</li> <li>Avoid over-testing trivial code (getters, simple properties)</li> <li>Test edge cases (empty inputs, None values, errors)</li> </ol> <p>Generate coverage report: <pre><code># HTML report\npytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai --cov-report=html\n\n# Terminal summary\npytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai --cov-report=term\n\n# XML for CI\npytest tests/ -m \"not discord_e2e\" --cov=src/zetherion_ai --cov-report=xml\n</code></pre></p>"},{"location":"DEVELOPMENT/#debugging","title":"Debugging","text":""},{"location":"DEVELOPMENT/#logging-levels","title":"Logging Levels","text":"<pre><code>import structlog\n\nlog = structlog.get_logger(__name__)\n\n# Development\nlog.debug(\"Detailed debug info\", variable=value)\nlog.info(\"Normal operation\", event=\"message_received\")\n\n# Production\nlog.warning(\"Potential issue\", error=str(e))\nlog.error(\"Error occurred\", exc_info=True)\nlog.critical(\"System failure\", reason=\"Out of memory\")\n</code></pre>"},{"location":"DEVELOPMENT/#debug-configuration","title":"Debug Configuration","text":"<p>Enable debug logging: <pre><code># .env\nLOG_LEVEL=DEBUG\nENVIRONMENT=development\n</code></pre></p> <p>Console output (development): - Colored output with <code>structlog.dev.ConsoleRenderer</code> - Pretty-printed key-value pairs - Timestamp and level</p> <p>File output (production): - JSON format for parsing with <code>jq</code> - Rotating log files (10MB \u00d7 6 files) - Located in <code>logs/zetherion_ai.log</code></p>"},{"location":"DEVELOPMENT/#common-debugging-tasks","title":"Common Debugging Tasks","text":"<p>1. Debug Discord bot interactions: <pre><code># Watch logs in real-time\ntail -f logs/zetherion_ai.log | jq 'select(.event | contains(\"discord\"))'\n</code></pre></p> <p>2. Debug router decisions: <pre><code># Filter routing events\njq 'select(.event == \"message_routed\")' logs/zetherion_ai.log\n</code></pre></p> <p>3. Debug LLM API calls: <pre><code># Set LOG_LEVEL=DEBUG\n# Check logs for API request/response\njq 'select(.event | contains(\"api\"))' logs/zetherion_ai.log\n</code></pre></p> <p>4. Debug memory searches: <pre><code># Check embedding and search operations\njq 'select(.event | contains(\"memory\") or contains(\"embedding\"))' logs/zetherion_ai.log\n</code></pre></p> <p>5. Interactive debugging with pdb: <pre><code>import pdb; pdb.set_trace()  # Add breakpoint\n</code></pre></p> <p>6. Pytest debugging: <pre><code># Drop into pdb on first failure\npytest tests/ --pdb\n\n# Show print statements\npytest tests/ -s\n\n# Verbose output\npytest tests/ -vv\n\n# Show local variables on failure\npytest tests/ -l\n</code></pre></p>"},{"location":"DEVELOPMENT/#docker-debugging","title":"Docker Debugging","text":"<p>Check container logs: <pre><code>docker compose logs zetherion_ai -f\ndocker compose logs qdrant -f\ndocker compose logs ollama -f\n</code></pre></p> <p>Exec into containers: <pre><code># Zetherion AI container\ndocker exec -it zetherion_ai-bot bash\n\n# Qdrant container\ndocker exec -it zetherion_ai-qdrant bash\n\n# Ollama container\ndocker exec -it zetherion_ai-ollama bash\n</code></pre></p> <p>Check container health: <pre><code>docker compose ps\ndocker inspect zetherion_ai-bot | jq '.[0].State.Health'\n</code></pre></p> <p>Network debugging: <pre><code># Check network connectivity\ndocker exec zetherion_ai-bot ping qdrant\ndocker exec zetherion_ai-bot ping ollama\n\n# Check port availability\ndocker exec zetherion_ai-bot nc -zv qdrant 6333\ndocker exec zetherion_ai-bot nc -zv ollama 11434\n</code></pre></p>"},{"location":"DEVELOPMENT/#performance-optimization","title":"Performance Optimization","text":""},{"location":"DEVELOPMENT/#critical-optimizations-applied","title":"Critical Optimizations Applied","text":"<ol> <li> <p>Async Qdrant Client - Prevents event loop blocking    <pre><code># \u274c Bad (blocks event loop)\nfrom qdrant_client import QdrantClient\nclient = QdrantClient(url=\"http://qdrant:6333\")\n\n# \u2705 Good (non-blocking)\nfrom qdrant_client import AsyncQdrantClient\nclient = AsyncQdrantClient(url=\"http://qdrant:6333\")\n</code></pre></p> </li> <li> <p>Parallel Embeddings - 10x faster for batches    <pre><code># \u274c Bad (sequential)\nembeddings = [await embed_text(text) for text in texts]\n\n# \u2705 Good (parallel)\ntasks = [embed_text(text) for text in texts]\nembeddings = await asyncio.gather(*tasks)\n</code></pre></p> </li> <li> <p>Deduplicated Memory Searches - 50% fewer API calls    <pre><code># \u274c Bad (searches twice)\ncontext_claude = await search_memory(message)\ncontext_gemini = await search_memory(message)\n\n# \u2705 Good (searches once)\ncontext = await search_memory(message)\nresponse_claude = await generate_claude(message, context)\nresponse_gemini = await generate_gemini(message, context)\n</code></pre></p> </li> <li> <p>Retry with Exponential Backoff - Handles transient failures    <pre><code>retries = 0\nwhile retries &lt; max_retries:\n    try:\n        return await api_call()\n    except TransientError:\n        await asyncio.sleep(2 ** retries)\n        retries += 1\n</code></pre></p> </li> </ol>"},{"location":"DEVELOPMENT/#performance-monitoring","title":"Performance Monitoring","text":"<p>Measure async operation times: <pre><code>import time\nimport structlog\n\nlog = structlog.get_logger(__name__)\n\nasync def timed_operation(name: str):\n    start = time.time()\n    result = await some_async_function()\n    elapsed = time.time() - start\n    log.info(f\"{name} completed\", duration_ms=elapsed * 1000)\n    return result\n</code></pre></p> <p>Profile with cProfile: <pre><code>python -m cProfile -o output.prof src/zetherion_ai/main.py\npython -m pstats output.prof\n# (Pstats) sort cumulative\n# (Pstats) stats 20\n</code></pre></p> <p>Async profiling with yappi: <pre><code>pip install yappi\npython -m yappi --clock wall src/zetherion_ai/main.py\n</code></pre></p>"},{"location":"DEVELOPMENT/#adding-new-features","title":"Adding New Features","text":""},{"location":"DEVELOPMENT/#example-adding-a-new-discord-command","title":"Example: Adding a New Discord Command","text":"<ol> <li> <p>Define command in <code>discord/commands.py</code>:    <pre><code>@app_commands.command(name=\"status\", description=\"Show bot statistics\")\nasync def status_command(interaction: discord.Interaction) -&gt; None:\n    \"\"\"Handle /status command.\"\"\"\n    await interaction.response.defer()\n\n    stats = {\n        \"uptime\": get_uptime(),\n        \"messages_processed\": message_count,\n        \"memory_entries\": await get_memory_count(),\n    }\n\n    response = f\"**Bot Statistics**\\n\"\n    response += f\"Uptime: {stats['uptime']}\\n\"\n    response += f\"Messages: {stats['messages_processed']}\\n\"\n    response += f\"Memory Entries: {stats['memory_entries']}\"\n\n    await interaction.followup.send(response)\n</code></pre></p> </li> <li> <p>Register command in <code>discord/bot.py</code>:    <pre><code>async def _setup_commands(self) -&gt; None:\n    \"\"\"Set up slash commands.\"\"\"\n    self.tree.add_command(commands.status_command)\n    await self.tree.sync()\n</code></pre></p> </li> <li> <p>Write tests in <code>tests/unit/test_discord_commands.py</code>:    <pre><code>@pytest.mark.asyncio\nasync def test_status_command(mock_interaction):\n    \"\"\"Test /status command.\"\"\"\n    await status_command(mock_interaction)\n\n    mock_interaction.response.defer.assert_called_once()\n    mock_interaction.followup.send.assert_called_once()\n    response = mock_interaction.followup.send.call_args[0][0]\n    assert \"Bot Statistics\" in response\n    assert \"Uptime:\" in response\n</code></pre></p> </li> <li> <p>Update documentation:</p> </li> <li>Add command to <code>docs/COMMANDS.md</code></li> <li>Update README.md if user-facing</li> <li>Add to CHANGELOG.md</li> </ol>"},{"location":"DEVELOPMENT/#example-adding-a-new-router-backend","title":"Example: Adding a New Router Backend","text":"<ol> <li> <p>Create backend implementation:    <pre><code># src/zetherion_ai/agent/router_newbackend.py\nimport structlog\nfrom zetherion_ai.agent.router_base import RoutingDecision\n\nlog = structlog.get_logger(__name__)\n\nclass NewBackendRouter:\n    \"\"\"Router backend using NewBackend API.\"\"\"\n\n    async def classify(self, message: str) -&gt; RoutingDecision:\n        \"\"\"Classify message intent.\"\"\"\n        # Implementation\n\n    async def generate_simple_response(self, message: str) -&gt; str:\n        \"\"\"Generate simple response.\"\"\"\n        # Implementation\n\n    async def health_check(self) -&gt; bool:\n        \"\"\"Check if backend is healthy.\"\"\"\n        # Implementation\n</code></pre></p> </li> <li> <p>Update factory:    <pre><code># src/zetherion_ai/agent/router_factory.py\nfrom zetherion_ai.agent.router_newbackend import NewBackendRouter\n\ndef create_router() -&gt; MessageRouter:\n    settings = get_settings()\n\n    if settings.router_backend == \"newbackend\":\n        return MessageRouter(NewBackendRouter())\n    # ... existing backends\n</code></pre></p> </li> <li> <p>Add configuration:    <pre><code># src/zetherion_ai/config.py\nrouter_backend: str = Field(\n    default=\"gemini\",\n    description=\"Router backend: 'gemini', 'ollama', or 'newbackend'\"\n)\n\n@field_validator(\"router_backend\")\n@classmethod\ndef validate_router_backend(cls, v: str) -&gt; str:\n    valid = [\"gemini\", \"ollama\", \"newbackend\"]\n    if v not in valid:\n        raise ValueError(f\"router_backend must be one of {valid}\")\n    return v\n</code></pre></p> </li> <li> <p>Write comprehensive tests:    <pre><code># tests/test_router_newbackend.py\n@pytest.mark.asyncio\nasync def test_newbackend_classify():\n    \"\"\"Test NewBackend classification.\"\"\"\n    # Test implementation\n</code></pre></p> </li> </ol>"},{"location":"DEVELOPMENT/#docker-development","title":"Docker Development","text":""},{"location":"DEVELOPMENT/#development-vs-production-compose","title":"Development vs Production Compose","text":"<p>Development (<code>docker-compose.dev.yml</code>): - Hot-reload for code changes - Volume mounts for local development - Exposed ports for debugging</p> <p>Production (<code>docker-compose.yml</code>): - Optimized image layers - Health checks - Restart policies - Resource limits (optional)</p>"},{"location":"DEVELOPMENT/#common-docker-tasks","title":"Common Docker Tasks","text":"<p>Start development environment: <pre><code>docker compose -f docker-compose.dev.yml up -d\n</code></pre></p> <p>Rebuild after dependency changes: <pre><code>docker compose build --no-cache\ndocker compose up -d\n</code></pre></p> <p>View logs: <pre><code># All services\ndocker compose logs -f\n\n# Specific service\ndocker compose logs zetherion_ai -f\n\n# Last 100 lines\ndocker compose logs --tail=100 zetherion_ai\n</code></pre></p> <p>Restart single service: <pre><code>docker compose restart zetherion_ai\n</code></pre></p> <p>Clean up everything: <pre><code>docker compose down -v  # Removes volumes (data loss!)\ndocker system prune -a  # Removes all unused images\n</code></pre></p>"},{"location":"DEVELOPMENT/#dockerfile-best-practices","title":"Dockerfile Best Practices","text":"<p>Current optimizations: - Multi-stage builds (not yet implemented, see Future) - Slim base image (<code>python:3.12-slim</code>) - Layer caching (dependencies before code) - Non-root user (not yet implemented, see Gap Analysis) - Health checks</p>"},{"location":"DEVELOPMENT/#docker-memory-management","title":"Docker Memory Management","text":"<p>Check Docker memory: <pre><code>docker stats\n</code></pre></p> <p>Increase Docker memory (macOS): <pre><code>./scripts/increase-docker-memory.sh 8  # 8GB\n</code></pre></p> <p>Monitor Ollama memory usage: <pre><code>docker stats zetherion_ai-ollama\n</code></pre></p>"},{"location":"DEVELOPMENT/#common-tasks","title":"Common Tasks","text":""},{"location":"DEVELOPMENT/#update-model-versions","title":"Update Model Versions","text":"<ol> <li>Check latest models:</li> <li>Claude: https://docs.anthropic.com/en/docs/about-claude/models</li> <li>OpenAI: https://platform.openai.com/docs/models</li> <li> <p>Gemini: https://ai.google.dev/gemini-api/docs/models</p> </li> <li> <p>Update <code>src/zetherion_ai/config.py</code>:    <pre><code>claude_model: str = Field(default=\"claude-sonnet-4-5-20250929\")\nopenai_model: str = Field(default=\"gpt-4o\")\nrouter_model: str = Field(default=\"gemini-2.5-flash\")\n</code></pre></p> </li> <li> <p>Test new models:    <pre><code>pytest tests/integration/test_e2e.py -v\n</code></pre></p> </li> <li> <p>Update documentation (comment in config.py with date)</p> </li> </ol>"},{"location":"DEVELOPMENT/#update-dependencies","title":"Update Dependencies","text":"<p>Check for updates: <pre><code>pip list --outdated\n</code></pre></p> <p>Update specific package: <pre><code>pip install --upgrade package-name\npip freeze | grep package-name &gt;&gt; requirements.txt\n</code></pre></p> <p>Update all dev dependencies: <pre><code>pip install --upgrade -r requirements-dev.txt\npip freeze &gt; requirements-dev.txt\n</code></pre></p> <p>Test after updates: <pre><code>pytest tests/ -m \"not discord_e2e\"\n</code></pre></p>"},{"location":"DEVELOPMENT/#run-security-scans","title":"Run Security Scans","text":"<p>Gitleaks (secret scanning): <pre><code>pre-commit run gitleaks --all-files\n</code></pre></p> <p>Bandit (Python security): <pre><code>bandit -r src/zetherion_ai -ll\n</code></pre></p> <p>Dependency vulnerabilities (not yet implemented): <pre><code>pip install pip-audit\npip-audit\n</code></pre></p>"},{"location":"DEVELOPMENT/#generate-documentation","title":"Generate Documentation","text":"<p>API documentation with Sphinx (not yet implemented): <pre><code>pip install sphinx sphinx-rtd-theme\nsphinx-quickstart docs/api\nsphinx-build -b html docs/api docs/api/_build\n</code></pre></p> <p>Coverage badge: <pre><code>pytest tests/ --cov=src/zetherion_ai --cov-report=term\n# Update badge URL in README.md\n</code></pre></p>"},{"location":"DEVELOPMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DEVELOPMENT/#common-development-issues","title":"Common Development Issues","text":"<p>Issue: Pre-commit hooks fail <pre><code># Fix Ruff issues\nruff check --fix .\nruff format .\n\n# Fix Mypy issues\nmypy src/zetherion_ai --show-error-codes\n\n# Skip hooks (emergency only)\ngit commit --no-verify\n</code></pre></p> <p>Issue: Tests fail with import errors <pre><code># Ensure virtual environment is activated\nsource venv/bin/activate\n\n# Reinstall in editable mode\npip install -e .\n</code></pre></p> <p>Issue: Docker services won't start <pre><code># Check Docker daemon\ndocker info\n\n# Check ports are free\nlsof -i :6333  # Qdrant\nlsof -i :11434  # Ollama\n\n# Restart Docker Desktop\n# macOS: osascript -e 'quit app \"Docker\"' &amp;&amp; open -a Docker\n</code></pre></p> <p>Issue: Ollama model not found <pre><code>docker exec zetherion_ai-ollama ollama list\ndocker exec zetherion_ai-ollama ollama pull llama3.1:8b\n</code></pre></p> <p>Issue: Qdrant connection refused <pre><code># Check Qdrant is running\ndocker ps | grep qdrant\n\n# Check health\ncurl http://localhost:6333/health\n\n# Restart Qdrant\ndocker compose restart qdrant\n</code></pre></p>"},{"location":"DEVELOPMENT/#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for detailed contribution guidelines.</p> <p>Quick checklist before submitting PR: - [ ] All tests pass (<code>pytest tests/ -m \"not discord_e2e\"</code>) - [ ] Coverage maintained or improved - [ ] Pre-commit hooks pass - [ ] Documentation updated - [ ] Conventional commit messages - [ ] CHANGELOG.md updated</p>"},{"location":"DEVELOPMENT/#additional-resources","title":"Additional Resources","text":"<ul> <li>Architecture Documentation</li> <li>Security Documentation</li> <li>Testing Guide</li> <li>Troubleshooting Guide</li> <li>FAQ</li> <li>Discord Commands Reference</li> </ul>"},{"location":"DEVELOPMENT/#contact","title":"Contact","text":"<ul> <li>Issues: https://github.com/jimtin/zetherion-ai/issues</li> <li>Discussions: https://github.com/jimtin/zetherion-ai/discussions</li> <li>Email: [Your email if public]</li> </ul>"},{"location":"DOCKER_ARCHITECTURE/","title":"Docker Architecture and Memory Management","text":"<p>This document explains how Docker works on macOS, the distinction between Docker Desktop and containers, and how Zetherion AI automatically manages Docker memory allocation for Ollama models.</p>"},{"location":"DOCKER_ARCHITECTURE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Docker Desktop vs Docker Containers</li> <li>Memory Hierarchy</li> <li>Why Docker Desktop Needs to Restart</li> <li>Automated Memory Management</li> <li>Manual Docker Configuration</li> <li>Troubleshooting</li> </ol>"},{"location":"DOCKER_ARCHITECTURE/#docker-desktop-vs-docker-containers","title":"Docker Desktop vs Docker Containers","text":""},{"location":"DOCKER_ARCHITECTURE/#docker-desktop-the-virtual-machine","title":"Docker Desktop (The Virtual Machine)","text":"<p>On macOS, Docker runs inside a lightweight virtual machine (VM) called Docker Desktop. This is necessary because Docker uses Linux-specific features that don't exist natively on macOS.</p> <p>Key points: - Docker Desktop is the host environment that runs all containers - It has its own fixed memory allocation from your Mac's RAM - This memory allocation is configured in Docker Desktop settings - Located at: <code>~/Library/Group Containers/group.com.docker/settings.json</code> - The <code>memoryMiB</code> field controls how much RAM the VM gets</p> <p>Think of it like this: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Your Mac (Physical Hardware)      \u2502\n\u2502   Total RAM: e.g., 16GB             \u2502\n\u2502                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Docker Desktop VM             \u2502 \u2502\n\u2502  \u2502  Allocated: e.g., 8GB          \u2502 \u2502\n\u2502  \u2502                                \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502  Container 1 (Bot)       \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  Limit: 2GB              \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502                                \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502  Container 2 (Ollama)    \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  Limit: 10GB             \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2502                                \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502  \u2502  Container 3 (Qdrant)    \u2502 \u2502 \u2502\n\u2502  \u2502  \u2502  Limit: 1GB              \u2502 \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"DOCKER_ARCHITECTURE/#docker-containers","title":"Docker Containers","text":"<p>Containers are isolated processes that run inside the Docker Desktop VM.</p> <p>Key points: - Each container can have a memory limit (e.g., <code>--memory=10g</code>) - Container limits cannot exceed the Docker Desktop VM's total allocation - If Docker Desktop has 8GB, you cannot give a container 10GB - Multiple containers share the Docker Desktop VM's memory pool</p>"},{"location":"DOCKER_ARCHITECTURE/#the-critical-difference","title":"The Critical Difference","text":"<p>Setting a container's memory limit does NOT increase Docker Desktop's memory allocation.</p> <p>If you run: <pre><code>docker run --memory=10g ollama/ollama\n</code></pre></p> <p>But Docker Desktop only has 4GB allocated, the container will: - Be limited to 4GB (the VM's total) - Or fail to start if it requires more than available - Not automatically increase Docker Desktop's allocation</p> <p>That's why we need to restart Docker Desktop - to resize the VM itself.</p>"},{"location":"DOCKER_ARCHITECTURE/#memory-hierarchy","title":"Memory Hierarchy","text":"<p>Understanding the three-layer memory hierarchy is crucial:</p>"},{"location":"DOCKER_ARCHITECTURE/#layer-1-physical-ram","title":"Layer 1: Physical RAM","text":"<ul> <li>Your Mac's actual memory (e.g., 16GB, 32GB)</li> <li>Shared between macOS and all applications</li> <li>Fixed amount, cannot be changed without hardware upgrade</li> </ul>"},{"location":"DOCKER_ARCHITECTURE/#layer-2-docker-desktop-vm-allocation","title":"Layer 2: Docker Desktop VM Allocation","text":"<ul> <li>A portion of physical RAM reserved for Docker</li> <li>Configured in Docker Desktop settings</li> <li>Requires Docker Desktop restart to change</li> <li>Default: 2-4GB (too small for Ollama models)</li> </ul>"},{"location":"DOCKER_ARCHITECTURE/#layer-3-container-memory-limits","title":"Layer 3: Container Memory Limits","text":"<ul> <li>Individual limits for each container</li> <li>Set via <code>docker run --memory=X</code> or <code>docker-compose.yml</code></li> <li>Cannot exceed Layer 2 (Docker Desktop VM allocation)</li> <li>Can be changed without restarting Docker Desktop</li> </ul>"},{"location":"DOCKER_ARCHITECTURE/#example-scenario","title":"Example Scenario","text":"<p>System: MacBook Pro with 16GB RAM</p> <p>Problem: - Docker Desktop allocated: 4GB (Layer 2) - Ollama model needs: 10GB (Layer 3 requirement)</p> <p>Why it fails: <pre><code>Layer 1 (Physical):    16GB  \u2713 Enough\nLayer 2 (Docker VM):    4GB  \u2717 NOT enough\nLayer 3 (Container):   10GB  \u2717 CANNOT allocate (exceeds Layer 2)\n</code></pre></p> <p>Solution: 1. Increase Layer 2 (Docker Desktop) to 12GB 2. Restart Docker Desktop to apply 3. Now Layer 3 (container) can use 10GB</p>"},{"location":"DOCKER_ARCHITECTURE/#why-docker-desktop-needs-to-restart","title":"Why Docker Desktop Needs to Restart","text":""},{"location":"DOCKER_ARCHITECTURE/#the-technical-reason","title":"The Technical Reason","text":"<p>Docker Desktop's VM allocation is a boot-time parameter. The hypervisor (the software that creates the VM) needs to:</p> <ol> <li>Allocate physical pages in your Mac's RAM</li> <li>Initialize the virtual memory space for the VM</li> <li>Configure the hypervisor with new limits</li> </ol> <p>These operations cannot be done while the VM is running.</p>"},{"location":"DOCKER_ARCHITECTURE/#the-analogy","title":"The Analogy","text":"<p>Think of it like upgrading RAM in a desktop computer: - You can't add more RAM while the computer is running - You must shut down, install RAM, then boot up - The BIOS needs to detect and configure the new RAM</p> <p>Docker Desktop is similar: - Can't change VM memory while running - Must quit Docker Desktop - Restart with new memory allocation</p>"},{"location":"DOCKER_ARCHITECTURE/#what-about-memory-flags","title":"What About <code>--memory</code> Flags?","text":"<p>Container memory limits (<code>docker run --memory=X</code>) are different: - They're enforced by cgroups (Linux control groups) - Can be changed at runtime - But they're just soft limits within the VM's total allocation - Like dividing a pie - you can't create more pie, just slice it differently</p>"},{"location":"DOCKER_ARCHITECTURE/#automated-memory-management","title":"Automated Memory Management","text":"<p>Zetherion AI includes a fully automated pipeline to handle Docker memory requirements for Ollama models.</p>"},{"location":"DOCKER_ARCHITECTURE/#the-pipeline","title":"The Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Hardware Detection (scripts/assess-system.py)           \u2502\n\u2502     - Detect CPU, RAM, GPU                                  \u2502\n\u2502     - Recommend appropriate Ollama model                     \u2502\n\u2502     - Calculate required Docker memory                       \u2502\n\u2502     - Save to .env: OLLAMA_ROUTER_MODEL, OLLAMA_DOCKER_MEMORY\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. Startup Check (start.sh)                                \u2502\n\u2502     - Read required memory from .env                         \u2502\n\u2502     - Check current Docker Desktop allocation               \u2502\n\u2502     - If insufficient, prompt user                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. User Choice                                             \u2502\n\u2502     \u2022 Automatically increase (default)                       \u2502\n\u2502     \u2022 Choose smaller model                                   \u2502\n\u2502     \u2022 Continue anyway (not recommended)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc (if \"automatically increase\")\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  4. Memory Increase (scripts/increase-docker-memory.sh)     \u2502\n\u2502     - Backup Docker settings JSON                            \u2502\n\u2502     - Update memoryMiB field                                \u2502\n\u2502     - Stop Docker Desktop (osascript or killall)            \u2502\n\u2502     - Wait for full shutdown (up to 20 seconds)             \u2502\n\u2502     - Launch Docker Desktop                                  \u2502\n\u2502     - Wait for daemon readiness (up to 60 seconds)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"DOCKER_ARCHITECTURE/#model-recommendations-with-memory-requirements","title":"Model Recommendations with Memory Requirements","text":"<p>The system recommends models based on your hardware:</p> Model Size RAM Req Docker Memory Speed (CPU) Quality phi3:mini 2.3GB 4GB 5GB ~1s Basic llama3.1:8b 4.7GB 8GB 8GB ~2-3s Excellent qwen2.5:7b 4.7GB 8GB 10GB ~2-3s Best mistral:7b 4.1GB 8GB 7GB ~1-2s Very Good <p>Docker Memory = Model Size + Overhead - Phi3: 2.3GB + 2GB overhead = 5GB - Llama3.1: 4.7GB + 3GB overhead = 8GB - Qwen2.5: 4.7GB + 5GB overhead = 10GB (needs more for quality) - Mistral: 4.1GB + 3GB overhead = 7GB</p>"},{"location":"DOCKER_ARCHITECTURE/#environment-variables","title":"Environment Variables","text":"<p>The system manages these automatically:</p> <pre><code># Set by assess-system.py\nOLLAMA_ROUTER_MODEL=llama3.1:8b\nOLLAMA_DOCKER_MEMORY=8  # in GB\n\n# Used by start.sh\nREQUIRED_MEMORY=${OLLAMA_DOCKER_MEMORY:-8}\n\n# Used by Docker Compose\nOLLAMA_DOCKER_MEMORY=8  # for container limits\n</code></pre>"},{"location":"DOCKER_ARCHITECTURE/#process-control","title":"Process Control","text":"<p>The memory increase script uses robust process management:</p> <ol> <li> <p>Check if Docker is running <pre><code>pgrep -x \"Docker\" &gt; /dev/null\n</code></pre></p> </li> <li> <p>Stop Docker Desktop</p> </li> <li>Try AppleScript first: <code>osascript -e 'quit app \"Docker\"'</code></li> <li>Fallback to killall: <code>killall Docker</code></li> <li> <p>Wait for full stop (up to 20 seconds)</p> </li> <li> <p>Verify shutdown</p> </li> <li>Loop checking <code>pgrep</code> until process gone</li> <li> <p>Timeout protection to prevent infinite wait</p> </li> <li> <p>Start Docker Desktop <pre><code>open -a Docker\n</code></pre></p> </li> <li> <p>Wait for daemon readiness</p> </li> <li>Not just GUI launch - wait for daemon to respond</li> <li>Use <code>docker info</code> to verify daemon is ready</li> <li>Can take 30-60 seconds on cold start</li> <li>Timeout after 60 seconds with helpful error message</li> </ol>"},{"location":"DOCKER_ARCHITECTURE/#manual-docker-configuration","title":"Manual Docker Configuration","text":"<p>If you prefer to configure Docker Desktop manually:</p>"},{"location":"DOCKER_ARCHITECTURE/#via-docker-desktop-gui","title":"Via Docker Desktop GUI","text":"<ol> <li>Open Docker Desktop</li> <li>Go to Settings (gear icon)</li> <li>Navigate to Resources \u2192 Advanced</li> <li>Adjust the Memory slider to desired amount</li> <li>Click Apply &amp; Restart</li> <li>Wait for Docker Desktop to restart (30-60 seconds)</li> </ol>"},{"location":"DOCKER_ARCHITECTURE/#via-settings-file-advanced","title":"Via Settings File (Advanced)","text":"<p>Location: <code>~/Library/Group Containers/group.com.docker/settings.json</code></p> <ol> <li> <p>Backup the file first: <pre><code>cp ~/Library/Group\\ Containers/group.com.docker/settings.json \\\n   ~/Library/Group\\ Containers/group.com.docker/settings.json.backup\n</code></pre></p> </li> <li> <p>Edit the file: <pre><code>vim ~/Library/Group\\ Containers/group.com.docker/settings.json\n</code></pre></p> </li> <li> <p>Find and update memoryMiB: <pre><code>{\n  \"memoryMiB\": 10240,  // 10GB in MiB\n  ...\n}\n</code></pre></p> </li> <li> <p>Restart Docker Desktop: <pre><code>osascript -e 'quit app \"Docker\"'\nsleep 5\nopen -a Docker\n</code></pre></p> </li> </ol>"},{"location":"DOCKER_ARCHITECTURE/#verify-the-change","title":"Verify the Change","text":"<pre><code># Check Docker's total memory\ndocker info | grep \"Total Memory\"\n\n# Should show: Total Memory: 10 GiB (or your configured amount)\n</code></pre>"},{"location":"DOCKER_ARCHITECTURE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DOCKER_ARCHITECTURE/#docker-desktop-wont-start-after-increasing-memory","title":"Docker Desktop Won't Start After Increasing Memory","text":"<p>Symptom: Docker Desktop GUI opens but daemon never becomes ready</p> <p>Possible Causes: 1. Requested memory exceeds available system RAM 2. Other apps consuming too much memory 3. Docker Desktop settings corrupted</p> <p>Solutions:</p> <ol> <li> <p>Check available RAM: <pre><code># macOS\nvm_stat | head -2\n</code></pre></p> </li> <li> <p>Reduce requested memory:</p> </li> <li>Restore backup settings:      <pre><code>cp ~/Library/Group\\ Containers/group.com.docker/settings.json.backup \\\n   ~/Library/Group\\ Containers/group.com.docker/settings.json\n</code></pre></li> <li> <p>Restart Docker Desktop</p> </li> <li> <p>Reset Docker Desktop:</p> </li> <li>Docker Desktop \u2192 Troubleshoot \u2192 Reset to factory defaults</li> <li>WARNING: This deletes all containers and images</li> </ol>"},{"location":"DOCKER_ARCHITECTURE/#container-fails-with-out-of-memory","title":"Container Fails with \"Out of Memory\"","text":"<p>Symptom: Container starts but crashes with OOM errors</p> <p>Check: 1. Docker Desktop allocation: <pre><code>docker info | grep \"Total Memory\"\n</code></pre></p> <ol> <li> <p>Container limit: <pre><code>docker inspect CONTAINER_ID | grep Memory\n</code></pre></p> </li> <li> <p>Model requirements:</p> </li> <li>Check <code>OLLAMA_DOCKER_MEMORY</code> in <code>.env</code></li> <li>Verify it matches model needs (see table above)</li> </ol> <p>Fix: <pre><code># Run the automated increase script\ncd scripts\n./increase-docker-memory.sh --yes\n\n# Or manually increase Docker Desktop memory via GUI\n</code></pre></p>"},{"location":"DOCKER_ARCHITECTURE/#unable-to-find-image-ollamaollamalatest","title":"\"Unable to find image 'ollama/ollama:latest'\"","text":"<p>Symptom: Ollama container fails to start</p> <p>Cause: Image not pulled yet (expected on first run)</p> <p>Fix: <pre><code># The startup script handles this, but manual pull:\ndocker pull ollama/ollama:latest\n</code></pre></p>"},{"location":"DOCKER_ARCHITECTURE/#docker-daemon-not-responding-after-start","title":"Docker Daemon Not Responding After Start","text":"<p>Symptom: <code>docker info</code> returns error after launching Docker Desktop</p> <p>Cause: Daemon still initializing (can take 30-60 seconds)</p> <p>Check: <pre><code># Wait and retry\nfor i in {1..60}; do\n    docker info &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"Ready!\" &amp;&amp; break\n    echo \"Waiting... ($i/60)\"\n    sleep 1\ndone\n</code></pre></p> <p>If still fails after 60 seconds: 1. Check Console.app for Docker errors 2. Check Activity Monitor for Docker processes 3. Try restarting Mac (last resort)</p>"},{"location":"DOCKER_ARCHITECTURE/#automated-script-hangs","title":"Automated Script Hangs","text":"<p>Symptom: <code>increase-docker-memory.sh</code> hangs during Docker restart</p> <p>Debug: <pre><code># Run with manual prompts to see progress\ncd scripts\n./increase-docker-memory.sh  # without --yes flag\n\n# Check Docker process\npgrep -x \"Docker\"\n\n# Check daemon\ndocker info\n</code></pre></p> <p>Common causes: 1. Docker.app not installed at <code>/Applications/Docker.app</code> 2. Permissions issues with settings file 3. Docker Desktop GUI stuck in update/crash loop</p> <p>Solutions: 1. Verify Docker.app location: <pre><code>ls -la /Applications/Docker.app\n</code></pre></p> <ol> <li> <p>Check settings file permissions: <pre><code>ls -la ~/Library/Group\\ Containers/group.com.docker/settings.json\n</code></pre></p> </li> <li> <p>Force quit all Docker processes: <pre><code>pkill -9 -x \"Docker\"\npkill -9 -f \"com.docker\"\nsleep 3\nopen -a Docker\n</code></pre></p> </li> </ol>"},{"location":"DOCKER_ARCHITECTURE/#advanced-topics","title":"Advanced Topics","text":""},{"location":"DOCKER_ARCHITECTURE/#docker-desktop-architecture-on-macos","title":"Docker Desktop Architecture on macOS","text":"<p>Docker Desktop uses HyperKit (a lightweight hypervisor) to run a minimal Linux VM:</p> <ol> <li>HyperKit creates the VM</li> <li>LinuxKit provides the minimal Linux environment</li> <li>containerd manages container lifecycle</li> <li>Docker daemon provides the API</li> </ol> <p>Your containers run inside the LinuxKit VM, not directly on macOS.</p>"},{"location":"DOCKER_ARCHITECTURE/#memory-management-internals","title":"Memory Management Internals","text":"<p>At the hypervisor level: - HyperKit allocates physical RAM pages from macOS - Creates guest physical memory space for Linux VM - Cannot dynamically resize without restart</p> <p>At the Linux level (inside VM): - cgroups enforce container memory limits - OOM killer terminates processes exceeding limits - Can be adjusted without restarting VM</p>"},{"location":"DOCKER_ARCHITECTURE/#performance-considerations","title":"Performance Considerations","text":"<p>Docker Desktop allocation: - Too little: Containers OOM, poor performance - Too much: Less RAM for macOS, potential swapping - Sweet spot: 50-70% of total RAM for Docker</p> <p>Example for 16GB Mac: - Docker: 10GB (Ollama + other services) - macOS: 6GB (system + apps)</p> <p>With 8GB Mac: - Docker: 5GB (smaller Ollama model) - macOS: 3GB (minimal) - May need to close other apps</p>"},{"location":"DOCKER_ARCHITECTURE/#references","title":"References","text":"<ul> <li>Docker Desktop for Mac documentation</li> <li>HyperKit GitHub</li> <li>LinuxKit GitHub</li> <li>Docker Memory Limits</li> </ul>"},{"location":"FAQ/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"FAQ/#general-questions","title":"General Questions","text":""},{"location":"FAQ/#what-is-zetherion-ai","title":"What is Zetherion AI?","text":"<p>Zetherion AI is a secure, intelligent Discord bot with vector-based memory. It can remember conversations, answer questions, and assist with complex tasks using multiple AI models (Gemini, Claude, GPT-4).</p>"},{"location":"FAQ/#is-zetherion-ai-free-to-use","title":"Is Zetherion AI free to use?","text":"<p>The bot itself is open source and free. However, you need API keys: - Gemini API - Free tier available (sufficient for personal use) - Discord Bot - Free - Claude/GPT-4 - Optional, paid tiers only</p>"},{"location":"FAQ/#can-i-use-zetherion-ai-in-production","title":"Can I use Zetherion AI in production?","text":"<p>Yes, but: - Set <code>ALLOWED_USER_IDS</code> to restrict access - Use proper API rate limits - Monitor costs for paid API usage - Consider enabling additional security features</p>"},{"location":"FAQ/#does-zetherion-ai-store-my-data","title":"Does Zetherion AI store my data?","text":"<p>Yes, Zetherion AI stores: - Conversation history - In Qdrant vector database (local to your machine) - Long-term memories - Things you explicitly ask it to remember - Nothing is sent to third parties - All data stays on your infrastructure</p> <p>Data is stored locally in the <code>qdrant_storage/</code> directory.</p>"},{"location":"FAQ/#setup-questions","title":"Setup Questions","text":""},{"location":"FAQ/#what-hardware-do-i-need","title":"What hardware do I need?","text":"<p>Minimum: - 4GB RAM - 2GB free disk space - macOS/Linux (Windows with WSL)</p> <p>Recommended: - 8GB+ RAM - 10GB free disk space - SSD storage</p>"},{"location":"FAQ/#can-i-run-this-on-windows","title":"Can I run this on Windows?","text":"<p>Yes, but with modifications: 1. Install Docker Desktop for Windows 2. Use WSL2 (Windows Subsystem for Linux) 3. Run commands in WSL2 terminal 4. Scripts will need adjustment (or use Docker Compose instead)</p>"},{"location":"FAQ/#can-i-run-this-on-a-raspberry-pi","title":"Can I run this on a Raspberry Pi?","text":"<p>Possibly, but not recommended: - Requires 64-bit OS - Minimum 4GB RAM model - Performance will be limited - Qdrant may struggle with large datasets</p>"},{"location":"FAQ/#do-i-need-all-three-api-keys","title":"Do I need all three API keys?","text":"<p>Required: - Discord Token - Gemini API Key</p> <p>Optional (but recommended): - Anthropic (Claude) - For better quality on complex tasks - OpenAI (GPT-4) - Alternative to Claude</p> <p>Without Claude/GPT-4, all queries use Gemini Flash (still very capable).</p>"},{"location":"FAQ/#usage-questions","title":"Usage Questions","text":""},{"location":"FAQ/#how-do-i-talk-to-the-bot","title":"How do I talk to the bot?","text":"<p>In Discord: - DM the bot directly - Just send a message - Mention in server - <code>@Zetherion AI your message here</code> - Slash commands - <code>/ask</code>, <code>/remember</code>, <code>/search</code>, <code>/ping</code></p>"},{"location":"FAQ/#whats-the-difference-between-ask-and-mentioning","title":"What's the difference between <code>/ask</code> and mentioning?","text":"<p>None functionally - both do the same thing. Use whatever is more convenient: - <code>/ask</code> - More explicit, good for servers - Mentioning - More natural, like talking to a person</p>"},{"location":"FAQ/#can-it-remember-previous-conversations","title":"Can it remember previous conversations?","text":"<p>Yes! Zetherion AI automatically: - Remembers recent conversation context (last 20 messages) - Searches relevant past conversations using vector similarity - Recalls explicitly stored memories</p>"},{"location":"FAQ/#how-do-i-make-it-remember-something-specific","title":"How do I make it remember something specific?","text":"<pre><code># Any of these work:\n/remember I prefer dark mode\n\"Remember that I'm a Python developer\"\n\"Note: My birthday is March 15\"\n</code></pre>"},{"location":"FAQ/#how-do-i-search-my-memories","title":"How do I search my memories?","text":"<pre><code>/search preferences\n/search birthday\n/search python projects\n</code></pre> <p>Returns the 5 most relevant memories with similarity scores.</p>"},{"location":"FAQ/#can-i-delete-memories","title":"Can I delete memories?","text":"<p>Not yet via commands, but you can: <pre><code># Delete all memories:\n./stop.sh\nrm -rf qdrant_storage/\n./start.sh\n</code></pre></p> <p>Future versions will add <code>/forget</code> command.</p>"},{"location":"FAQ/#technical-questions","title":"Technical Questions","text":""},{"location":"FAQ/#what-ai-models-does-it-use","title":"What AI models does it use?","text":"<p>Routing &amp; Simple Queries: - Gemini 2.0 Flash (fast, cheap, handles 90% of queries)</p> <p>Complex Tasks: - Claude 3.5 Sonnet (default for code, analysis, creative tasks) - GPT-4 (alternative, configure via OPENAI_API_KEY)</p> <p>Embeddings: - Gemini text-embedding-004 (768 dimensions)</p>"},{"location":"FAQ/#how-does-the-routing-work","title":"How does the routing work?","text":"<ol> <li>User sends message</li> <li>Gemini Flash analyzes intent + complexity</li> <li>If simple (greeting, factual question) \u2192 Gemini Flash responds</li> <li>If complex (code, analysis) \u2192 Routes to Claude/GPT-4</li> </ol> <p>Threshold: 70% confidence that task is complex</p>"},{"location":"FAQ/#what-is-qdrant","title":"What is Qdrant?","text":"<p>Qdrant is a vector database that stores: - Conversation history as semantic vectors - Long-term memories - Enables similarity search (find related past conversations)</p> <p>Think of it like a smart search engine for your conversations.</p>"},{"location":"FAQ/#how-much-does-it-cost-to-run","title":"How much does it cost to run?","text":"<p>Free Tier Usage (Gemini only): - ~1000 messages/day on free tier - $0/month</p> <p>With Claude API (Recommended): - ~$0.003 per message (simple) - ~$0.03 per complex task - ~$5-20/month for personal use</p> <p>With GPT-4: - ~$0.01 per message - ~$10-30/month for personal use</p>"},{"location":"FAQ/#how-do-i-reduce-costs","title":"How do I reduce costs?","text":"<ol> <li>Use Gemini-only (remove Claude/OpenAI keys)</li> <li>Increase routing threshold (fewer complex tasks)</li> <li>Reduce memory context limits</li> <li>Set <code>ALLOWED_USER_IDS</code> to restrict usage</li> </ol>"},{"location":"FAQ/#can-i-use-different-models","title":"Can I use different models?","text":"<p>Yes! Edit <code>.env</code>: <pre><code># Use Claude Haiku (cheaper, faster):\nCLAUDE_MODEL=claude-3-haiku-20240307\n\n# Use GPT-3.5 instead of GPT-4:\nOPENAI_MODEL=gpt-3.5-turbo\n\n# Use different Gemini model:\nROUTER_MODEL=gemini-1.5-flash\n</code></pre></p> <p>See <code>src/zetherion_ai/config.py</code> for all model options.</p>"},{"location":"FAQ/#security-questions","title":"Security Questions","text":""},{"location":"FAQ/#is-it-safe-to-put-api-keys-in-env","title":"Is it safe to put API keys in .env?","text":"<p>Yes, if: - <code>.env</code> is in <code>.gitignore</code> (it is by default) - You don't commit it to GitHub - File permissions are restricted: <code>chmod 600 .env</code></p> <p>Never: - Share <code>.env</code> file - Commit it to Git - Post it in Discord/forums</p>"},{"location":"FAQ/#should-i-enable-the-user-allowlist","title":"Should I enable the user allowlist?","text":"<p>For personal use: Not required, but recommended <pre><code>ALLOWED_USER_IDS=your_discord_id\n</code></pre></p> <p>For server use: CRITICAL <pre><code>ALLOWED_USER_IDS=id1,id2,id3\n</code></pre></p> <p>Otherwise anyone in the server can use (and rack up API costs).</p>"},{"location":"FAQ/#what-about-prompt-injection-attacks","title":"What about prompt injection attacks?","text":"<p>Zetherion AI has built-in protection: - 17 regex patterns detect injection attempts - Unicode obfuscation detection - Excessive role-play marker detection - Auto-rejects suspicious messages</p> <p>See <code>src/zetherion_ai/discord/security.py</code> for details.</p>"},{"location":"FAQ/#can-someone-hack-my-bot","title":"Can someone hack my bot?","text":"<p>Attack vectors: 1. Stolen Discord Token - Keep token secret, rotate if exposed 2. API Key Theft - Protect <code>.env</code> file 3. Prompt Injection - Built-in protection, but not 100% 4. Rate Limiting Abuse - Set user allowlist + rate limits</p> <p>Best Practices: - Use <code>ALLOWED_USER_IDS</code> for production - Monitor API usage dashboards - Enable Discord 2FA - Rotate tokens periodically</p>"},{"location":"FAQ/#development-questions","title":"Development Questions","text":""},{"location":"FAQ/#can-i-contribute-to-zetherion-ai","title":"Can I contribute to Zetherion AI?","text":"<p>Yes! Contributions welcome: 1. Fork the repo 2. Create feature branch 3. Make changes + add tests 4. Submit PR with description</p> <p>See CONTRIBUTING.md for guidelines.</p>"},{"location":"FAQ/#how-do-i-run-tests","title":"How do I run tests?","text":"<pre><code># Install dev dependencies:\npip install -r requirements-dev.txt\n\n# Run all tests:\npytest tests/ -v\n\n# With coverage:\npytest tests/ --cov=src/zetherion_ai --cov-report=html\n\n# Open coverage report:\nopen htmlcov/index.html\n</code></pre>"},{"location":"FAQ/#how-do-i-add-a-new-slash-command","title":"How do I add a new slash command?","text":"<ol> <li>Edit <code>src/zetherion_ai/discord/bot.py</code></li> <li>Add command in <code>_setup_commands()</code> method</li> <li>Create handler method (e.g., <code>_handle_my_command</code>)</li> <li>Restart bot - commands sync automatically</li> </ol> <p>Example: <pre><code>@self._tree.command(name=\"hello\", description=\"Say hello\")\nasync def hello_command(interaction: discord.Interaction) -&gt; None:\n    await interaction.response.send_message(\"Hello!\")\n</code></pre></p>"},{"location":"FAQ/#how-do-i-change-the-system-prompt","title":"How do I change the system prompt?","text":"<p>Edit <code>src/zetherion_ai/agent/prompts.py</code>: - <code>CLAUDE_SYSTEM_PROMPT</code> - Instructions for Claude - <code>OPENAI_SYSTEM_PROMPT</code> - Instructions for GPT-4</p> <p>Restart bot after changes.</p>"},{"location":"FAQ/#can-i-use-a-different-vector-database","title":"Can I use a different vector database?","text":"<p>Technically yes, but requires code changes: - Current: Qdrant (recommended, fast, easy) - Alternatives: Pinecone, Weaviate, Milvus</p> <p>You'd need to implement the same interface in <code>src/zetherion_ai/memory/</code>.</p>"},{"location":"FAQ/#troubleshooting","title":"Troubleshooting","text":""},{"location":"FAQ/#bot-is-not-responding","title":"Bot is not responding","text":"<p>See TROUBLESHOOTING.md</p>"},{"location":"FAQ/#getting-api-errors","title":"Getting API errors","text":"<p>See TROUBLESHOOTING.md</p>"},{"location":"FAQ/#qdrant-connection-issues","title":"Qdrant connection issues","text":"<p>See TROUBLESHOOTING.md</p>"},{"location":"FAQ/#performance-problems","title":"Performance problems","text":"<p>See TROUBLESHOOTING.md</p>"},{"location":"FAQ/#still-have-questions","title":"Still Have Questions?","text":"<ol> <li>Check TROUBLESHOOTING.md</li> <li>Search GitHub Issues</li> <li>Ask in GitHub Discussions</li> <li>Create new issue with <code>[Question]</code> tag</li> </ol>"},{"location":"FEATURES/","title":"Advanced Features Guide","text":"<p>Zetherion AI includes powerful advanced features for cost optimization, privacy, user personalization, and extensibility. This guide provides an overview of Phase 5+ features.</p>"},{"location":"FEATURES/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Feature Overview</li> <li>InferenceBroker (Multi-Provider Routing)</li> <li>Cost Tracking System</li> <li>Model Discovery &amp; Registry</li> <li>Notification System</li> <li>Profile System</li> <li>Skills Framework</li> <li>Heartbeat Scheduler</li> <li>Field-Level Encryption</li> </ul>"},{"location":"FEATURES/#feature-overview","title":"Feature Overview","text":"Feature Purpose Configuration InferenceBroker Smart multi-provider LLM routing <code>INFERENCE_BROKER_ENABLED=true</code> Cost Tracking Monitor and budget API usage <code>COST_TRACKING_ENABLED=true</code> Model Discovery Auto-discover new models <code>MODEL_DISCOVERY_ENABLED=true</code> Notifications Alerts for costs, models, errors <code>NOTIFICATIONS_ENABLED=true</code> Profile System User preference learning <code>PROFILE_INFERENCE_ENABLED=true</code> Skills Extensible task system Built-in, always available Heartbeat Proactive scheduled actions Configured per skill Encryption AES-256-GCM data protection <code>ENCRYPTION_ENABLED=true</code>"},{"location":"FEATURES/#inferencebroker-multi-provider-routing","title":"InferenceBroker (Multi-Provider Routing)","text":"<p>The InferenceBroker intelligently routes requests to the optimal LLM provider based on task type, cost, and availability.</p>"},{"location":"FEATURES/#how-it-works","title":"How It Works","text":"<ol> <li>Task Classification: Each query is classified by type (simple, complex, code, creative)</li> <li>Provider Selection: Best provider chosen based on:</li> <li>Task requirements (quality vs speed)</li> <li>Provider availability and health</li> <li>Cost optimization preferences</li> <li>Configured tier preferences</li> <li>Fallback Chain: If primary provider fails, automatically tries alternatives</li> </ol>"},{"location":"FEATURES/#supported-providers","title":"Supported Providers","text":"Provider Best For Cost Speed Gemini Flash Simple queries, routing Free tier Fastest Claude Sonnet Complex reasoning, code $3/M tokens Fast GPT-4o General tasks $2.50/M tokens Fast Ollama Privacy, offline Free (local) Varies"},{"location":"FEATURES/#configuration","title":"Configuration","text":"<pre><code># Enable InferenceBroker\nINFERENCE_BROKER_ENABLED=true\n\n# Provider tier preferences (quality, balanced, fast)\nANTHROPIC_TIER=quality\nOPENAI_TIER=balanced\nGOOGLE_TIER=fast\n</code></pre>"},{"location":"FEATURES/#task-to-provider-mapping","title":"Task-to-Provider Mapping","text":"Task Type Default Provider Fallback Simple Query Gemini Flash Ollama Complex Reasoning Claude Sonnet GPT-4o Code Generation Claude Sonnet GPT-4o Creative Writing GPT-4o Claude Memory Operations Gemini Flash Ollama"},{"location":"FEATURES/#cost-tracking-system","title":"Cost Tracking System","text":"<p>Monitor API usage, set budgets, and receive alerts before exceeding limits.</p>"},{"location":"FEATURES/#features","title":"Features","text":"<ul> <li>Real-time tracking: Every API call logged with cost</li> <li>Budget alerts: Warnings at configurable thresholds</li> <li>Daily/Monthly summaries: Aggregate spending reports</li> <li>Per-provider breakdown: See costs by provider and task type</li> <li>SQLite storage: Persistent, queryable cost database</li> </ul>"},{"location":"FEATURES/#configuration_1","title":"Configuration","text":"<pre><code># Enable cost tracking\nCOST_TRACKING_ENABLED=true\n\n# Budget configuration\nDAILY_BUDGET_USD=5.00\nMONTHLY_BUDGET_USD=50.00\nBUDGET_WARNING_PCT=80.0  # Alert at 80% of budget\n\n# Storage\nCOST_DB_PATH=data/costs.db\n</code></pre>"},{"location":"FEATURES/#budget-alerts","title":"Budget Alerts","text":"<p>When spending reaches thresholds: - 80% warning: Notification sent, bot continues - 100% exceeded: Notification sent, complex queries may be limited</p>"},{"location":"FEATURES/#viewing-cost-reports","title":"Viewing Cost Reports","text":"<pre><code># View cost summary (when implemented)\ndocker exec zetherion-ai-bot python -c \"\nfrom zetherion_ai.costs import CostTracker\ntracker = CostTracker()\nprint(tracker.get_daily_summary())\n\"\n</code></pre> <p>See Cost Tracking Guide for detailed usage.</p>"},{"location":"FEATURES/#model-discovery-registry","title":"Model Discovery &amp; Registry","text":"<p>Automatically discover new models from provider APIs and track deprecations.</p>"},{"location":"FEATURES/#features_1","title":"Features","text":"<ul> <li>Auto-discovery: Polls provider APIs every 24 hours</li> <li>Tier classification: Models categorized as quality/balanced/fast</li> <li>Deprecation tracking: Alerts when models are deprecated</li> <li>Pricing updates: Automatic pricing data refresh</li> <li>New model notifications: Alerts for newly available models</li> </ul>"},{"location":"FEATURES/#configuration_2","title":"Configuration","text":"<pre><code># Enable model discovery\nMODEL_DISCOVERY_ENABLED=true\n\n# Refresh interval (hours)\nMODEL_REFRESH_HOURS=24\n\n# Notifications for model changes\nNOTIFY_ON_NEW_MODELS=true\nNOTIFY_ON_DEPRECATION=true\nNOTIFY_ON_MISSING_PRICING=true\n</code></pre>"},{"location":"FEATURES/#model-tiers","title":"Model Tiers","text":"Tier Characteristics Example Models Quality Best results, higher cost Claude Sonnet, GPT-4o Balanced Good results, moderate cost Claude Haiku, GPT-4o-mini Fast Quick responses, lower cost Gemini Flash, Ollama"},{"location":"FEATURES/#notification-system","title":"Notification System","text":"<p>Receive alerts for important events via Discord.</p>"},{"location":"FEATURES/#notification-types","title":"Notification Types","text":"Type Trigger Priority <code>MODEL_DISCOVERED</code> New model available LOW <code>MODEL_DEPRECATED</code> Model being retired HIGH <code>BUDGET_WARNING</code> 80% of budget reached HIGH <code>BUDGET_EXCEEDED</code> 100% of budget reached CRITICAL <code>DAILY_SUMMARY</code> End of day report LOW <code>RATE_LIMIT_HIT</code> API rate limited MEDIUM <code>SYSTEM_ERROR</code> Unexpected error CRITICAL"},{"location":"FEATURES/#configuration_3","title":"Configuration","text":"<pre><code># Enable notifications\nNOTIFICATIONS_ENABLED=true\n\n# Notification preferences\nNOTIFY_ON_NEW_MODELS=true\nNOTIFY_ON_DEPRECATION=true\nNOTIFY_ON_MISSING_PRICING=false\n\n# Daily summary\nDAILY_SUMMARY_ENABLED=true\nDAILY_SUMMARY_HOUR=20  # 8 PM local time\n</code></pre>"},{"location":"FEATURES/#notification-channels","title":"Notification Channels","text":"<p>Notifications are sent via Discord DM to allowed users. Future versions may support: - Slack webhooks - Email alerts - Custom webhooks</p>"},{"location":"FEATURES/#profile-system","title":"Profile System","text":"<p>Learn user preferences and adapt responses over time.</p>"},{"location":"FEATURES/#features_2","title":"Features","text":"<ul> <li>Automatic extraction: Learns from conversations</li> <li>Confidence scoring: Tracks certainty of learned facts</li> <li>Confirmation workflow: Asks to confirm uncertain information</li> <li>Privacy controls: Users can view, update, or delete their profile</li> <li>GDPR compliance: Full data export and deletion</li> </ul>"},{"location":"FEATURES/#profile-categories","title":"Profile Categories","text":"Category Examples Identity Name, location, timezone Preferences Coding style, verbosity preference Schedule Work hours, availability Projects Current projects, technologies Relationships Team members, managers Skills Programming languages, expertise Goals Learning objectives, deadlines Habits Communication style, shortcuts"},{"location":"FEATURES/#configuration_4","title":"Configuration","text":"<pre><code># Enable profile inference\nPROFILE_INFERENCE_ENABLED=true\n\n# Inference settings\nPROFILE_CONFIDENCE_THRESHOLD=0.6  # Minimum confidence to auto-apply\nPROFILE_CACHE_TTL=300  # Cache for 5 minutes\n\n# Response customization defaults\nDEFAULT_FORMALITY=0.5  # 0=casual, 1=formal\nDEFAULT_VERBOSITY=0.5  # 0=brief, 1=detailed\nDEFAULT_PROACTIVITY=0.5  # 0=reactive, 1=proactive\n</code></pre>"},{"location":"FEATURES/#user-commands","title":"User Commands","text":"<pre><code>@Zetherion AI show my profile\n@Zetherion AI update my name to James\n@Zetherion AI forget my location\n@Zetherion AI export my data\n</code></pre> <p>See Profile System Guide for detailed usage.</p>"},{"location":"FEATURES/#skills-framework","title":"Skills Framework","text":"<p>Extensible system for adding new capabilities.</p>"},{"location":"FEATURES/#built-in-skills","title":"Built-in Skills","text":"Skill Purpose Intents Task Manager Track tasks and todos Create, list, complete, delete tasks Calendar Schedule awareness Check availability, work hours Profile User preferences View, update, export profile"},{"location":"FEATURES/#task-manager-examples","title":"Task Manager Examples","text":"<pre><code>@Zetherion AI add task: Review PR #123\n@Zetherion AI list my tasks\n@Zetherion AI complete task 1\n@Zetherion AI show task summary\n</code></pre>"},{"location":"FEATURES/#calendar-examples","title":"Calendar Examples","text":"<pre><code>@Zetherion AI what's my schedule today?\n@Zetherion AI am I free at 3pm?\n@Zetherion AI when are my work hours?\n</code></pre>"},{"location":"FEATURES/#skill-intents","title":"Skill Intents","text":"<p>The router classifies messages into intents:</p> Intent Examples <code>TASK_MANAGEMENT</code> \"add task\", \"list todos\", \"complete\" <code>CALENDAR_QUERY</code> \"schedule\", \"free\", \"availability\" <code>PROFILE_QUERY</code> \"my profile\", \"update preference\" <code>SIMPLE_QUERY</code> \"what is...\", \"explain...\" <code>COMPLEX_TASK</code> \"analyze this code\", \"help me debug\" <code>MEMORY_STORE</code> \"remember that...\", \"note that...\" <code>MEMORY_RECALL</code> \"search for...\", \"what did I say about...\" <p>See Skills Guide for detailed usage and custom skill development.</p>"},{"location":"FEATURES/#heartbeat-scheduler","title":"Heartbeat Scheduler","text":"<p>Proactive behavior system for scheduled actions.</p>"},{"location":"FEATURES/#features_3","title":"Features","text":"<ul> <li>Periodic execution: Skills run on configurable intervals</li> <li>Quiet hours: Respects user availability</li> <li>Priority queue: Important actions first</li> <li>Rate limiting: Maximum actions per heartbeat</li> </ul>"},{"location":"FEATURES/#configuration_5","title":"Configuration","text":"<pre><code># Heartbeat runs every 5 minutes by default\n# Quiet hours: 10 PM to 7 AM (configurable per skill)\n</code></pre>"},{"location":"FEATURES/#proactive-actions","title":"Proactive Actions","text":"<p>Skills can define heartbeat actions: - Task reminders: Notify about upcoming deadlines - Daily summaries: Send end-of-day recaps - Cost alerts: Warn about budget thresholds - Health checks: Monitor system status</p>"},{"location":"FEATURES/#field-level-encryption","title":"Field-Level Encryption","text":"<p>AES-256-GCM encryption for sensitive data at rest.</p>"},{"location":"FEATURES/#whats-encrypted","title":"What's Encrypted","text":"<ul> <li>Memory content (conversations, facts)</li> <li>Profile data (personal information)</li> <li>Task details (descriptions, notes)</li> <li>Calendar events (titles, descriptions)</li> </ul>"},{"location":"FEATURES/#whats-not-encrypted","title":"What's NOT Encrypted","text":"<ul> <li>Metadata (timestamps, IDs, user IDs)</li> <li>Vector embeddings (required for search)</li> <li>Configuration data</li> </ul>"},{"location":"FEATURES/#configuration_6","title":"Configuration","text":"<pre><code># Enable encryption\nENCRYPTION_ENABLED=true\n\n# Strong passphrase (minimum 16 characters)\nENCRYPTION_PASSPHRASE=your-very-secure-passphrase-here\n\n# Salt file location\nENCRYPTION_SALT_PATH=data/.encryption_salt\n</code></pre>"},{"location":"FEATURES/#security-notes","title":"Security Notes","text":"<ul> <li>Passphrase storage: Keep backup in secure location (not git)</li> <li>Key rotation: Requires data migration (planned feature)</li> <li>Recovery: Cannot decrypt without passphrase</li> <li>Performance: Minimal overhead (~1-2ms per operation)</li> </ul> <p>See Security Guide for comprehensive security documentation.</p>"},{"location":"FEATURES/#enabling-all-features","title":"Enabling All Features","text":"<p>For maximum functionality, enable all Phase 5+ features:</p> <pre><code># Core features\nINFERENCE_BROKER_ENABLED=true\nCOST_TRACKING_ENABLED=true\nMODEL_DISCOVERY_ENABLED=true\nNOTIFICATIONS_ENABLED=true\nPROFILE_INFERENCE_ENABLED=true\nENCRYPTION_ENABLED=true\n\n# Budgets\nDAILY_BUDGET_USD=10.00\nMONTHLY_BUDGET_USD=100.00\n\n# Encryption (generate with: openssl rand -base64 32)\nENCRYPTION_PASSPHRASE=your-secure-passphrase\n\n# Notifications\nNOTIFY_ON_NEW_MODELS=true\nNOTIFY_ON_DEPRECATION=true\nDAILY_SUMMARY_ENABLED=true\n</code></pre>"},{"location":"FEATURES/#feature-dependencies","title":"Feature Dependencies","text":"<p>Some features depend on others:</p> Feature Requires Cost Tracking InferenceBroker Budget Alerts Cost Tracking + Notifications Model Notifications Model Discovery + Notifications Daily Summaries Cost Tracking + Notifications"},{"location":"FEATURES/#additional-resources","title":"Additional Resources","text":"<ul> <li>Skills Guide - Detailed skills documentation</li> <li>Cost Tracking Guide - Budget management</li> <li>Profile System Guide - User personalization</li> <li>Security Guide - Encryption and security</li> <li>Configuration Reference - All settings</li> </ul> <p>Last Updated: 2026-02-07 Version: 3.0.0 (Phase 5+ Features)</p>"},{"location":"GITHUB_SECRETS/","title":"GitHub Secrets Configuration","text":"<p>This document lists all GitHub secrets for Zetherion AI's CI/CD pipeline.</p> <p>\u26a0\ufe0f Important: As of v1.1.0, integration tests run locally only. GitHub CI runs only unit tests, linting, type checking, security scans, and Docker builds. No secrets are required for CI/CD to pass.</p>"},{"location":"GITHUB_SECRETS/#quick-reference","title":"\ud83d\udccb Quick Reference","text":"Secret Name Required? Purpose Used In <code>DISCORD_TOKEN</code> \u26a0\ufe0f Optional Production bot token Local integration tests only <code>GEMINI_API_KEY</code> \u26a0\ufe0f Optional Gemini API for routing &amp; embeddings Local integration tests only <code>ANTHROPIC_API_KEY</code> \u26a0\ufe0f Optional Claude API for complex tasks Local integration tests only <code>OPENAI_API_KEY</code> \u26a0\ufe0f Optional OpenAI API for complex tasks Local integration tests only <code>TEST_DISCORD_BOT_TOKEN</code> \u26a0\ufe0f Optional Test bot token for E2E tests Local Discord E2E tests only <code>TEST_DISCORD_CHANNEL_ID</code> \u26a0\ufe0f Optional Test channel ID for E2E tests Local Discord E2E tests only <p>All secrets are now optional - they're only needed if you want to run integration tests locally.</p>"},{"location":"GITHUB_SECRETS/#cicd-pipeline-no-secrets-required","title":"\u2705 CI/CD Pipeline (No Secrets Required)","text":"<p>The GitHub Actions CI/CD pipeline runs: - \u2705 Linting &amp; Formatting - \u2705 Type Checking - \u2705 Security Scanning - \u2705 Unit Tests (Python 3.12 &amp; 3.13) - \u2705 Docker Build Test</p> <p>These require no secrets and will pass out of the box.</p>"},{"location":"GITHUB_SECRETS/#local-integration-testing-optional","title":"\ud83e\uddea Local Integration Testing (Optional)","text":""},{"location":"GITHUB_SECRETS/#1-discord_token","title":"1. DISCORD_TOKEN","text":"<p>Purpose: Your production Discord bot token for local integration testing.</p> <p>How to get it: 1. Go to Discord Developer Portal 2. Select your application 3. Go to \"Bot\" section 4. Click \"Reset Token\" or copy existing token 5. Copy the token immediately (it won't be shown again)</p> <p>Format: Three dot-separated parts totaling 70+ characters</p> <p>Security Notes: - This token grants full access to your bot - Never commit this to version control - Regenerate if accidentally exposed</p> <p>Add to GitHub: <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\nName: DISCORD_TOKEN\nValue: &lt;your-token-here&gt;\n</code></pre></p>"},{"location":"GITHUB_SECRETS/#2-gemini_api_key","title":"2. GEMINI_API_KEY","text":"<p>Purpose: Google Gemini API key for routing, embeddings, and simple queries.</p> <p>How to get it: 1. Go to Google AI Studio 2. Click \"Create API Key\" 3. Select or create a Google Cloud project 4. Copy the API key</p> <p>Format: <code>AIzaSy...</code> followed by 33 alphanumeric characters</p> <p>Free Tier: - 60 requests per minute - 1,500 requests per day - Sufficient for CI/CD testing</p> <p>Add to GitHub: <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\nName: GEMINI_API_KEY\nValue: &lt;your-api-key-here&gt;\n</code></pre></p>"},{"location":"GITHUB_SECRETS/#optional-secrets","title":"\u26a0\ufe0f Optional Secrets","text":""},{"location":"GITHUB_SECRETS/#3-anthropic_api_key-optional","title":"3. ANTHROPIC_API_KEY (Optional)","text":"<p>Purpose: Claude API for handling complex tasks and code generation.</p> <p>Required for: - Testing Claude-based response generation - If not provided, tests will use Gemini for all queries</p> <p>How to get it: 1. Go to Anthropic Console 2. Navigate to \"API Keys\" 3. Click \"Create Key\" 4. Copy the API key</p> <p>Format: <code>sk-ant-api03-</code> followed by 95-100 alphanumeric characters</p> <p>Pricing: - Pay-as-you-go - Claude Sonnet 4.5: $3 per million input tokens, $15 per million output tokens - CI usage: ~$0.10-0.50 per pipeline run</p> <p>Add to GitHub: <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\nName: ANTHROPIC_API_KEY\nValue: &lt;your-api-key-here&gt;\n</code></pre></p>"},{"location":"GITHUB_SECRETS/#4-openai_api_key-optional","title":"4. OPENAI_API_KEY (Optional)","text":"<p>Purpose: OpenAI API for alternative complex task handling.</p> <p>Required for: - Testing OpenAI-based response generation - If not provided, tests will use Claude or Gemini</p> <p>How to get it: 1. Go to OpenAI Platform 2. Click \"Create new secret key\" 3. Name it (e.g., \"Zetherion AI CI\") 4. Copy the API key immediately</p> <p>Format: <code>sk-proj-</code> followed by ~48 alphanumeric characters</p> <p>Pricing: - Pay-as-you-go - GPT-4o: $2.50 per million input tokens, $10 per million output tokens - CI usage: ~$0.05-0.30 per pipeline run</p> <p>Add to GitHub: <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\nName: OPENAI_API_KEY\nValue: &lt;your-api-key-here&gt;\n</code></pre></p>"},{"location":"GITHUB_SECRETS/#5-test_discord_bot_token-optional","title":"5. TEST_DISCORD_BOT_TOKEN (Optional)","text":"<p>Purpose: Separate Discord bot token for end-to-end testing with real Discord API.</p> <p>Required for: - Discord E2E tests (<code>test_discord_e2e.py</code>) - Testing real bot responses, slash commands, and message handling - If not provided, Discord E2E tests are skipped</p> <p>How to get it: 1. Create a separate Discord application for testing 2. Go to Discord Developer Portal 3. Click \"New Application\" 4. Name it \"Zetherion AI Test Bot\" 5. Go to \"Bot\" section 6. Copy the bot token</p> <p>Important: - Use a DIFFERENT bot than your production bot - Add this test bot to a dedicated test server - Give it minimal permissions (Read Messages, Send Messages)</p> <p>Format: Same as <code>DISCORD_TOKEN</code></p> <p>Add to GitHub: <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\nName: TEST_DISCORD_BOT_TOKEN\nValue: &lt;your-test-bot-token-here&gt;\n</code></pre></p>"},{"location":"GITHUB_SECRETS/#6-test_discord_channel_id-optional","title":"6. TEST_DISCORD_CHANNEL_ID (Optional)","text":"<p>Purpose: Discord channel ID where the test bot will send messages.</p> <p>Required for: - Discord E2E tests alongside <code>TEST_DISCORD_BOT_TOKEN</code> - If not provided, Discord E2E tests are skipped</p> <p>How to get it: 1. Enable Developer Mode in Discord:    - User Settings \u2192 Advanced \u2192 Developer Mode (toggle ON) 2. Right-click the test channel 3. Click \"Copy Channel ID\"</p> <p>Format: <code>1234567890123456789</code> (18-19 digits)</p> <p>Important: - Use a dedicated test channel - The test bot must have access to this channel - Messages will be posted during CI runs</p> <p>Add to GitHub: <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\nName: TEST_DISCORD_CHANNEL_ID\nValue: &lt;your-channel-id-here&gt;\n</code></pre></p>"},{"location":"GITHUB_SECRETS/#quick-setup-guide","title":"\ud83d\ude80 Quick Setup Guide","text":""},{"location":"GITHUB_SECRETS/#for-github-cicd-no-setup-needed","title":"For GitHub CI/CD (No Setup Needed!)","text":"<p>No secrets required! The CI/CD pipeline runs unit tests, linting, type checking, security scans, and Docker builds - all without any secrets.</p> <p>Just push your code and CI will pass automatically.</p>"},{"location":"GITHUB_SECRETS/#for-local-integration-tests-optional","title":"For Local Integration Tests (Optional)","text":"<p>If you want to run integration tests locally, add these to your <code>.env</code> file:</p> <pre><code># Required for local integration tests\nDISCORD_TOKEN=&lt;your-production-bot-token&gt;\nGEMINI_API_KEY=&lt;your-gemini-api-key&gt;\n\n# Optional (improves test coverage)\nANTHROPIC_API_KEY=&lt;your-claude-api-key&gt;\nOPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre> <p>Run with: <code>./scripts/run-integration-tests.sh</code></p>"},{"location":"GITHUB_SECRETS/#for-local-discord-e2e-tests-optional","title":"For Local Discord E2E Tests (Optional)","text":"<p>To run Discord E2E tests locally with real Discord API:</p> <pre><code># Required for Discord E2E tests\nDISCORD_TOKEN=&lt;your-production-bot-token&gt;\nGEMINI_API_KEY=&lt;your-gemini-api-key&gt;\nTEST_DISCORD_BOT_TOKEN=&lt;your-test-bot-token&gt;\nTEST_DISCORD_CHANNEL_ID=&lt;your-test-channel-id&gt;\n\n# Optional but recommended\nANTHROPIC_API_KEY=&lt;your-claude-api-key&gt;\nOPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre> <p>Run with: <code>pytest tests/integration/test_discord_e2e.py -v -s -m discord_e2e</code></p>"},{"location":"GITHUB_SECRETS/#adding-secrets-to-github-optional","title":"\ud83d\udcdd Adding Secrets to GitHub (Optional)","text":"<p>Note: GitHub secrets are optional now since integration tests run locally only.</p> <p>If you want to add secrets for future use:</p>"},{"location":"GITHUB_SECRETS/#via-github-web-ui","title":"Via GitHub Web UI","text":"<ol> <li>Go to your repository on GitHub</li> <li>Click Settings (top menu)</li> <li>In the left sidebar, click Secrets and variables \u2192 Actions</li> <li>Click New repository secret</li> <li>Enter the Name (exactly as shown above)</li> <li>Enter the Value (your API key/token)</li> <li>Click Add secret</li> </ol>"},{"location":"GITHUB_SECRETS/#via-github-cli","title":"Via GitHub CLI","text":"<pre><code># All secrets are optional for local testing\ngh secret set DISCORD_TOKEN\ngh secret set GEMINI_API_KEY\ngh secret set ANTHROPIC_API_KEY\ngh secret set OPENAI_API_KEY\ngh secret set TEST_DISCORD_BOT_TOKEN\ngh secret set TEST_DISCORD_CHANNEL_ID\n</code></pre> <p>You'll be prompted to paste the value for each secret.</p>"},{"location":"GITHUB_SECRETS/#verifying-cicd","title":"\ud83d\udd0d Verifying CI/CD","text":""},{"location":"GITHUB_SECRETS/#check-ci-status","title":"Check CI Status","text":"<p>Push a commit and check the GitHub Actions tab. You should see:</p>"},{"location":"GITHUB_SECRETS/#expected-ci-output-no-secrets-needed","title":"Expected CI Output (No Secrets Needed)","text":"<p>All CI jobs should pass without any secrets:</p> <pre><code>\u2705 Linting &amp; Formatting\n\u2705 Type Checking\n\u2705 Security Scanning\n\u2705 Unit Tests (Python 3.12)\n\u2705 Unit Tests (Python 3.13)\n\u2705 Docker Build\n\u2705 CI Summary\n\nNote: Integration tests (E2E) run locally only.\n</code></pre>"},{"location":"GITHUB_SECRETS/#security-best-practices","title":"\ud83d\udd10 Security Best Practices","text":""},{"location":"GITHUB_SECRETS/#do","title":"DO \u2705","text":"<ul> <li>Use separate test bot tokens from production</li> <li>Regenerate tokens if accidentally exposed</li> <li>Use API keys with minimal required permissions</li> <li>Monitor API usage dashboards for unexpected activity</li> <li>Set up billing alerts for paid APIs</li> </ul>"},{"location":"GITHUB_SECRETS/#dont","title":"DON'T \u274c","text":"<ul> <li>Never commit secrets to version control</li> <li>Never share secrets in Discord, Slack, or email</li> <li>Never use production bot tokens in public CI</li> <li>Never skip secret rotation after team member departures</li> <li>Never set secrets as environment variables in CI config files</li> </ul>"},{"location":"GITHUB_SECRETS/#cost-estimates","title":"\ud83d\udcb0 Cost Estimates","text":""},{"location":"GITHUB_SECRETS/#cicd-cost-0month","title":"CI/CD Cost: $0/month","text":"<p>GitHub CI/CD is completely free! No API keys or secrets required.</p>"},{"location":"GITHUB_SECRETS/#local-integration-testing-cost","title":"Local Integration Testing Cost","text":"<p>If you run integration tests locally:</p> Service Usage Cost Gemini API 10-20 requests/test run Free (within free tier) Anthropic Claude 5-10 requests/test run ~$0.10-0.50/run OpenAI GPT-4o 5-10 requests/test run ~$0.05-0.30/run Discord API Unlimited Free <p>Typical cost: $0-1/month for occasional local testing.</p>"},{"location":"GITHUB_SECRETS/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"GITHUB_SECRETS/#ci-is-failing","title":"CI is failing","text":"<p>Cause: Usually linting, type checking, or unit test failures.</p> <p>Solution: 1. Run locally: <code>pytest tests/ -m \"not integration\"</code> 2. Check pre-commit hooks: <code>pre-commit run --all-files</code> 3. Review GitHub Actions logs for specific error</p>"},{"location":"GITHUB_SECRETS/#local-integration-tests-failing-with-auth-errors","title":"Local integration tests failing with auth errors","text":"<p>Cause: <code>DISCORD_TOKEN</code> or <code>GEMINI_API_KEY</code> invalid or missing in <code>.env</code> file.</p> <p>Solution: 1. Verify <code>.env</code> file exists and has required keys 2. Regenerate tokens if expired 3. Check for typos in environment variable names (case-sensitive!)</p>"},{"location":"GITHUB_SECRETS/#api-rate-limit-errors-local-testing","title":"API rate limit errors (local testing)","text":"<p>Cause: Too many local test runs hitting API limits.</p> <p>Solution: 1. Wait for quotas to reset (Gemini: 60 seconds for per-minute limits) 2. Increase API quotas (Gemini: upgrade from free tier) 3. Run fewer tests: <code>pytest tests/integration/test_e2e.py::test_simple_question -v</code></p>"},{"location":"GITHUB_SECRETS/#secret-not-found-errors","title":"\"Secret not found\" errors","text":"<p>Cause: Secret name mismatch or not set at repository level.</p> <p>Solution: - Secret names are case-sensitive - Must be set at repository level (not environment) - Use exact names from this guide</p>"},{"location":"GITHUB_SECRETS/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>GitHub Encrypted Secrets Docs</li> <li>Discord Developer Portal</li> <li>Google AI Studio</li> <li>Anthropic Console</li> <li>OpenAI Platform</li> </ul>"},{"location":"GITHUB_SECRETS/#last-updated","title":"\ud83d\udd04 Last Updated","text":"<p>Date: 2026-02-06 CI/CD Version: v1.0.0 Zetherion AI Version: Phases 1-4 complete</p>"},{"location":"GITHUB_SECRETS/#questions","title":"Questions?","text":"<p>If you have issues with secrets configuration: 1. Check Troubleshooting section above 2. Review CI/CD Documentation 3. Open an issue on GitHub</p>"},{"location":"HARDWARE-RECOMMENDATIONS/","title":"Hardware Recommendations","text":"<p>This guide helps you choose the optimal hardware configuration for running Zetherion AI, including recommendations for Ollama model selection based on your system resources.</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Decision Guide</li> <li>Minimum Requirements</li> <li>Recommended Specifications</li> <li>Ollama Model Recommendations</li> <li>GPU Acceleration</li> <li>Docker Memory Configuration</li> <li>Performance Benchmarks</li> <li>Cost Analysis</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#quick-decision-guide","title":"Quick Decision Guide","text":"<p>Choose your path:</p> <ol> <li>I want minimal setup and don't need local AI</li> <li>Use Gemini backend (cloud-based)</li> <li>Requires: 8GB RAM, any modern CPU</li> <li>Setup time: ~3 minutes</li> <li> <p>Best for: Quick deployments, cloud-based workflows</p> </li> <li> <p>I want privacy with modest hardware (8-16GB RAM)</p> </li> <li>Use Ollama with <code>llama3.1:8b</code> or <code>mistral:7b</code></li> <li>Setup time: ~9 minutes (includes model download)</li> <li> <p>Best for: Privacy-conscious users, moderate performance</p> </li> <li> <p>I have powerful hardware (16GB+ RAM or GPU)</p> </li> <li>Use Ollama with <code>qwen2.5:14b</code> or larger models</li> <li>Setup time: ~12 minutes (larger downloads)</li> <li>Best for: Best quality local AI, offline capability</li> </ol> <p>The <code>start.sh</code>/<code>start.ps1</code> script automatically detects your hardware and recommends the optimal configuration.</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#minimum-requirements","title":"Minimum Requirements","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#for-gemini-backend-cloud-routing","title":"For Gemini Backend (Cloud Routing)","text":"Component Minimum Specification OS Windows 10/11, macOS 10.15+, Ubuntu 20.04+, or compatible Linux CPU Any modern x86_64 or ARM64 CPU (2+ cores) RAM 4GB system RAM Docker Docker Desktop 4.0+ with 2GB RAM allocated Disk 10GB free space Network Internet connection (required for API calls) <p>Note: Gemini backend uses cloud services, so local resources are minimal.</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#for-ollama-backend-local-routing","title":"For Ollama Backend (Local Routing)","text":"Component Minimum Specification OS Windows 10/11, macOS 10.15+, Ubuntu 20.04+, or compatible Linux CPU Modern x86_64 or ARM64 CPU (4+ cores recommended) RAM 8GB system RAM Docker Docker Desktop 4.0+ with 6GB RAM allocated Disk 20GB free space (10GB for Docker images, 5-10GB for models) Network Internet connection (for initial model download)"},{"location":"HARDWARE-RECOMMENDATIONS/#recommended-specifications","title":"Recommended Specifications","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#for-best-experience-ollama-quality-models","title":"For Best Experience (Ollama + Quality Models)","text":"Component Recommended Specification OS Windows 11, macOS 12+, Ubuntu 22.04+ CPU Modern multi-core CPU (8+ cores, 3.0+ GHz) RAM 16GB system RAM (32GB for larger models) GPU NVIDIA RTX 3060 (8GB VRAM) or better, or Apple M1/M2/M3 Docker Docker Desktop 4.25+ with 12GB RAM allocated Disk 30GB+ free SSD space Network Broadband internet (for initial setup)"},{"location":"HARDWARE-RECOMMENDATIONS/#what-the-extra-resources-buy-you","title":"What The Extra Resources Buy You","text":"<ul> <li>16GB+ RAM: Can run larger, more capable models (<code>qwen2.5:14b</code>, <code>llama3.1:70b</code> variants)</li> <li>GPU: 5-10x faster inference, enabling real-time responses</li> <li>SSD: Faster model loading and Docker operations</li> <li>More CPU Cores: Better concurrent request handling</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#ollama-model-recommendations","title":"Ollama Model Recommendations","text":"<p>The startup script (<code>start.sh</code>/<code>start.ps1</code>) automatically detects your hardware and recommends the optimal model. Here's the logic:</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#model-tiers-by-hardware","title":"Model Tiers by Hardware","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#tier-1-lightweight-4-8gb-system-ram","title":"Tier 1: Lightweight (4-8GB System RAM)","text":"<p>Recommended Model: <code>phi3:mini</code></p> Attribute Value Model Size ~2.7GB download Docker Memory 5GB System RAM 8GB minimum GPU Optional (runs well on CPU) Inference Speed ~50 tokens/sec (CPU), ~200 tokens/sec (GPU) Quality Good for simple queries, basic conversation Best For Low-resource systems, fast responses <p>Example Hardware: Budget laptops, older desktops, single-board computers</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#tier-2-balanced-8-16gb-system-ram","title":"Tier 2: Balanced (8-16GB System RAM)","text":"<p>Recommended Model: <code>llama3.1:8b</code> (default)</p> Attribute Value Model Size ~4.7GB download Docker Memory 8GB System RAM 12GB minimum (16GB recommended) GPU Optional (6GB+ VRAM for GPU acceleration) Inference Speed ~30 tokens/sec (CPU), ~150 tokens/sec (GPU) Quality Excellent balance of quality and speed Best For General use, most users <p>Example Hardware: Modern laptops, mid-range desktops, NVIDIA GTX 1660+</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#tier-3-high-quality-16gb-system-ram","title":"Tier 3: High Quality (16GB+ System RAM)","text":"<p>Recommended Model: <code>qwen2.5:14b</code></p> Attribute Value Model Size ~9.0GB download Docker Memory 12GB System RAM 16GB minimum (24GB recommended) GPU Highly recommended (8GB+ VRAM) Inference Speed ~20 tokens/sec (CPU), ~100 tokens/sec (GPU) Quality Best quality local AI, comparable to cloud models Best For Power users, quality-focused workflows <p>Example Hardware: High-end laptops, gaming PCs, workstations, NVIDIA RTX 3060+</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#tier-4-maximum-quality-32gb-system-ram-gpu","title":"Tier 4: Maximum Quality (32GB+ System RAM + GPU)","text":"<p>Recommended Model: <code>qwen2.5:32b</code> or <code>llama3.1:70b</code> variants</p> Attribute Value Model Size 18-40GB download Docker Memory 20-32GB System RAM 32GB minimum (64GB recommended) GPU Required (16GB+ VRAM) Inference Speed ~10-15 tokens/sec (GPU) Quality Highest quality local AI available Best For Professionals, research, offline deployments <p>Example Hardware: NVIDIA RTX 3090/4090, Apple M2 Ultra, workstation PCs</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#alternative-models-by-use-case","title":"Alternative Models by Use Case","text":"Use Case Model Why Fastest Response <code>mistral:7b</code> Optimized for speed, lower quality Best Quality per GB <code>qwen2.5:7b</code> Excellent performance for size Code Generation <code>codellama:13b</code> Specialized for programming tasks Privacy + Quality <code>qwen2.5:14b</code> Best local model under 10GB Offline Capability <code>llama3.1:8b</code> Robust, well-tested, reliable"},{"location":"HARDWARE-RECOMMENDATIONS/#gpu-acceleration","title":"GPU Acceleration","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#supported-gpus","title":"Supported GPUs","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#nvidia-cuda","title":"NVIDIA (CUDA)","text":"<ul> <li>Excellent Support: RTX 3060, 3070, 3080, 3090, 4070, 4080, 4090</li> <li>Good Support: GTX 1660, 1070, 1080, RTX 2060, 2070, 2080</li> <li>Minimum: 6GB VRAM for <code>llama3.1:8b</code>, 8GB+ for larger models</li> <li>Drivers: CUDA 11.8+ recommended</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#amd-rocm","title":"AMD (ROCm)","text":"<ul> <li>Excellent Support: RX 6800, 6900 XT, 7900 XT, 7900 XTX</li> <li>Good Support: RX 5700 XT, Vega 64</li> <li>Minimum: 8GB VRAM</li> <li>Drivers: ROCm 5.5+ on Linux only (Windows not supported)</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#apple-silicon-metal","title":"Apple Silicon (Metal)","text":"<ul> <li>Excellent Support: M1 Max, M2 Pro/Max/Ultra, M3 Pro/Max/Ultra</li> <li>Good Support: M1 Pro, M2</li> <li>Minimum: 16GB unified memory for best experience</li> <li>Drivers: Built into macOS (no additional install)</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#gpu-vs-cpu-performance","title":"GPU vs CPU Performance","text":"<p>Example: <code>llama3.1:8b</code> on typical hardware</p> Configuration Tokens/Second Response Time (100 tokens) CPU (Intel i7-12700) ~30 tok/s ~3.3 seconds GPU (NVIDIA RTX 3060) ~150 tok/s ~0.7 seconds GPU (NVIDIA RTX 4090) ~300 tok/s ~0.3 seconds Apple (M2 Max) ~120 tok/s ~0.8 seconds <p>Speedup: GPU provides 5-10x faster inference compared to CPU-only.</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#enabling-gpu-support","title":"Enabling GPU Support","text":"<p>GPU support is automatic if you have compatible hardware:</p> <ol> <li>NVIDIA: Install NVIDIA Container Toolkit</li> <li>AMD: Install ROCm (Linux only)</li> <li>Apple Silicon: Built-in, no additional setup</li> </ol> <p>Ollama Docker container will automatically detect and use available GPU.</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#docker-memory-configuration","title":"Docker Memory Configuration","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#why-docker-memory-matters","title":"Why Docker Memory Matters","text":"<p>Ollama models run inside Docker containers with memory limits. If the limit is too low, the model will crash or fail to load.</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#automatic-memory-management","title":"Automatic Memory Management","text":"<p>The startup scripts automatically manage Docker memory:</p> <pre><code># Script detects your model choice and sets Docker memory\n# For llama3.1:8b \u2192 Sets Docker to 8GB\n# For qwen2.5:14b \u2192 Sets Docker to 12GB\n</code></pre>"},{"location":"HARDWARE-RECOMMENDATIONS/#manual-configuration-if-needed","title":"Manual Configuration (if needed)","text":"<p>Docker Desktop (GUI): 1. Open Docker Desktop 2. Settings \u2192 Resources \u2192 Memory 3. Set slider to recommended amount:    - <code>llama3.1:8b</code>: 8GB    - <code>qwen2.5:14b</code>: 12GB    - <code>qwen2.5:32b</code>: 24GB 4. Click \"Apply &amp; Restart\"</p> <p>Docker Desktop (Command Line): <pre><code># macOS\nosascript -e 'tell application \"Docker Desktop\" to quit'\ndefaults write ~/Library/Group\\ Containers/group.com.docker/settings.json memoryMiB 8192\nopen -a Docker\n\n# Linux\n# Edit /etc/docker/daemon.json\n{\n  \"default-runtime\": \"nvidia\",\n  \"memory\": \"8g\"\n}\nsudo systemctl restart docker\n</code></pre></p>"},{"location":"HARDWARE-RECOMMENDATIONS/#memory-requirements-by-model","title":"Memory Requirements by Model","text":"Model Docker Memory System RAM Notes <code>phi3:mini</code> 5GB 8GB Minimal setup <code>mistral:7b</code> 7GB 10GB Fast inference <code>llama3.1:8b</code> 8GB 12GB Recommended default <code>qwen2.5:7b</code> 10GB 14GB Best quality &lt;10GB <code>qwen2.5:14b</code> 12GB 16GB High quality <code>qwen2.5:32b</code> 24GB 32GB Maximum quality <code>llama3.1:70b</code> 48GB 64GB Research/professional <p>Formula: Docker Memory = Model Size + 2-3GB overhead</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#real-world-response-times","title":"Real-World Response Times","text":"<p>Testing setup: - Query: \"Explain quantum computing in simple terms (100-150 words)\" - Location: Response generation only (excludes network/Discord latency)</p> Configuration Backend Model Response Time Quality (1-10) Budget Laptop (8GB RAM) Gemini gemini-flash 0.8s 7/10 Budget Laptop (8GB RAM) Ollama phi3:mini 4.2s 6/10 Mid-Range PC (16GB RAM) Ollama llama3.1:8b 3.3s 8/10 Gaming PC (16GB, RTX 3060) Ollama llama3.1:8b 0.7s 8/10 Gaming PC (16GB, RTX 3060) Ollama qwen2.5:14b 1.2s 9/10 Workstation (32GB, RTX 4090) Ollama qwen2.5:32b 0.6s 10/10 M2 Max MacBook (32GB) Ollama qwen2.5:14b 0.9s 9/10 <p>Key Takeaways: - Gemini (cloud): Fastest and free, good quality, requires internet - GPU acceleration: Makes Ollama competitive with cloud speeds - Larger models: Better quality but slower (unless you have powerful GPU) - CPU-only Ollama: Slower but provides privacy and offline capability</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#memory-usage-comparison","title":"Memory Usage Comparison","text":"Configuration Memory Usage Idle During Inference Gemini backend only Docker: 2GB, System: 4GB 1GB 2GB Ollama <code>llama3.1:8b</code> (CPU) Docker: 8GB, System: 10GB 5GB 7.5GB Ollama <code>llama3.1:8b</code> (GPU) Docker: 6GB, System: 8GB, VRAM: 6GB 3GB, VRAM: 5GB 5GB, VRAM: 6GB Ollama <code>qwen2.5:14b</code> (GPU) Docker: 12GB, System: 14GB, VRAM: 10GB 5GB, VRAM: 8GB 10GB, VRAM: 10GB"},{"location":"HARDWARE-RECOMMENDATIONS/#cost-analysis","title":"Cost Analysis","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#total-cost-of-ownership-1-year","title":"Total Cost of Ownership (1 year)","text":"Configuration Hardware Cost Cloud API Cost Electricity Total Gemini Only $0 (existing PC) $0 (free tier) $5/year $5/year Gemini + Claude $0 (existing PC) $50-200/year $5/year $55-205/year Ollama (existing PC) $0 $0 $20/year $20/year Ollama (new GPU) $300-500 (RTX 3060) $0 $50/year $350-550 first year, $50/year after <p>Cloud API Costs (moderate usage: 100 requests/day): - Gemini Free Tier: $0 (covers 1,500 requests/day) - Claude Sonnet: ~$50-100/year (complex queries only) - OpenAI GPT-4: ~$100-200/year (alternative to Claude)</p> <p>Ollama Electricity (8 hours/day usage): - CPU-only: ~50W \u2192 $15/year - GPU (RTX 3060): ~200W \u2192 $50/year - (Assumes $0.12/kWh electricity rate)</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#break-even-analysis","title":"Break-Even Analysis","text":"<p>If you're considering buying a GPU for Ollama:</p> <ul> <li>RTX 3060 ($350): Breaks even vs cloud APIs in ~2-3 years</li> <li>RTX 4060 ($300): Breaks even in ~2 years</li> <li>Used RTX 3060 ($200): Breaks even in ~1 year</li> </ul> <p>Best Value: 1. Free tier: Gemini only (excellent for light use) 2. Budget: Ollama on existing hardware (privacy + no API costs) 3. Quality: Gemini + Claude (best results, cloud-based) 4. Long-term: Ollama + GPU upgrade (upfront cost, zero API fees)</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#hardware-upgrade-path","title":"Hardware Upgrade Path","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#starting-point-future-upgrades","title":"Starting Point \u2192 Future Upgrades","text":"<p>Path 1: Budget to Mid-Range 1. Start: 8GB RAM, Gemini backend (free) 2. Upgrade: Add 8GB RAM \u2192 16GB total 3. Result: Can run <code>llama3.1:8b</code> smoothly on CPU</p> <p>Path 2: Mid-Range to High-End 1. Start: 16GB RAM, <code>llama3.1:8b</code> on CPU 2. Upgrade: Add GPU (RTX 3060 or better) 3. Result: 5-10x faster inference, can run <code>qwen2.5:14b</code></p> <p>Path 3: High-End to Workstation 1. Start: 16GB RAM, RTX 3060, <code>qwen2.5:14b</code> 2. Upgrade: RAM to 32GB, GPU to RTX 4080/4090 3. Result: Can run <code>qwen2.5:32b</code> or larger models</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#recommendations-by-user-type","title":"Recommendations by User Type","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#hobbyist-personal-use","title":"Hobbyist / Personal Use","text":"<ul> <li>Backend: Gemini (cloud)</li> <li>Hardware: Any modern computer (8GB+ RAM)</li> <li>Why: Zero cost, minimal setup, good quality</li> <li>Upgrade: Add RAM if you want to try local models</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#privacy-conscious-user","title":"Privacy-Conscious User","text":"<ul> <li>Backend: Ollama with <code>llama3.1:8b</code></li> <li>Hardware: 16GB RAM, modern CPU</li> <li>Why: No data sent to cloud for routing, offline capable</li> <li>Upgrade: Add GPU for faster responses</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#power-user","title":"Power User","text":"<ul> <li>Backend: Ollama with <code>qwen2.5:14b</code></li> <li>Hardware: 16GB+ RAM, RTX 3060 or better</li> <li>Why: Best quality local AI, fast responses</li> <li>Upgrade: More RAM/better GPU for larger models</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#professional-business","title":"Professional / Business","text":"<ul> <li>Backend: Hybrid (Gemini for routing, Claude for complex tasks)</li> <li>Hardware: 16GB+ RAM, good internet connection</li> <li>Why: Best quality, reliable, cloud-based</li> <li>Upgrade: Dedicated API budget for higher usage</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#developer-researcher","title":"Developer / Researcher","text":"<ul> <li>Backend: Ollama with large models (<code>qwen2.5:32b+</code>)</li> <li>Hardware: 32GB+ RAM, RTX 4090 or workstation GPU</li> <li>Why: Full control, offline, maximum quality</li> <li>Upgrade: More VRAM for even larger models</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"HARDWARE-RECOMMENDATIONS/#model-wont-load-or-oom-out-of-memory","title":"\"Model won't load\" or \"OOM (Out of Memory)\"","text":"<p>Symptoms: Container crashes, \"Out of memory\" errors in logs</p> <p>Solutions: 1. Increase Docker memory: See Docker Memory Configuration 2. Choose smaller model: Use <code>llama3.1:8b</code> instead of <code>qwen2.5:14b</code> 3. Close other applications: Free up system RAM 4. Upgrade RAM: Add more system memory</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#inference-is-slow","title":"\"Inference is slow\"","text":"<p>Symptoms: Responses take 5+ seconds, bot feels sluggish</p> <p>Solutions: 1. Enable GPU: If you have compatible GPU, install drivers 2. Choose faster model: Use <code>mistral:7b</code> instead of larger models 3. Switch to Gemini: Cloud backend is faster for most hardware 4. Upgrade hardware: Add GPU or faster CPU</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#docker-desktop-wont-allocate-enough-memory","title":"\"Docker Desktop won't allocate enough memory\"","text":"<p>Symptoms: Can't set Docker memory above 8GB on macOS</p> <p>Solution (macOS): <pre><code># Edit Docker's VM settings\nnano ~/Library/Group\\ Containers/group.com.docker/settings.json\n\n# Set memoryMiB to desired amount (in MB)\n{\n  \"memoryMiB\": 12288  # 12GB\n}\n\n# Restart Docker Desktop\n</code></pre></p> <p>Solution (Windows): 1. Open Docker Desktop 2. Settings \u2192 Resources \u2192 WSL Integration 3. Ensure \"Enable integration with default WSL distro\" is checked 4. In WSL terminal: <code>wsl --shutdown</code>, then restart Docker</p>"},{"location":"HARDWARE-RECOMMENDATIONS/#additional-resources","title":"Additional Resources","text":"<ul> <li>Ollama Model Library: https://ollama.ai/library</li> <li>Docker Desktop Documentation: https://docs.docker.com/desktop/</li> <li>NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/</li> <li>ROCm Documentation: https://rocm.docs.amd.com/</li> <li>Zetherion AI GitHub: https://github.com/jimtin/zetherion-ai</li> </ul>"},{"location":"HARDWARE-RECOMMENDATIONS/#community-benchmarks","title":"Community Benchmarks","text":"<p>Have you run Zetherion AI on different hardware? Share your benchmarks in GitHub Discussions to help other users!</p> <p>What to share: - Hardware specs (CPU, RAM, GPU) - Model used - Average response time - Quality assessment (1-10)</p>"},{"location":"INSTALLATION/","title":"Installation Guide","text":"<p>Complete step-by-step installation guide for Zetherion AI on Windows, macOS, and Linux.</p>"},{"location":"INSTALLATION/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Prerequisites</li> <li>Platform-Specific Installation</li> <li>Windows Installation</li> <li>macOS Installation</li> <li>Linux Installation</li> <li>Getting API Keys</li> <li>First-Time Setup</li> <li>Verification</li> <li>Troubleshooting</li> <li>Next Steps</li> </ul>"},{"location":"INSTALLATION/#overview","title":"Overview","text":"<p>Zetherion AI is 100% containerized using Docker. No local Python installation is required. The installation process is automated through platform-specific startup scripts:</p> <ul> <li>Windows: <code>start.ps1</code> (PowerShell)</li> <li>macOS/Linux: <code>start.sh</code> (Bash)</li> </ul> <p>Estimated Time: - First run: 3-9 minutes (depending on backend choice) - Subsequent runs: ~30 seconds (containers cached)</p>"},{"location":"INSTALLATION/#prerequisites","title":"Prerequisites","text":""},{"location":"INSTALLATION/#required","title":"Required","text":"<ol> <li>Operating System</li> <li>Windows 10/11 (64-bit)</li> <li>macOS 10.15+ (Catalina or later)</li> <li> <p>Linux: Ubuntu 20.04+, Debian 11+, Fedora 35+, or compatible</p> </li> <li> <p>Hardware</p> </li> <li>Minimum: 8GB RAM, 20GB free disk space</li> <li>Recommended: 16GB RAM, 30GB free SSD space</li> <li> <p>See Hardware Recommendations for detailed specs</p> </li> <li> <p>Network</p> </li> <li>Internet connection (for initial setup and API calls)</li> <li>Broadband recommended for Ollama model downloads</li> </ol>"},{"location":"INSTALLATION/#automatically-installed-if-missing","title":"Automatically Installed (if missing)","text":"<p>The startup script will offer to install these if not present:</p> <ol> <li>Docker Desktop (required)</li> <li>Windows/Mac: Installed via package manager</li> <li> <p>Linux: Manual installation required</p> </li> <li> <p>Git (recommended)</p> </li> <li>Used for cloning repository</li> <li>Optional but recommended</li> </ol>"},{"location":"INSTALLATION/#platform-specific-installation","title":"Platform-Specific Installation","text":""},{"location":"INSTALLATION/#windows-installation","title":"Windows Installation","text":""},{"location":"INSTALLATION/#step-1-open-powershell-as-administrator","title":"Step 1: Open PowerShell as Administrator","text":"<p>Why Administrator? Required to install Docker Desktop and Git if missing.</p> <ol> <li>Press <code>Win + X</code></li> <li>Select \"Windows PowerShell (Admin)\" or \"Terminal (Admin)\"</li> <li>Click \"Yes\" on the UAC prompt</li> </ol>"},{"location":"INSTALLATION/#step-2-clone-repository","title":"Step 2: Clone Repository","text":"<pre><code># Navigate to desired location\ncd $HOME\\Documents\n\n# Clone repository\ngit clone https://github.com/jimtin/zetherion-ai.git\ncd zetherion-ai\n</code></pre> <p>Don't have Git? Download ZIP from GitHub: 1. Go to https://github.com/jimtin/zetherion-ai 2. Click \"Code\" \u2192 \"Download ZIP\" 3. Extract to desired location 4. Open PowerShell in that folder</p>"},{"location":"INSTALLATION/#step-3-run-startup-script","title":"Step 3: Run Startup Script","text":"<pre><code># Run the automated deployment script\n.\\start.ps1\n</code></pre> <p>What happens next: - Script checks for Docker Desktop (offers to install if missing) - Script checks for Git (offers to install if missing) - Launches Docker Desktop if not running - Guides you through interactive configuration - Builds and starts all containers</p>"},{"location":"INSTALLATION/#step-4-follow-interactive-prompts","title":"Step 4: Follow Interactive Prompts","text":"<p>The script will ask for: 1. Discord Bot Token (required) 2. Gemini API Key (required) 3. Router Backend (Gemini or Ollama) 4. Ollama Model (if Ollama selected, shows hardware recommendation)</p> <p>First run timing: - Gemini backend: ~3 minutes - Ollama backend: ~9 minutes (includes model download)</p>"},{"location":"INSTALLATION/#macos-installation","title":"macOS Installation","text":""},{"location":"INSTALLATION/#step-1-open-terminal","title":"Step 1: Open Terminal","text":"<ol> <li>Press <code>Cmd + Space</code></li> <li>Type \"Terminal\"</li> <li>Press <code>Enter</code></li> </ol>"},{"location":"INSTALLATION/#step-2-clone-repository_1","title":"Step 2: Clone Repository","text":"<pre><code># Navigate to desired location\ncd ~/Documents\n\n# Clone repository\ngit clone https://github.com/jimtin/zetherion-ai.git\ncd zetherion-ai\n</code></pre> <p>Don't have Git? Homebrew will prompt to install Command Line Tools.</p>"},{"location":"INSTALLATION/#step-3-run-startup-script_1","title":"Step 3: Run Startup Script","text":"<pre><code># Make script executable\nchmod +x start.sh\n\n# Run the automated deployment script\n./start.sh\n</code></pre> <p>What happens next: - Script checks for Docker Desktop (offers to install via Homebrew) - Script checks for Git (offers to install via Homebrew) - Launches Docker Desktop if not running - Guides you through interactive configuration - Builds and starts all containers</p>"},{"location":"INSTALLATION/#step-4-grant-permissions","title":"Step 4: Grant Permissions","text":"<p>Docker Desktop may request permissions: - File Access: Allow (needed for volumes) - Network: Allow (needed for containers)</p> <p>Click \"OK\" on all prompts.</p>"},{"location":"INSTALLATION/#linux-installation","title":"Linux Installation","text":""},{"location":"INSTALLATION/#step-1-install-docker-if-needed","title":"Step 1: Install Docker (if needed)","text":"<p>Ubuntu/Debian: <pre><code># Update package index\nsudo apt-get update\n\n# Install prerequisites\nsudo apt-get install -y ca-certificates curl gnupg\n\n# Add Docker's official GPG key\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Set up repository\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker Engine\nsudo apt-get update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n\n# Add user to docker group (avoid sudo)\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p> <p>Fedora: <pre><code>sudo dnf -y install dnf-plugins-core\nsudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\nsudo dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nsudo systemctl start docker\nsudo systemctl enable docker\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p>"},{"location":"INSTALLATION/#step-2-clone-repository_2","title":"Step 2: Clone Repository","text":"<pre><code># Navigate to desired location\ncd ~/Documents\n\n# Clone repository\ngit clone https://github.com/jimtin/zetherion-ai.git\ncd zetherion-ai\n</code></pre>"},{"location":"INSTALLATION/#step-3-run-startup-script_2","title":"Step 3: Run Startup Script","text":"<pre><code># Make script executable\nchmod +x start.sh\n\n# Run the automated deployment script\n./start.sh\n</code></pre> <p>Note: On Linux, the script will not auto-install Docker. You must install it manually (see Step 1).</p>"},{"location":"INSTALLATION/#getting-api-keys","title":"Getting API Keys","text":"<p>Before running the setup, gather these API keys:</p>"},{"location":"INSTALLATION/#1-discord-bot-token-required","title":"1. Discord Bot Token (Required)","text":"<p>Steps: 1. Go to Discord Developer Portal 2. Click \"New Application\" 3. Name it (e.g., \"Zetherion AI\") 4. Go to \"Bot\" tab \u2192 Click \"Reset Token\" 5. Copy token immediately (you won't see it again) 6. Enable \"Message Content Intent\" (required)</p> <p>Invite Bot to Server: 1. Go to \"OAuth2\" \u2192 \"URL Generator\" 2. Select scopes: <code>bot</code>, <code>applications.commands</code> 3. Select permissions: <code>Send Messages</code>, <code>Embed Links</code>, <code>Attach Files</code>, <code>Read Message History</code>, <code>View Channels</code> 4. Copy generated URL and open in browser 5. Select server and authorize</p>"},{"location":"INSTALLATION/#2-gemini-api-key-required","title":"2. Gemini API Key (Required)","text":"<p>Steps: 1. Go to Google AI Studio 2. Sign in with Google account 3. Click \"Create API key\" 4. Select or create Google Cloud project 5. Copy API key (starts with <code>AIzaSy...</code>)</p> <p>Pricing: Free tier (1,500 requests/day) - sufficient for most users</p>"},{"location":"INSTALLATION/#3-anthropic-api-key-optional","title":"3. Anthropic API Key (Optional)","text":"<p>For Claude Sonnet 4.5 (complex reasoning tasks):</p> <ol> <li>Go to Anthropic Console</li> <li>Sign up or sign in</li> <li>Go to Settings \u2192 API Keys</li> <li>Click \"Create Key\"</li> <li>Copy key (starts with <code>sk-ant-...</code>)</li> <li>Add payment method and credits ($5 minimum)</li> </ol> <p>Pricing: ~$3 per million input tokens</p>"},{"location":"INSTALLATION/#4-openai-api-key-optional","title":"4. OpenAI API Key (Optional)","text":"<p>For GPT-4o (alternative to Claude):</p> <ol> <li>Go to OpenAI Platform</li> <li>Sign in or create account</li> <li>Click profile \u2192 \"View API keys\"</li> <li>Click \"Create new secret key\"</li> <li>Copy key (starts with <code>sk-...</code>)</li> <li>Add payment method and credits</li> </ol> <p>Pricing: ~$2.50 per million input tokens</p>"},{"location":"INSTALLATION/#first-time-setup","title":"First-Time Setup","text":""},{"location":"INSTALLATION/#interactive-configuration","title":"Interactive Configuration","text":"<p>When you run <code>start.ps1</code> or <code>start.sh</code> for the first time, you'll be guided through setup:</p>"},{"location":"INSTALLATION/#1-prerequisites-check","title":"1. Prerequisites Check","text":"<p>The script checks: - \u2705 Docker Desktop installed and running - \u2705 Git installed (optional) - \u2705 Sufficient disk space (20GB+)</p> <p>Auto-Install Prompts: <pre><code>Docker Desktop not found\nInstall Docker Desktop? (Y/n):\n</code></pre></p> <p>Type <code>Y</code> and press Enter to auto-install.</p>"},{"location":"INSTALLATION/#2-hardware-assessment","title":"2. Hardware Assessment","text":"<p>Script detects your system: <pre><code>System Hardware:\n  CPU: Intel Core i7-12700K (12 cores, 20 threads)\n  RAM: 32 GB total, 24 GB available\n  GPU: NVIDIA GeForce RTX 3060 (12GB)\n\nRecommended Ollama Model:\n  Model: qwen2.5:14b\n  Size: 9.0 GB download\n  Quality: High\n  Speed: Fast (with GPU)\n  Reason: Powerful GPU detected, high-quality model recommended\n</code></pre></p>"},{"location":"INSTALLATION/#3-configuration-setup","title":"3. Configuration Setup","text":"<p>Prompts for API keys:</p> <pre><code>Discord Bot Token (required):\n&gt; MTQ2ODc4MDQxODY1MTI2MzEyOQ.GGFum2.lsf_abc123...\n\nGemini API Key (required):\n&gt; AIzaSyCO9WodgUFJfW-7qK4Vtbnc...\n\nAnthropic API Key (optional, press Enter to skip):\n&gt; sk-ant-api03-OEKnlIipBFzx...\n\nOpenAI API Key (optional, press Enter to skip):\n&gt; [Enter]\n</code></pre>"},{"location":"INSTALLATION/#4-router-backend-selection","title":"4. Router Backend Selection","text":"<pre><code>Router Backend:\n  1. Gemini (cloud-based, fast, minimal resources)\n  2. Ollama (local, private, ~5GB download)\n\nChoose [1/2]: 2\n</code></pre> <p>If you choose Ollama: <pre><code>Recommended model for your hardware: qwen2.5:14b\nUse recommended model? (Y/n): Y\n</code></pre></p>"},{"location":"INSTALLATION/#5-deployment","title":"5. Deployment","text":"<p>Script automatically: 1. Builds distroless Docker images (~2 minutes) 2. Starts all containers (Qdrant, Skills, Bot, Ollama if selected) 3. Waits for health checks 4. Downloads Ollama model if selected (~5-7 minutes) 5. Verifies all services running</p>"},{"location":"INSTALLATION/#6-success","title":"6. Success","text":"<pre><code>============================================================\n  Zetherion AI is now running!\n============================================================\n\nNext Steps:\n  1. View logs:        docker-compose logs -f\n  2. Check status:     ./status.sh\n  3. Stop bot:         ./stop.sh\n\n  4. Invite bot to Discord:\n     https://discord.com/developers/applications\n\nDeployment successful!\n</code></pre>"},{"location":"INSTALLATION/#verification","title":"Verification","text":""},{"location":"INSTALLATION/#check-container-status","title":"Check Container Status","text":"<p>Windows: <pre><code>.\\status.ps1\n</code></pre></p> <p>macOS/Linux: <pre><code>./status.sh\n</code></pre></p> <p>Expected Output: <pre><code>============================================================\n  Zetherion AI Status\n============================================================\n\n[OK] Qdrant is running and healthy\n    Collections: 0\n\n[OK] Ollama is running and healthy\n    Models: 1\n      - qwen2.5:14b\n\n[OK] Skills service is running and healthy\n\n[OK] Bot is running and healthy\n    Uptime: 0d 0h 2m 15s\n\n[OK] Zetherion AI is fully operational\n\nContainer Summary:\nzetherion-ai-qdrant   Up 2 minutes (healthy)\nzetherion-ai-ollama   Up 2 minutes (healthy)\nzetherion-ai-skills   Up 2 minutes (healthy)\nzetherion-ai-bot      Up 2 minutes (healthy)\n</code></pre></p>"},{"location":"INSTALLATION/#test-discord-bot","title":"Test Discord Bot","text":"<ol> <li>Open Discord</li> <li>Go to server where bot was invited</li> <li>Type: <code>@Zetherion AI hello</code></li> <li>Bot should respond within 1-2 seconds</li> </ol> <p>If bot doesn't respond: - Check bot is online in member list - Verify \"Message Content Intent\" is enabled - Check logs: <code>docker-compose logs -f zetherion-ai-bot</code></p>"},{"location":"INSTALLATION/#check-qdrant-dashboard","title":"Check Qdrant Dashboard","text":"<p>Open browser: http://localhost:6333/dashboard</p> <p>You should see: - Qdrant UI loads - Collections tab (may be empty initially) - Cluster info shows healthy</p>"},{"location":"INSTALLATION/#check-ollama-if-enabled","title":"Check Ollama (if enabled)","text":"<p>Open browser: http://localhost:11434/api/tags</p> <p>You should see JSON with loaded models: <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"qwen2.5:14b\",\n      \"modified_at\": \"2026-02-07T10:30:00Z\",\n      \"size\": 8900000000\n    }\n  ]\n}\n</code></pre></p>"},{"location":"INSTALLATION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INSTALLATION/#docker-desktop-wont-start","title":"Docker Desktop Won't Start","text":"<p>Windows: 1. Check WSL 2 is installed: <code>wsl --status</code> 2. Update WSL: <code>wsl --update</code> 3. Restart computer 4. Try starting Docker Desktop manually</p> <p>macOS: 1. Check System Preferences \u2192 Security &amp; Privacy 2. Allow Docker Desktop if blocked 3. Restart Docker Desktop from menu bar</p> <p>Linux: 1. Check Docker service: <code>sudo systemctl status docker</code> 2. Start Docker: <code>sudo systemctl start docker</code> 3. Enable auto-start: <code>sudo systemctl enable docker</code></p>"},{"location":"INSTALLATION/#script-says-docker-not-found-after-installing","title":"Script Says \"Docker Not Found\" After Installing","text":"<p>Issue: PATH not refreshed after installation</p> <p>Solution: 1. Close and reopen terminal/PowerShell 2. Run script again 3. If still not working, restart computer</p>"},{"location":"INSTALLATION/#permission-denied-errors-linux","title":"\"Permission Denied\" Errors (Linux)","text":"<p>Issue: User not in docker group</p> <p>Solution: <pre><code># Add user to docker group\nsudo usermod -aG docker $USER\n\n# Activate group membership\nnewgrp docker\n\n# Or logout and login again\n</code></pre></p>"},{"location":"INSTALLATION/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Issue: Docker memory allocation too low</p> <p>Solution: 1. Open Docker Desktop 2. Settings \u2192 Resources \u2192 Memory 3. Increase to recommended amount:    - Gemini: 4GB minimum    - Ollama (llama3.1:8b): 8GB    - Ollama (qwen2.5:14b): 12GB 4. Click \"Apply &amp; Restart\"</p>"},{"location":"INSTALLATION/#model-download-fails","title":"Model Download Fails","text":"<p>Issue: Network timeout or disk space</p> <p>Solutions: 1. Check internet connection 2. Verify disk space: <code>df -h</code> (Linux/Mac) or <code>Get-PSDrive</code> (Windows) 3. Retry download:    <pre><code>docker exec zetherion-ai-ollama ollama pull llama3.1:8b\n</code></pre></p>"},{"location":"INSTALLATION/#port-already-in-use","title":"\"Port Already in Use\"","text":"<p>Issue: Another service using ports 6333, 8080, or 11434</p> <p>Solutions: 1. Find process using port:    <pre><code># Linux/Mac\nlsof -i :6333\n\n# Windows\nnetstat -ano | findstr :6333\n</code></pre> 2. Stop conflicting service or change Zetherion AI ports in <code>docker-compose.yml</code></p>"},{"location":"INSTALLATION/#interactive-setup-fails","title":"Interactive Setup Fails","text":"<p>Issue: Python error during setup</p> <p>Solution: <pre><code># Manually create .env from template\ncp .env.example .env\n\n# Edit with your favorite editor\nnano .env  # or vim, code, notepad, etc.\n\n# Fill in at minimum:\n# DISCORD_TOKEN=your_token_here\n# GEMINI_API_KEY=your_key_here\n# ROUTER_BACKEND=gemini\n\n# Save and run start script again\n./start.sh  # or start.ps1 on Windows\n</code></pre></p>"},{"location":"INSTALLATION/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p>"},{"location":"INSTALLATION/#1-explore-commands","title":"1. Explore Commands","text":"<p>Try these Discord commands: <pre><code>@Zetherion AI what can you do?\n@Zetherion AI remember I prefer Python for coding\n@Zetherion AI search for what I told you about coding\n</code></pre></p> <p>See Command Reference for full list.</p>"},{"location":"INSTALLATION/#2-configure-advanced-features","title":"2. Configure Advanced Features","text":"<p>Edit <code>.env</code> to customize: - Rate limiting - User allowlist - Logging level - Model selection</p> <p>See Configuration Guide for details.</p>"},{"location":"INSTALLATION/#3-set-up-encryption-optional","title":"3. Set Up Encryption (Optional)","text":"<p>Enable AES-256-GCM encryption for vector storage:</p> <pre><code># Generate encryption passphrase\nopenssl rand -base64 32\n\n# Add to .env\nENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=\"your-generated-passphrase\"\n\n# Restart bot\n./stop.sh &amp;&amp; ./start.sh\n</code></pre>"},{"location":"INSTALLATION/#4-monitor-performance","title":"4. Monitor Performance","text":"<p>Check system resources: <pre><code># View container resource usage\ndocker stats\n\n# View bot logs\ndocker-compose logs -f zetherion-ai-bot\n\n# Check Qdrant metrics\ncurl http://localhost:6333/metrics\n</code></pre></p>"},{"location":"INSTALLATION/#5-set-up-backups","title":"5. Set Up Backups","text":"<p>Backup important data:</p> <pre><code># Backup Qdrant data\ndocker run --rm -v zetherion-ai_qdrant_storage:/data \\\n  -v $(pwd)/backups:/backup alpine \\\n  tar czf /backup/qdrant-backup-$(date +%Y%m%d).tar.gz /data\n\n# Backup .env (careful - contains secrets!)\ncp .env .env.backup\nchmod 600 .env.backup\n</code></pre>"},{"location":"INSTALLATION/#6-update-regularly","title":"6. Update Regularly","text":"<p>Keep Zetherion AI up to date:</p> <pre><code># Pull latest code\ngit pull origin main\n\n# Rebuild containers\n./stop.sh\n./start.sh --force-rebuild\n</code></pre>"},{"location":"INSTALLATION/#additional-resources","title":"Additional Resources","text":"<ul> <li>Hardware Recommendations - Optimize for your system</li> <li>Configuration Guide - Customize settings</li> <li>Security Guide - Distroless containers and encryption</li> <li>Troubleshooting - Common issues and solutions</li> <li>GitHub Discussions - Community help</li> </ul>"},{"location":"INSTALLATION/#getting-help","title":"Getting Help","text":"<p>Before asking for help: 1. Check Troubleshooting Guide 2. Review logs: <code>docker-compose logs</code> 3. Check GitHub Issues</p> <p>Where to get help: - GitHub Issues: Bug reports and feature requests - GitHub Discussions: General questions and community support - Discord Server: (link if available)</p> <p>When reporting issues, include: - Operating system and version - Docker Desktop version - Output of <code>./status.sh</code> or <code>status.ps1</code> - Relevant error messages from logs - Steps to reproduce</p> <p>Congratulations! You've successfully installed Zetherion AI. Enjoy your new AI assistant! \ud83c\udf89</p>"},{"location":"PROFILES/","title":"Profile System Guide","text":"<p>The Profile System enables Zetherion AI to learn and remember user preferences, adapting responses to individual needs while respecting privacy.</p>"},{"location":"PROFILES/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Configuration</li> <li>Profile Categories</li> <li>Automatic Learning</li> <li>Manual Profile Management</li> <li>Confidence and Confirmation</li> <li>Response Adaptation</li> <li>Privacy and Data Control</li> <li>Troubleshooting</li> </ul>"},{"location":"PROFILES/#overview","title":"Overview","text":"<p>The Profile System:</p> <ul> <li>Learns automatically from conversations</li> <li>Tracks confidence in learned information</li> <li>Asks for confirmation when uncertain</li> <li>Adapts responses to preferences</li> <li>Respects privacy with full user control</li> </ul>"},{"location":"PROFILES/#how-it-works","title":"How It Works","text":"<pre><code>Conversation \u2192 Profile Inference \u2192 Confidence Scoring \u2192 Storage\n                                          \u2193\n                              Confirmation (if uncertain)\n                                          \u2193\n                              Response Adaptation\n</code></pre>"},{"location":"PROFILES/#configuration","title":"Configuration","text":""},{"location":"PROFILES/#enable-profile-system","title":"Enable Profile System","text":"<pre><code># Enable profile inference\nPROFILE_INFERENCE_ENABLED=true\n\n# Confidence threshold for auto-apply (0.0-1.0)\nPROFILE_CONFIDENCE_THRESHOLD=0.6\n\n# Cache TTL in seconds\nPROFILE_CACHE_TTL=300\n\n# Database location\nPROFILE_DB_PATH=data/profiles.db\n</code></pre>"},{"location":"PROFILES/#confirmation-settings","title":"Confirmation Settings","text":"<pre><code># Maximum pending confirmations per user\nPROFILE_MAX_PENDING_CONFIRMATIONS=5\n\n# Hours before confirmation expires\nPROFILE_CONFIRMATION_EXPIRY_HOURS=24\n</code></pre>"},{"location":"PROFILES/#response-defaults","title":"Response Defaults","text":"<pre><code># Default communication style (0.0-1.0)\nDEFAULT_FORMALITY=0.5    # 0=casual, 1=formal\nDEFAULT_VERBOSITY=0.5    # 0=brief, 1=detailed\nDEFAULT_PROACTIVITY=0.5  # 0=reactive, 1=proactive\n\n# Trust evolution rate (how quickly trust increases)\nTRUST_EVOLUTION_RATE=0.05\n</code></pre>"},{"location":"PROFILES/#profile-categories","title":"Profile Categories","text":""},{"location":"PROFILES/#identity","title":"Identity","text":"<p>Personal identification information.</p> Field Examples Privacy Level Name \"James\", \"Dr. Smith\" Medium Nickname \"Jim\", \"Jamie\" Low Pronouns \"he/him\", \"they/them\" Medium Location \"Sydney\", \"Australia\" High Timezone \"AEDT\", \"UTC+11\" Low"},{"location":"PROFILES/#preferences","title":"Preferences","text":"<p>Communication and interaction preferences.</p> Field Examples Default Formality Casual, Professional 0.5 Verbosity Brief, Detailed 0.5 Coding Style Python, TypeScript None Response Format Markdown, Plain Markdown Explanation Depth High-level, Detailed Medium"},{"location":"PROFILES/#schedule","title":"Schedule","text":"<p>Work and availability patterns.</p> Field Examples Work Hours \"9am-5pm weekdays\" Timezone \"AEDT (UTC+11)\" Availability \"Busy mornings\" Meeting Preferences \"Prefer afternoon calls\""},{"location":"PROFILES/#projects","title":"Projects","text":"<p>Current work and interests.</p> Field Examples Current Projects \"Zetherion AI\", \"API Migration\" Technologies \"Python\", \"Docker\", \"React\" Interests \"Machine learning\", \"DevOps\" Learning Goals \"Learning Rust\", \"AWS certification\""},{"location":"PROFILES/#relationships","title":"Relationships","text":"<p>Professional and team connections.</p> Field Examples Manager \"Sarah\" Team \"Platform Engineering\" Direct Reports \"Alice, Bob\" Collaborators \"DevOps team\""},{"location":"PROFILES/#skills","title":"Skills","text":"<p>Expertise and capabilities.</p> Field Examples Languages \"Python (expert)\", \"Go (intermediate)\" Frameworks \"FastAPI\", \"React\" Domains \"Backend\", \"Infrastructure\" Certifications \"AWS Solutions Architect\""},{"location":"PROFILES/#goals","title":"Goals","text":"<p>Objectives and deadlines.</p> Field Examples Short-term \"Ship v2.0 by Friday\" Long-term \"Become team lead\" Learning \"Complete ML course\" Personal \"Better work-life balance\""},{"location":"PROFILES/#habits","title":"Habits","text":"<p>Behavioral patterns and shortcuts.</p> Field Examples Communication \"Prefers async\" Shortcuts \"Uses 'lgtm' for approval\" Patterns \"Reviews PRs in morning\""},{"location":"PROFILES/#automatic-learning","title":"Automatic Learning","text":"<p>The system learns from natural conversation.</p>"},{"location":"PROFILES/#learning-triggers","title":"Learning Triggers","text":"<pre><code>User: \"I prefer Python over JavaScript\"\n\u2192 Learns: Preferences.coding_style = \"Python\" (confidence: 0.85)\n\nUser: \"My name is James\"\n\u2192 Learns: Identity.name = \"James\" (confidence: 0.95)\n\nUser: \"I work from 9 to 5\"\n\u2192 Learns: Schedule.work_hours = \"9am-5pm\" (confidence: 0.80)\n\nUser: \"I'm working on the API migration project\"\n\u2192 Learns: Projects.current = \"API migration\" (confidence: 0.75)\n</code></pre>"},{"location":"PROFILES/#inference-tiers","title":"Inference Tiers","text":"<p>The system uses two inference tiers:</p> <p>Tier 1 (Regex-based) - Fast pattern matching - High confidence for explicit statements - Examples: \"My name is X\", \"I prefer Y\", \"I work at Z\"</p> <p>Tier 2 (LLM-based, optional) - Deeper context understanding - Lower confidence, requires more confirmation - Examples: Inferred preferences from conversation style</p> <pre><code># Use only Tier 1 (faster, less resource-intensive)\nPROFILE_TIER1_ONLY=true\n</code></pre>"},{"location":"PROFILES/#manual-profile-management","title":"Manual Profile Management","text":""},{"location":"PROFILES/#viewing-your-profile","title":"Viewing Your Profile","text":"<pre><code>@Zetherion AI show my profile\n</code></pre> <p>Output: <pre><code>\ud83d\udccb Your Profile\n\nIdentity:\n  Name: James (95%)\n  Location: Sydney, Australia (80%)\n  Timezone: AEDT (90%)\n\nPreferences:\n  Coding Style: Python (90%)\n  Verbosity: Concise (75%)\n  Formality: Casual (85%)\n\nWork:\n  Role: Software Engineer (85%)\n  Team: Platform (70%)\n  Current Project: Zetherion AI (95%)\n\n(Percentages indicate confidence levels)\n</code></pre></p>"},{"location":"PROFILES/#updating-profile","title":"Updating Profile","text":"<p>Explicit updates: <pre><code>@Zetherion AI update my name to James\n@Zetherion AI set my timezone to AEDT\n@Zetherion AI I prefer detailed explanations\n@Zetherion AI my coding language is Python\n</code></pre></p> <p>Updating specific fields: <pre><code>@Zetherion AI update my profile: role = Senior Engineer\n@Zetherion AI set preference: verbosity = detailed\n</code></pre></p>"},{"location":"PROFILES/#viewing-specific-categories","title":"Viewing Specific Categories","text":"<pre><code>@Zetherion AI show my preferences\n@Zetherion AI what do you know about my schedule?\n@Zetherion AI show my projects\n</code></pre>"},{"location":"PROFILES/#deleting-information","title":"Deleting Information","text":"<p>Single field: <pre><code>@Zetherion AI forget my location\n@Zetherion AI remove my manager from profile\n</code></pre></p> <p>Category: <pre><code>@Zetherion AI clear my relationships\n</code></pre></p> <p>Full profile (requires confirmation): <pre><code>@Zetherion AI delete my entire profile\n&gt; Are you sure? This cannot be undone. Reply 'yes' to confirm.\n</code></pre></p>"},{"location":"PROFILES/#confidence-and-confirmation","title":"Confidence and Confirmation","text":""},{"location":"PROFILES/#confidence-levels","title":"Confidence Levels","text":"Level Score Behavior High 0.9+ Auto-applied, no confirmation Medium 0.6-0.9 Applied, may ask confirmation Low &lt;0.6 Stored as pending, asks confirmation"},{"location":"PROFILES/#confidence-sources","title":"Confidence Sources","text":"Source Base Confidence Explicit statement 0.95 (\"My name is James\") Direct answer 0.85 (\"I prefer Python\") Contextual mention 0.70 (\"Working on the API...\") Inference 0.50 (Inferred from behavior)"},{"location":"PROFILES/#confirmation-flow","title":"Confirmation Flow","text":"<p>When confidence is low:</p> <pre><code>Bot: I noticed you might prefer Python for coding. Is that correct?\nUser: Yes\nBot: Great! I've added that to your profile.\n</code></pre> <p>Or:</p> <pre><code>Bot: I noticed you might prefer Python for coding. Is that correct?\nUser: No, I actually prefer Go\nBot: Thanks for clarifying! I've updated your preference to Go.\n</code></pre>"},{"location":"PROFILES/#pending-confirmations","title":"Pending Confirmations","text":"<p>View pending confirmations: <pre><code>@Zetherion AI show pending confirmations\n</code></pre></p> <p>Output: <pre><code>Pending Profile Confirmations:\n\n1. Location: \"Melbourne, Australia\" (60% confident)\n   Detected from: \"heading to Melbourne next week\"\n   \u2713 Confirm | \u2717 Reject | \ud83d\udcdd Correct\n\n2. Project: \"Data Pipeline\" (55% confident)\n   Detected from: \"working on data stuff\"\n   \u2713 Confirm | \u2717 Reject | \ud83d\udcdd Correct\n</code></pre></p>"},{"location":"PROFILES/#confidence-decay","title":"Confidence Decay","text":"<p>Confidence decreases over time if not reinforced: - Initial: Set at detection confidence - After 30 days: -10% - After 90 days: -20% - Below 0.2: Automatically asks for re-confirmation</p> <pre><code># Trust evolution affects how quickly confidence grows\nTRUST_EVOLUTION_RATE=0.05  # Per positive interaction\n</code></pre>"},{"location":"PROFILES/#response-adaptation","title":"Response Adaptation","text":""},{"location":"PROFILES/#formality-adaptation","title":"Formality Adaptation","text":"<p>Based on <code>Preferences.formality</code>:</p> <p>Casual (0.0-0.3): <pre><code>Hey! Here's the thing about that...\n</code></pre></p> <p>Balanced (0.4-0.6): <pre><code>Here's what you need to know about that topic...\n</code></pre></p> <p>Formal (0.7-1.0): <pre><code>I would like to provide you with information regarding...\n</code></pre></p>"},{"location":"PROFILES/#verbosity-adaptation","title":"Verbosity Adaptation","text":"<p>Based on <code>Preferences.verbosity</code>:</p> <p>Brief (0.0-0.3): <pre><code>Use `git rebase -i HEAD~3` to squash commits.\n</code></pre></p> <p>Balanced (0.4-0.6): <pre><code>To squash the last 3 commits, use interactive rebase:\n`git rebase -i HEAD~3`\nThen change 'pick' to 'squash' for commits to combine.\n</code></pre></p> <p>Detailed (0.7-1.0): <pre><code>To squash commits, you'll use interactive rebase...\n[Full explanation with examples, edge cases, and warnings]\n</code></pre></p>"},{"location":"PROFILES/#technical-level","title":"Technical Level","text":"<p>Based on <code>Skills</code> profile:</p> <p>Beginner-friendly: <pre><code>Docker is like a lightweight virtual machine...\n</code></pre></p> <p>Technical: <pre><code>Docker uses Linux namespaces and cgroups for container isolation...\n</code></pre></p>"},{"location":"PROFILES/#coding-style","title":"Coding Style","text":"<p>Based on <code>Preferences.coding_style</code>:</p> <p>If user prefers Python, code examples will be in Python when possible: <pre><code>def greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n</code></pre></p>"},{"location":"PROFILES/#privacy-and-data-control","title":"Privacy and Data Control","text":""},{"location":"PROFILES/#data-storage","title":"Data Storage","text":"<p>Profile data is stored in: - Qdrant: Vector embeddings for semantic search - SQLite: Structured profile data</p>"},{"location":"PROFILES/#encryption","title":"Encryption","text":"<p>When enabled, sensitive profile fields are encrypted:</p> <pre><code>ENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=your-secure-passphrase\n</code></pre> <p>Encrypted fields: - Name, location (Identity) - All Relationships data - Project details - Goals content</p>"},{"location":"PROFILES/#data-export-gdpr","title":"Data Export (GDPR)","text":"<p>Export all your data:</p> <pre><code>@Zetherion AI export my data\n</code></pre> <p>Output: <pre><code>\ud83d\udce6 Data Export\n\nYour data has been compiled. Here's a summary:\n\nProfile: 15 entries\nMemories: 47 items\nTasks: 12 items\nCalendar: 8 events\n\nDownload link: [Generated JSON file]\n</code></pre></p>"},{"location":"PROFILES/#data-deletion","title":"Data Deletion","text":"<p>Delete all personal data:</p> <pre><code>@Zetherion AI delete all my data\n&gt; This will permanently delete:\n&gt; - Your profile (15 entries)\n&gt; - Your memories (47 items)\n&gt; - Your tasks (12 items)\n&gt; - Your calendar events (8 events)\n&gt;\n&gt; Type 'DELETE ALL' to confirm.\n</code></pre>"},{"location":"PROFILES/#opt-out","title":"Opt-Out","text":"<p>Disable profile learning entirely:</p> <pre><code>PROFILE_INFERENCE_ENABLED=false\n</code></pre> <p>Or per-user: <pre><code>@Zetherion AI disable profile learning\n</code></pre></p>"},{"location":"PROFILES/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PROFILES/#profile-not-learning","title":"Profile Not Learning","text":"<p>Check configuration: <pre><code>PROFILE_INFERENCE_ENABLED=true\n</code></pre></p> <p>Check logs: <pre><code>docker-compose logs zetherion-ai-bot | grep -i profile\n</code></pre></p>"},{"location":"PROFILES/#wrong-information-learned","title":"Wrong Information Learned","text":"<p>Correct it: <pre><code>@Zetherion AI that's not right, my name is actually James\n@Zetherion AI update my location to Sydney\n</code></pre></p> <p>Or delete and re-add: <pre><code>@Zetherion AI forget my name\n@Zetherion AI my name is James\n</code></pre></p>"},{"location":"PROFILES/#too-many-confirmations","title":"Too Many Confirmations","text":"<p>Increase auto-apply threshold: <pre><code>PROFILE_CONFIDENCE_THRESHOLD=0.5  # Lower = more auto-applies\n</code></pre></p> <p>Or use Tier 1 only: <pre><code>PROFILE_TIER1_ONLY=true\n</code></pre></p>"},{"location":"PROFILES/#profile-not-affecting-responses","title":"Profile Not Affecting Responses","text":"<p>Check profile is loaded: <pre><code>@Zetherion AI show my profile\n</code></pre></p> <p>Ensure preferences are set: <pre><code>@Zetherion AI set my preference for detailed explanations\n</code></pre></p>"},{"location":"PROFILES/#database-issues","title":"Database Issues","text":"<p>Reset profile database: <pre><code># Backup first\ndocker cp zetherion-ai-bot:/app/data/profiles.db ./profiles_backup.db\n\n# Reset\ndocker exec zetherion-ai-bot rm /app/data/profiles.db\ndocker-compose restart zetherion-ai-bot\n</code></pre></p>"},{"location":"PROFILES/#database-schema","title":"Database Schema","text":"<p>Profile data uses SQLite:</p> <pre><code>CREATE TABLE profiles (\n    user_id TEXT PRIMARY KEY,\n    data JSON NOT NULL,\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE profile_entries (\n    id INTEGER PRIMARY KEY,\n    user_id TEXT NOT NULL,\n    category TEXT NOT NULL,\n    field TEXT NOT NULL,\n    value TEXT NOT NULL,\n    confidence REAL NOT NULL,\n    source TEXT NOT NULL,\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n    UNIQUE(user_id, category, field)\n);\n\nCREATE TABLE pending_confirmations (\n    id INTEGER PRIMARY KEY,\n    user_id TEXT NOT NULL,\n    category TEXT NOT NULL,\n    field TEXT NOT NULL,\n    proposed_value TEXT NOT NULL,\n    confidence REAL NOT NULL,\n    source_message TEXT,\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n    expires_at DATETIME NOT NULL\n);\n</code></pre>"},{"location":"PROFILES/#additional-resources","title":"Additional Resources","text":"<ul> <li>Features Overview - All Phase 5+ features</li> <li>Skills Guide - Profile skill commands</li> <li>Security Guide - Encryption details</li> <li>Configuration Reference - All settings</li> </ul> <p>Last Updated: 2026-02-07 Version: 3.0.0 (Profile System)</p>"},{"location":"SECURITY/","title":"Security Overview","text":"<p>This document describes how Zetherion AI is secured across the full development lifecycle: how we protect credentials, validate input, scan for vulnerabilities, test, and deploy.</p>"},{"location":"SECURITY/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Secret Management</li> <li>Secret Scanning (Pre-Commit)</li> <li>Input Validation &amp; Prompt Injection Defence</li> <li>Access Control</li> <li>Static Analysis &amp; Code Quality</li> <li>Dependency Management</li> <li>Container Security</li> <li>CI/CD Pipeline Security</li> <li>Testing Strategy</li> <li>Logging &amp; Monitoring</li> <li>Network Security</li> <li>Data Encryption (Phase 5A)</li> <li>Gap Analysis &amp; Recommendations</li> </ol>"},{"location":"SECURITY/#1-secret-management","title":"1. Secret Management","text":"<p>All credentials are loaded from environment variables via a <code>.env</code> file and never hardcoded in source.</p> Control Implementation File Typed secrets <code>pydantic.SecretStr</code> prevents accidental logging/serialisation of tokens <code>src/zetherion_ai/config.py</code> <code>.env</code> excluded from Git <code>.gitignore</code> blocks <code>.env</code>, <code>data/</code>, <code>ollama_models/</code> <code>.gitignore</code> Example file provided <code>.env.example</code> documents every variable without real values <code>.env.example</code> Minimal exposure <code>SecretStr.get_secret_value()</code> called only at point-of-use (API client init) Agent &amp; router modules Startup masking <code>start.sh</code> prints only the first 20 characters of tokens in its config summary <code>start.sh</code>"},{"location":"SECURITY/#how-secrets-flow","title":"How Secrets Flow","text":"<pre><code>.env  --&gt;  pydantic-settings (SecretStr)  --&gt;  get_secret_value() at API call site\n            ^                                         |\n            |  Never logged, never serialised         v\n            +--- structlog sees SecretStr repr: '**********'\n</code></pre>"},{"location":"SECURITY/#2-secret-scanning-pre-commit","title":"2. Secret Scanning (Pre-Commit)","text":"<p>Gitleaks runs on every <code>git commit</code> to prevent credentials from entering version control.</p> <p>Configuration: <code>.gitleaks.toml</code></p>"},{"location":"SECURITY/#what-it-detects","title":"What It Detects","text":"Rule Pattern Example Discord bot tokens <code>[MN][A-Za-z\\d]{23}\\.[\\w-]{6}\\.[\\w-]{27}</code> <code>MTk...</code> Discord webhooks <code>discord(app)?\\.com/api/webhooks/\\d+/[\\w-]+</code> Webhook URLs Google/Gemini API keys <code>AIza[0-9A-Za-z\\\\-_]{35}</code> <code>AIzaSy...</code> Anthropic API keys <code>sk-ant-api03-[A-Za-z0-9_-]{95,100}</code> <code>sk-ant-api03-...</code> OpenAI API keys <code>sk-[a-zA-Z0-9]{48}</code> <code>sk-...</code> GitHub PATs <code>ghp_[A-Za-z0-9_]{36}</code> <code>ghp_...</code> AWS access keys <code>AKIA[0-9A-Z]{16}</code> <code>AKIA...</code> Slack tokens <code>xox[baprs]-[A-Za-z0-9-]+</code> <code>xoxb-...</code> Private keys <code>BEGIN.*PRIVATE KEY</code> PEM/SSH/PGP keys JWT tokens <code>eyJ[A-Za-z0-9_-]+\\.eyJ[A-Za-z0-9_-]+</code> <code>eyJhbG...</code> Passwords in URLs <code>://[^/\\s]+:[^/\\s]+@</code> <code>postgres://user:pass@host</code> High-entropy strings Shannon entropy &gt; 4.5 Random-looking hex/base64"},{"location":"SECURITY/#false-positive-filtering","title":"False Positive Filtering","text":"<p>The allowlist avoids blocking legitimate code:</p> <ul> <li>Paths: <code>.env.example</code>, <code>README.md</code>, <code>docs/</code>, test fixtures</li> <li>Patterns: <code>test-*-key</code>, <code>.get_secret_value()</code> calls, hash patterns</li> <li>Gitignored files: <code>.env</code>, <code>data/</code>, <code>ollama_models/</code> are excluded automatically</li> </ul>"},{"location":"SECURITY/#3-input-validation-prompt-injection-defence","title":"3. Input Validation &amp; Prompt Injection Defence","text":"<p>File: <code>src/zetherion_ai/discord/security.py</code></p> <p>Every user message is checked before being forwarded to LLM backends.</p>"},{"location":"SECURITY/#detection-techniques","title":"Detection Techniques","text":"<ol> <li>17 regex patterns covering prompt injection variations:</li> <li>\"ignore previous/prior instructions\" (with spacing/punctuation tolerance)</li> <li>\"disregard/forget/override your rules\"</li> <li>\"you are now a...\" role reassignment</li> <li>\"act as if / pretend to be\"</li> <li>\"new instructions:\" injection headers</li> <li>\"system prompt/message:\" attempts</li> <li>\"jailbreak\", \"DAN mode\", \"developer mode\" keywords</li> <li>\"disable/bypass filters/safety/restrictions\"</li> <li> <p>All patterns are case-insensitive</p> </li> <li> <p>Roleplay marker heuristic: Flags messages with &gt; 5 brackets or <code>(system</code> markers, which indicate structured injection attempts.</p> </li> <li> <p>Unicode obfuscation detection: Compares NFKC-normalised text to original. A length difference &gt; 10% indicates homoglyph substitution (e.g. Cyrillic \"\u0430\" replacing Latin \"a\" to bypass keyword filters).</p> </li> </ol>"},{"location":"SECURITY/#defence-behaviour","title":"Defence Behaviour","text":"<ul> <li>Flagged messages are logged with <code>potential_prompt_injection_detected</code> and the matched pattern.</li> <li>The message is rejected before reaching any LLM.</li> <li>Graceful degradation: if the Unicode check fails, it is skipped rather than crashing.</li> </ul>"},{"location":"SECURITY/#4-access-control","title":"4. Access Control","text":""},{"location":"SECURITY/#user-allowlist","title":"User Allowlist","text":"Mode Behaviour <code>ALLOWED_USER_IDS</code> set Only listed Discord user IDs can interact with the bot <code>ALLOWED_USER_IDS</code> empty All users permitted (logs a warning at startup) <p>Users can be added/removed at runtime via <code>UserAllowlist.add()</code> / <code>.remove()</code>. All changes are logged.</p>"},{"location":"SECURITY/#rate-limiting","title":"Rate Limiting","text":"Parameter Default Max messages per window 10 Window duration 60 seconds Warning cooldown 30 seconds <p>Per-user tracking with automatic timestamp cleanup. Exceeding the limit returns a user-facing warning (throttled to one warning per cooldown period).</p>"},{"location":"SECURITY/#5-static-analysis-code-quality","title":"5. Static Analysis &amp; Code Quality","text":""},{"location":"SECURITY/#tools-that-run-on-every-commit-pre-commit-hooks","title":"Tools That Run on Every Commit (Pre-Commit Hooks)","text":"Tool Version What It Checks Ruff linter v0.2.2 PEP 8, unused imports, complexity, bugbear, simplify (rule sets: E, F, I, N, W, UP, B, C4, SIM) Ruff formatter v0.2.2 Consistent formatting, 100-char line length mypy v1.8.0 Full strict type checking (<code>strict = true</code>) on <code>src/zetherion_ai</code> Bandit 1.7.7 Common Python security issues (SQL injection, hardcoded passwords, exec calls) Hadolint v2.12.0 Dockerfile best practices pre-commit-hooks v4.5.0 Large files (&gt;1MB), merge conflicts, YAML/TOML/JSON syntax, trailing whitespace, private key detection"},{"location":"SECURITY/#codeql-github-advanced-security","title":"CodeQL (GitHub Advanced Security)","text":"<ul> <li>File: <code>.github/workflows/codeql.yml</code></li> <li>Schedule: Every Tuesday + on push/PR to <code>main</code></li> <li>Language: Python semantic analysis</li> <li>Detects injection flaws, insecure deserialization, crypto weaknesses, etc.</li> </ul>"},{"location":"SECURITY/#6-dependency-management","title":"6. Dependency Management","text":"Control Implementation Pinned versions <code>requirements.txt</code> pins every dependency to exact versions (e.g. <code>discord.py==2.4.0</code>) Dependabot <code>.github/dependabot.yml</code> checks pip (weekly), Docker images (weekly), GitHub Actions (monthly) Separated environments <code>requirements.txt</code> (production) vs <code>requirements-dev.txt</code> (dev tools) No cache in image <code>pip install --no-cache-dir</code> prevents stale packages in containers Multi-version testing CI runs tests on Python 3.12 and 3.13"},{"location":"SECURITY/#7-container-security","title":"7. Container Security","text":""},{"location":"SECURITY/#distroless-containers-updated-2026-02-07","title":"Distroless Containers (Updated 2026-02-07)","text":"<p>Zetherion AI now uses Google's distroless base images for all production containers, providing a significant security improvement over traditional container images.</p>"},{"location":"SECURITY/#what-are-distroless-containers","title":"What Are Distroless Containers?","text":"<p>Distroless images contain only your application and its runtime dependencies. They do not include: - \u274c Shell (<code>/bin/sh</code>, <code>/bin/bash</code>) - \u274c Package managers (<code>apt</code>, <code>yum</code>, <code>apk</code>) - \u274c System utilities (<code>curl</code>, <code>wget</code>, <code>nc</code>, etc.) - \u274c OS libraries not required by the application</p>"},{"location":"SECURITY/#security-benefits","title":"Security Benefits","text":"Feature Traditional (<code>python:3.11-slim</code>) Distroless (<code>gcr.io/distroless/python3-debian12</code>) Image Size ~150MB ~50MB (70% smaller) Shell Access \u2705 <code>/bin/bash</code>, <code>/bin/sh</code> \u274c No shell Package Managers \u2705 <code>apt</code>, <code>dpkg</code> \u274c None System Utilities \u2705 <code>curl</code>, <code>wget</code>, <code>nc</code>, etc. \u274c None Default User <code>root</code> (UID 0) <code>nonroot</code> (UID 65532) Attack Surface Large (hundreds of binaries) Minimal (Python runtime + app only) CVE Count High (OS packages + Python) Low (Python runtime only) GitHub Security Scan \u26a0\ufe0f Multiple vulnerabilities \u2705 Zero critical/high CVEs"},{"location":"SECURITY/#multi-stage-build-process","title":"Multi-Stage Build Process","text":"<pre><code># Stage 1: Builder (python:3.11-slim)\nFROM python:3.11-slim as builder\nWORKDIR /app\nCOPY requirements.txt ./\nRUN pip install --user --no-cache-dir -r requirements.txt\nCOPY src ./src\nENV PYTHONPATH=/app/src\n# Verify imports work before creating runtime image\nRUN python -c \"from zetherion_ai.discord.bot import ZetherionAIBot; print('\u2713 Verified')\"\n\n# Stage 2: Runtime (distroless)\nFROM gcr.io/distroless/python3-debian12:nonroot\nCOPY --from=builder /root/.local /root/.local\nCOPY --from=builder /app/src /app/src\nENV PYTHONPATH=/app/src\nENV PATH=/root/.local/bin:$PATH\nCMD [\"python\", \"-m\", \"zetherion_ai\"]\n</code></pre> <p>Key Features: 1. Builder stage: Uses full Python image with pip to install dependencies 2. Import verification: Tests all imports work before creating runtime image 3. Runtime stage: Copies only installed packages and application code 4. Non-root user: Runs as UID 65532 (<code>nonroot</code>) by default 5. No shell: ENTRYPOINT is <code>/usr/bin/python3.11</code>, CMD only passes arguments</p>"},{"location":"SECURITY/#attack-surface-reduction","title":"Attack Surface Reduction","text":"<p>Before (Traditional Container): Attacker gains code execution \u2192 Can use shell to: - \u274c Download malware with <code>curl</code>/<code>wget</code> - \u274c Scan network with <code>nc</code>/<code>nmap</code> - \u274c Install packages with <code>apt install</code> - \u274c Escalate privileges with system utilities - \u274c Persist with cron jobs or systemd services</p> <p>After (Distroless Container): Attacker gains code execution \u2192 Limited to Python: - \u2705 No shell to execute commands - \u2705 No package managers to install tools - \u2705 No system utilities for reconnaissance - \u2705 Can only use Python standard library - \u2705 Runs as non-root (UID 65532) - \u2705 Minimal filesystem (only app code + Python libs)</p>"},{"location":"SECURITY/#verification","title":"Verification","text":"<pre><code># Try to get a shell (will fail - no shell in distroless)\ndocker exec -it zetherion-ai-bot /bin/sh\n# Error: exec: \"/bin/sh\": stat /bin/sh: no such file or directory\n\n# Verify running as non-root\ndocker exec zetherion-ai-bot python -c \"import os; print(f'UID: {os.getuid()}')\"\n# UID: 65532 (nonroot)\n\n# Check image size\ndocker images | grep zetherion-ai\n# zetherion-ai-bot    latest    ...    ~50MB (vs ~150MB traditional)\n</code></pre>"},{"location":"SECURITY/#dockerfile-controls","title":"Dockerfile Controls","text":""},{"location":"SECURITY/#docker-compose-docker-composeyml","title":"Docker Compose (<code>docker-compose.yml</code>)","text":"Control Detail Health checks All services (Qdrant, Ollama) have TCP health checks with intervals, timeouts, retries, and start periods Restart policy <code>unless-stopped</code> for resilience Service dependency Bot waits for <code>qdrant: service_healthy</code> before starting Network isolation All services on a dedicated <code>zetherion_ai-net</code> bridge network Named volumes <code>qdrant_storage</code>, <code>ollama_models</code> for persistent data No privileged mode Containers run without elevated privileges"},{"location":"SECURITY/#8-cicd-pipeline-security","title":"8. CI/CD Pipeline Security","text":"<p>File: <code>.github/workflows/ci.yml</code></p>"},{"location":"SECURITY/#pipeline-architecture-6-jobs-summary","title":"Pipeline Architecture (6 Jobs + Summary)","text":"<pre><code>Push/PR to main or develop\n          |\n          v\n   +------+------+-------+\n   |      |      |       |\n  Lint  Types  Security  Docker Build\n   |      |      |       |\n   +------+------+       |\n          |               |\n          v               |\n     Unit Tests           |\n    (Py 3.12, 3.13)      |\n          |               |\n          +-------+-------+\n                  |\n                  v\n          Integration Tests\n         (E2E with Docker)\n                  |\n                  v\n            CI Summary\n       (aggregates results)\n</code></pre>"},{"location":"SECURITY/#job-details","title":"Job Details","text":"Job Duration What It Does Lint ~5s <code>ruff check</code> + <code>ruff format --check</code> on <code>src/</code> and <code>tests/</code> Type Check ~10s <code>mypy src/zetherion_ai</code> in strict mode Security ~10s <code>bandit -r src/</code> for Python security issues Test ~60s Unit tests on Python 3.12 + 3.13 matrix, coverage to Codecov Docker Build ~30s Buildx with GHA caching, validates <code>docker compose config</code> Integration ~2-3min Full E2E with Docker Compose, uploads logs on failure, auto-cleanup Summary ~5s Markdown table in PR, fails if any check failed"},{"location":"SECURITY/#security-specific-ci-controls","title":"Security-Specific CI Controls","text":"<ul> <li>Secrets via GitHub Actions secrets: <code>DISCORD_TOKEN</code>, <code>GEMINI_API_KEY</code>, etc. are injected at runtime, never stored in code.</li> <li>Conditional integration tests: Skip with <code>[skip integration]</code> in commit message.</li> <li>Artifact retention: Coverage reports (30 days), failure logs (7 days).</li> <li>Container cleanup: <code>docker compose down -v</code> runs in <code>if: always()</code> block.</li> </ul>"},{"location":"SECURITY/#9-testing-strategy","title":"9. Testing Strategy","text":""},{"location":"SECURITY/#three-tier-testing-approach","title":"Three-Tier Testing Approach","text":"Tier When Duration What Pre-commit On <code>git commit</code> ~5-10s Linting, formatting, secret scanning, Bandit, Hadolint Pre-push On <code>git push</code> ~30-60s Ruff, mypy, full pytest suite with coverage CI/CD On push/PR ~5-10min All of the above + Docker build + integration tests + CodeQL"},{"location":"SECURITY/#test-coverage","title":"Test Coverage","text":"<p>Overall Coverage: 87.58% (255 unit tests + 14 integration tests + 4 Discord E2E tests)</p> Module Coverage Tests What's Tested Router Factory 100% 12 Async/sync factory functions, health checks, Ollama\u2192Gemini fallback, error handling, backend selection validation Config 96.88% 49 Settings validation, SecretStr handling, field validators, environment variable isolation, comma-separated parsing Security 94.12% 37 Rate limiter (under/over limit, per-user isolation), user allowlist (empty/configured, add/remove), 24+ prompt injection patterns, Unicode obfuscation, false positive prevention Agent Core 94.76% 41 Agent initialisation, context building, retry logic with exponential backoff, dual-generator responses, memory operations Discord Bot 89.92% 30 Command handling (/ask, /remember, /search, /channels), authorization, rate limiting, message splitting (2000 char limit), DM vs mention handling, agent not ready edge cases Qdrant Memory 88.73% 7 Vector DB operations (store, search, delete), async client usage, collection management Router (Gemini) 83.19% 21 Gemini routing, JSON parsing, classification, simple response generation, error handling Router (Ollama) 98.00% 26 Ollama routing, model selection, health checks, fallback behaviour, timeout handling Embeddings 100% 5 Embedding generation, batch processing, parallel operations Integration Tests N/A 14 Full stack: Docker services healthy (Qdrant + Ollama), collections exist, message flow through both Gemini and Ollama backends, memory persistence, conversation context Discord E2E Tests N/A 4 Real Discord API: bot responses, complex queries, memory recall validation (LLM-based), mention handling"},{"location":"SECURITY/#recent-test-improvements-phase-1-2","title":"Recent Test Improvements (Phase 1 &amp; 2)","text":"<p>Phase 1: Fixed All Test Failures - Fixed 10 config tests with environment variable isolation using <code>monkeypatch.delenv()</code> - Fixed 13 agent core tests with improved mocking and assertions - Fixed 3 security tests with enhanced prompt injection pattern detection - Fixed 14 Docker integration tests with proper <code>.env</code> loading and container cleanup - Added <code>pythonpath = [\"src\"]</code> to pytest configuration for proper module imports</p> <p>Phase 2: Improved Coverage to 87.58% - Router Factory: 26% \u2192 100% (added 12 comprehensive tests for factory pattern, health checks, fallback logic) - Discord Bot: 68.55% \u2192 89.92% (added 11 edge case tests for /channels command, message splitting, agent readiness) - Overall: ~42% \u2192 87.58% (net gain of 45.58 percentage points)</p> <p>All 255 unit tests now pass with zero failures, comprehensive edge case coverage, and proper async/await support throughout.</p>"},{"location":"SECURITY/#integration-test-parametrisation","title":"Integration Test Parametrisation","text":"<p>Tests run against both router backends automatically:</p> <pre><code>@pytest.fixture(scope=\"module\", params=[\"gemini\", \"ollama\"])\ndef router_backend(request, docker_env):\n    ...\n</code></pre> <p>This produces 14 tests (7 scenarios x 2 backends) with no code duplication.</p>"},{"location":"SECURITY/#coverage","title":"Coverage","text":"<ul> <li>Branch coverage enabled</li> <li>HTML + XML reports generated</li> <li>Uploaded to Codecov on Python 3.12 runs</li> <li>Exclusions: <code>pragma: no cover</code>, <code>__repr__</code>, <code>TYPE_CHECKING</code>, abstract methods</li> </ul>"},{"location":"SECURITY/#10-logging-monitoring","title":"10. Logging &amp; Monitoring","text":"<p>File: <code>src/zetherion_ai/logging.py</code></p> Control Detail Structured logging <code>structlog</code> with JSON output (production) or coloured console (development) Log rotation <code>RotatingFileHandler</code>: 10MB max, 5 backups No credential leakage <code>SecretStr</code> objects log as <code>'**********'</code>, never the actual value Security events logged Prompt injection attempts, allowlist changes, rate limit triggers Third-party noise suppression Discord and httpx loggers set to WARNING"},{"location":"SECURITY/#11-network-security","title":"11. Network Security","text":"Control Detail Docker bridge network All services communicate on <code>zetherion_ai-net</code>, isolated from host network by default No exposed ports in production Only Qdrant (6333) and Ollama (11434) expose ports (for dev/test); the bot container exposes none TLS for external APIs All API clients (Anthropic, OpenAI, Gemini) use HTTPS by default via <code>httpx</code> No inbound web server The bot connects outbound to Discord's gateway; it does not listen on any HTTP port"},{"location":"SECURITY/#12-data-encryption-phase-5a","title":"12. Data Encryption (Phase 5A)","text":"<p>Files: <code>src/zetherion_ai/security/encryption.py</code>, <code>src/zetherion_ai/security/keys.py</code></p> <p>Zetherion AI implements application-layer encryption for sensitive data stored in Qdrant. This protects data at rest even if the database is compromised.</p>"},{"location":"SECURITY/#encryption-architecture","title":"Encryption Architecture","text":"<pre><code>User Message --&gt; Gemini Embedding --&gt; Qdrant Payload\n                                          |\n                                          v\n                              FieldEncryptor.encrypt_payload()\n                                          |\n                                          v\n                              \"content\" field encrypted with AES-256-GCM\n                                          |\n                                          v\n                              Stored in Qdrant (ciphertext)\n</code></pre>"},{"location":"SECURITY/#cryptographic-controls","title":"Cryptographic Controls","text":"Control Implementation Why Algorithm AES-256-GCM (authenticated encryption) Industry standard, provides both confidentiality and integrity Key Derivation PBKDF2-HMAC-SHA256, 600,000 iterations OWASP-recommended iteration count for password-based keys Nonce Generation 96-bit random nonce per encryption Prevents nonce reuse attacks; GCM standard Salt 256-bit random, persisted to <code>data/salt.bin</code> Unique per installation, prevents rainbow table attacks Passphrase User-provided via <code>ENCRYPTION_PASSPHRASE</code> (min 16 chars) SecretStr, never logged"},{"location":"SECURITY/#what-gets-encrypted","title":"What Gets Encrypted","text":"Collection Encrypted Field(s) Plaintext Fields <code>conversations</code> <code>content</code> <code>user_id</code>, <code>channel_id</code>, <code>role</code>, <code>timestamp</code> <code>long_term_memory</code> <code>content</code> <code>type</code>, <code>timestamp</code>, metadata <code>user_profiles</code> (Phase 5C) <code>key</code>, <code>value</code> <code>category</code>, <code>confidence</code>, <code>user_id</code> <code>skill_tasks</code> (Phase 5E) <code>title</code>, <code>description</code> <code>status</code>, <code>priority</code>, <code>deadline</code>"},{"location":"SECURITY/#security-properties","title":"Security Properties","text":"<ul> <li>Embeddings remain unencrypted: Required for vector similarity search. Property-preserving encryption is a future enhancement.</li> <li>Decryption failures: Logged and passed through (graceful handling of legacy unencrypted data).</li> <li>Key rotation: Supported via <code>KeyManager.rotate_key()</code> but requires data migration.</li> <li>Tamper detection: GCM authentication tag detects any modification to ciphertext.</li> </ul>"},{"location":"SECURITY/#enabling-encryption","title":"Enabling Encryption","text":"<pre><code># 1. Generate a strong passphrase\nopenssl rand -base64 32\n\n# 2. Add to .env\nENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=\"your-generated-passphrase-here\"\n\n# 3. Salt file created automatically on first run at data/salt.bin\n</code></pre>"},{"location":"SECURITY/#tls-for-qdrant-in-transit-encryption","title":"TLS for Qdrant (In-Transit Encryption)","text":"<p>For complete protection, TLS can be enabled for Qdrant connections:</p> <pre><code># Generate self-signed certificates\n./scripts/generate-qdrant-certs.sh\n\n# Enable in .env\nQDRANT_USE_TLS=true\n\n# Uncomment TLS mounts in docker-compose.yml\n</code></pre> <p>This provides encryption in transit between the bot container and Qdrant, completing the defense-in-depth for stored data.</p>"},{"location":"SECURITY/#13-gap-analysis-recommendations","title":"13. Gap Analysis &amp; Recommendations","text":"<p>The following automated security best practices have been fully implemented as of 2026-02-06. This section documents what was done and serves as a reference for the security controls now in place.</p>"},{"location":"SECURITY/#high-impact","title":"HIGH IMPACT","text":""},{"location":"SECURITY/#121-container-image-scanning","title":"12.1 Container Image Scanning","text":"<p>Gap: Docker images are built but never scanned for OS-level vulnerabilities (outdated <code>libc</code>, OpenSSL, etc.).</p> <p>Recommendation: Add Trivy or Grype scanning to CI.</p> <pre><code># Example: Add to .github/workflows/ci.yml after docker-build\n- name: Run Trivy vulnerability scanner\n  uses: aquasecurity/trivy-action@master\n  with:\n    image-ref: zetherion_ai:test\n    format: 'sarif'\n    output: 'trivy-results.sarif'\n    severity: 'CRITICAL,HIGH'\n\n- name: Upload Trivy scan results\n  uses: github/codeql-action/upload-sarif@v3\n  with:\n    sarif_file: 'trivy-results.sarif'\n</code></pre> <p>This integrates with GitHub's Security tab for a unified vulnerability view.</p> <p>References: - Trivy GitHub Action - OWASP Docker Security Cheat Sheet</p>"},{"location":"SECURITY/#122-pin-docker-base-images-by-digest","title":"12.2 Pin Docker Base Images by Digest","text":"<p>Gap: <code>python:3.12-slim</code> and <code>qdrant/qdrant:latest</code> use mutable tags. A compromised or broken upstream push would silently affect builds.</p> <p>Recommendation: Pin by SHA256 digest in <code>Dockerfile</code> and <code>docker-compose.yml</code>.</p> <pre><code># Instead of:\nFROM python:3.12-slim\n# Use:\nFROM python:3.12-slim@sha256:&lt;digest&gt;\n</code></pre> <p>Update digests via Dependabot (already configured for Docker ecosystem).</p> <p>References: - Docker Image Pinning Best Practices - Chainguard Images (minimal, signed alternatives)</p>"},{"location":"SECURITY/#123-software-bill-of-materials-sbom","title":"12.3 Software Bill of Materials (SBOM)","text":"<p>Gap: No SBOM is generated for the container image or Python dependencies. This makes it harder to respond to new CVEs (e.g. \"are we affected by CVE-XXXX in library Y?\").</p> <p>Recommendation: Generate SBOM during Docker build and store as a build artifact.</p> <pre><code>- name: Generate SBOM\n  uses: anchore/sbom-action@v0\n  with:\n    image: zetherion_ai:test\n    format: spdx-json\n    output-file: sbom.spdx.json\n\n- name: Upload SBOM\n  uses: actions/upload-artifact@v4\n  with:\n    name: sbom\n    path: sbom.spdx.json\n</code></pre> <p>References: - NTIA SBOM Minimum Elements - Anchore SBOM Action</p>"},{"location":"SECURITY/#124-signed-commits-branch-protection","title":"12.4 Signed Commits &amp; Branch Protection","text":"<p>Gap: No enforcement of signed commits or branch protection rules on <code>main</code>.</p> <p>Recommendation: 1. Enable branch protection on <code>main</code>: require PR reviews, status checks to pass, and linear history. 2. Require GPG or SSH signed commits to prevent commit spoofing. 3. Enable CODEOWNERS for critical paths (<code>src/zetherion_ai/discord/security.py</code>, <code>.github/workflows/</code>, <code>.gitleaks.toml</code>).</p> <pre><code># .github/CODEOWNERS\n/.github/       @jameshinton\n/src/zetherion_ai/discord/security.py  @jameshinton\n/.gitleaks.toml @jameshinton\n</code></pre> <p>References: - GitHub Branch Protection Rules - Signing Commits</p>"},{"location":"SECURITY/#medium-impact","title":"MEDIUM IMPACT","text":""},{"location":"SECURITY/#125-pip-audit-for-known-vulnerability-scanning","title":"12.5 <code>pip-audit</code> for Known Vulnerability Scanning","text":"<p>Gap: Dependencies are pinned but not checked against CVE databases. Dependabot covers this partially, but <code>pip-audit</code> gives faster feedback in CI.</p> <p>Recommendation: Add <code>pip-audit</code> as a CI step.</p> <pre><code>- name: Audit Python dependencies\n  run: |\n    pip install pip-audit\n    pip-audit -r requirements.txt --strict\n</code></pre> <p>References: - pip-audit - OSV (Open Source Vulnerabilities)</p>"},{"location":"SECURITY/#126-read-only-filesystem-in-production-container","title":"12.6 Read-Only Filesystem in Production Container","text":"<p>Gap: The bot container's filesystem is writable. A compromised process could modify application code.</p> <p>Recommendation: Set <code>read_only: true</code> in <code>docker-compose.yml</code> and use <code>tmpfs</code> for writable directories.</p> <pre><code>zetherion_ai:\n  read_only: true\n  tmpfs:\n    - /tmp\n  volumes:\n    - ./data:/app/data\n    - ./logs:/app/logs\n</code></pre> <p>References: - Docker Compose read_only - CIS Docker Benchmark 5.12</p>"},{"location":"SECURITY/#127-runtime-security-headers-resource-limits","title":"12.7 Runtime Security Headers / Resource Limits","text":"<p>Gap: No CPU/memory limits on containers. A runaway process (e.g. OOM from a large embedding batch) could starve other services.</p> <p>Recommendation: Set resource limits in <code>docker-compose.yml</code>.</p> <pre><code>zetherion_ai:\n  deploy:\n    resources:\n      limits:\n        cpus: '2.0'\n        memory: 2G\n      reservations:\n        cpus: '0.5'\n        memory: 512M\n</code></pre> <p>References: - Docker Compose Resource Constraints</p>"},{"location":"SECURITY/#128-non-root-user-in-dockerfile","title":"12.8 Non-Root User in Dockerfile","text":"<p>Status: \u2705 IMPLEMENTED (2026-02-07)</p> <p>Solution: Migrated to Google's distroless images which run as <code>nonroot</code> (UID 65532) by default.</p> <pre><code># Runtime stage uses distroless:nonroot variant\nFROM gcr.io/distroless/python3-debian12:nonroot\n# Automatically runs as UID 65532 (nonroot)\n</code></pre> <p>Benefits: - No need to manually create user (distroless handles it) - Consistent non-root UID across all deployments - Cannot accidentally run as root - Passes CIS Docker Benchmark 4.1</p> <p>Verification: <pre><code>docker exec zetherion-ai-bot python -c \"import os; print(f'UID: {os.getuid()}')\"\n# Output: UID: 65532\n</code></pre></p> <p>References: - Dockerfile USER best practice - CIS Docker Benchmark 4.1 - Google Distroless Nonroot Images</p>"},{"location":"SECURITY/#129-github-actions-workflow-hardening","title":"12.9 GitHub Actions Workflow Hardening","text":"<p>Gap: Actions use major version tags (e.g. <code>actions/checkout@v4</code>) which are mutable. A supply chain attack on an action would affect all workflows.</p> <p>Recommendation: Pin actions by SHA.</p> <pre><code># Instead of:\nuses: actions/checkout@v4\n# Use:\nuses: actions/checkout@&lt;full-sha&gt;\n</code></pre> <p>Also add <code>permissions</code> blocks to limit GITHUB_TOKEN scope per job:</p> <pre><code>jobs:\n  lint:\n    permissions:\n      contents: read\n</code></pre> <p>References: - GitHub Actions Security Hardening - StepSecurity Harden-Runner</p>"},{"location":"SECURITY/#low-impact-nice-to-have","title":"LOW IMPACT / NICE-TO-HAVE","text":""},{"location":"SECURITY/#1210-pre-commit-hook-integrity-verification","title":"12.10 Pre-Commit Hook Integrity Verification","text":"<p>Gap: <code>pre-commit</code> hooks can be skipped with <code>--no-verify</code>. There is no enforcement that hooks actually ran.</p> <p>Recommendation: Add a CI step that runs <code>pre-commit run --all-files</code> to catch any commits that bypassed local hooks. This is effectively a safety net.</p> <pre><code>- name: Run pre-commit checks\n  uses: pre-commit/action@v3.0.1\n</code></pre> <p>References: - pre-commit CI action</p>"},{"location":"SECURITY/#1211-security-policy-vulnerability-reporting","title":"12.11 Security Policy &amp; Vulnerability Reporting","text":"<p>Gap: No <code>SECURITY.md</code> at the repository root (GitHub's standard location) for vulnerability reporting instructions.</p> <p>Recommendation: Add a root-level <code>SECURITY.md</code> with: - Supported versions - How to report vulnerabilities (email or GitHub Security Advisories) - Expected response timeline</p> <p>References: - GitHub Security Policy</p>"},{"location":"SECURITY/#1212-automated-license-compliance","title":"12.12 Automated License Compliance","text":"<p>Gap: No automated check that all dependencies use compatible licenses (the project is MIT-licensed).</p> <p>Recommendation:</p> <pre><code>- name: Check dependency licenses\n  run: |\n    pip install pip-licenses\n    pip-licenses --allow-only=\"MIT;BSD;Apache-2.0;ISC;PSF;Python-2.0\" --fail-on-violation\n</code></pre> <p>References: - pip-licenses</p>"},{"location":"SECURITY/#summary-matrix","title":"Summary Matrix","text":"# Recommendation Impact Effort Status 12.1 Container image scanning (Trivy) HIGH Low \u2705 Implemented 12.2 Pin Docker images by digest HIGH Low \u2705 Implemented 12.3 SBOM generation HIGH Low \u2705 Implemented 12.4 Signed commits &amp; branch protection HIGH Medium \u2705 Implemented 12.5 <code>pip-audit</code> in CI MEDIUM Low \u2705 Implemented 12.6 Read-only container filesystem MEDIUM Low \u2705 Implemented 12.7 Container resource limits MEDIUM Low \u2705 Implemented 12.8 Explicit non-root user in Dockerfile MEDIUM Low \u2705 Implemented 12.9 Pin GitHub Actions by SHA MEDIUM Medium \u2705 Implemented 12.10 Pre-commit in CI (safety net) LOW Low \u2705 Implemented 12.11 Security policy at repo root LOW Low \u2705 Implemented 12.12 Automated license compliance LOW Low \u2705 Implemented"},{"location":"SECURITY/#additional-security-scanning-added-2026-02-06","title":"Additional Security Scanning (Added 2026-02-06)","text":"<p>Beyond the original 12 gaps, the following additional security measures were implemented:</p> Tool Category What It Does Semgrep CE SAST 3000+ Python rules with taint tracking and data-flow analysis. SARIF results uploaded to GitHub Security tab. Trivy (filesystem mode) Dependencies Scans OS-level packages inside the container that pip-audit can't see GitHub Ruleset Access Control Branch protection on <code>main</code>: require PRs, status checks, block force pushes CODEOWNERS Access Control Assigns ownership of security-critical paths for review requirements"},{"location":"SKILLS/","title":"Skills Framework Guide","text":"<p>The Skills Framework provides extensible capabilities for Zetherion AI, including task management, calendar awareness, and user profile management.</p>"},{"location":"SKILLS/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Built-in Skills</li> <li>Task Manager Skill</li> <li>Calendar Skill</li> <li>Profile Skill</li> <li>Skill Intents</li> <li>Skills Service Architecture</li> <li>Creating Custom Skills</li> </ul>"},{"location":"SKILLS/#overview","title":"Overview","text":"<p>Skills are modular components that extend Zetherion AI's capabilities. Each skill:</p> <ul> <li>Handles specific types of requests (intents)</li> <li>Maintains its own data storage</li> <li>Can perform proactive actions via heartbeat</li> <li>Has defined permissions and access controls</li> </ul>"},{"location":"SKILLS/#architecture","title":"Architecture","text":"<pre><code>User Message \u2192 Router \u2192 Intent Classification \u2192 Skill Handler \u2192 Response\n                                    \u2193\n                              Skill Service (REST API)\n                                    \u2193\n                              Skill Registry\n                                    \u2193\n                        Task Manager / Calendar / Profile\n</code></pre>"},{"location":"SKILLS/#built-in-skills","title":"Built-in Skills","text":"Skill Description Intents Task Manager Track tasks, todos, and projects TASK_MANAGEMENT Calendar Schedule awareness and availability CALENDAR_QUERY Profile User preference management PROFILE_QUERY"},{"location":"SKILLS/#task-manager-skill","title":"Task Manager Skill","text":"<p>Manage tasks, todos, and projects with priorities and deadlines.</p>"},{"location":"SKILLS/#task-properties","title":"Task Properties","text":"Property Description Values Title Task description Any text Status Current state BACKLOG, TODO, IN_PROGRESS, BLOCKED, DONE, CANCELLED Priority Importance level CRITICAL, HIGH, MEDIUM, LOW Project Grouping category Any text or null Deadline Due date ISO date or null Tags Labels List of strings"},{"location":"SKILLS/#commands","title":"Commands","text":""},{"location":"SKILLS/#creating-tasks","title":"Creating Tasks","text":"<pre><code>@Zetherion AI add task: Review PR #123\n@Zetherion AI create task: Update documentation with high priority\n@Zetherion AI new task: Deploy to production by Friday\n@Zetherion AI add task: Fix login bug for project AuthSystem\n</code></pre> <p>Parsing: - Priority detected from keywords: \"high priority\", \"urgent\", \"critical\" - Deadlines detected: \"by Friday\", \"due tomorrow\", \"before next week\" - Projects detected: \"for project X\", \"in project Y\"</p>"},{"location":"SKILLS/#listing-tasks","title":"Listing Tasks","text":"<pre><code>@Zetherion AI list my tasks\n@Zetherion AI show tasks for project AuthSystem\n@Zetherion AI what are my high priority tasks?\n@Zetherion AI show blocked tasks\n</code></pre> <p>Output: <pre><code>Your Tasks (5 total):\n\n\ud83d\udccb TODO:\n  1. [HIGH] Review PR #123\n  2. [MEDIUM] Update documentation\n\n\ud83d\udd04 IN PROGRESS:\n  3. [HIGH] Fix login bug (AuthSystem)\n\n\ud83d\udeab BLOCKED:\n  4. [CRITICAL] Deploy to production - waiting on QA\n\n\u2705 DONE (today):\n  5. [LOW] Update README\n</code></pre></p>"},{"location":"SKILLS/#completing-tasks","title":"Completing Tasks","text":"<pre><code>@Zetherion AI complete task 1\n@Zetherion AI mark task \"Review PR\" as done\n@Zetherion AI finish task 3\n</code></pre>"},{"location":"SKILLS/#updating-tasks","title":"Updating Tasks","text":"<pre><code>@Zetherion AI update task 1 priority to critical\n@Zetherion AI move task 2 to blocked\n@Zetherion AI set deadline for task 1 to tomorrow\n@Zetherion AI add tag \"urgent\" to task 3\n</code></pre>"},{"location":"SKILLS/#deleting-tasks","title":"Deleting Tasks","text":"<pre><code>@Zetherion AI delete task 5\n@Zetherion AI remove completed tasks\n@Zetherion AI clear all tasks (requires confirmation)\n</code></pre>"},{"location":"SKILLS/#task-summary","title":"Task Summary","text":"<pre><code>@Zetherion AI task summary\n@Zetherion AI show my task stats\n</code></pre> <p>Output: <pre><code>Task Summary:\n  Total: 15 tasks\n  - Backlog: 3\n  - Todo: 5\n  - In Progress: 4\n  - Blocked: 1\n  - Done (this week): 2\n\n  By Priority:\n  - Critical: 1\n  - High: 4\n  - Medium: 7\n  - Low: 3\n\n  Overdue: 2 tasks\n  Due Today: 1 task\n</code></pre></p>"},{"location":"SKILLS/#data-storage","title":"Data Storage","text":"<p>Tasks are stored in Qdrant collection <code>skill_tasks</code> with vector embeddings for semantic search.</p>"},{"location":"SKILLS/#calendar-skill","title":"Calendar Skill","text":"<p>Schedule awareness and availability checking. Currently operates in \"awareness mode\" - learning from conversation rather than syncing with external calendars.</p>"},{"location":"SKILLS/#features","title":"Features","text":"<ul> <li>Work hours tracking: Knows your typical schedule</li> <li>Availability checking: Answers \"am I free at X?\"</li> <li>Event reminders: Proactive notifications</li> <li>Recurring patterns: Understands daily/weekly patterns</li> </ul>"},{"location":"SKILLS/#event-properties","title":"Event Properties","text":"Property Description Values Title Event name Any text Type Event category MEETING, DEADLINE, REMINDER, WORK_HOURS, BREAK, FOCUS_TIME, PERSONAL Start/End Time range ISO datetime Recurrence Repeat pattern DAILY, WEEKLY, BIWEEKLY, MONTHLY, YEARLY, WEEKDAYS Location Where Any text or null"},{"location":"SKILLS/#commands_1","title":"Commands","text":""},{"location":"SKILLS/#checking-schedule","title":"Checking Schedule","text":"<pre><code>@Zetherion AI what's my schedule today?\n@Zetherion AI show my calendar for this week\n@Zetherion AI what meetings do I have tomorrow?\n</code></pre> <p>Output: <pre><code>Today's Schedule (Mon, Feb 7):\n\n09:00 - 09:30  Team standup (recurring)\n10:00 - 11:00  Sprint planning\n12:00 - 13:00  Lunch break\n14:00 - 15:00  1:1 with manager\n16:00 - 17:00  Focus time (blocked)\n\nAvailable slots: 11:00-12:00, 13:00-14:00, 15:00-16:00\n</code></pre></p>"},{"location":"SKILLS/#checking-availability","title":"Checking Availability","text":"<pre><code>@Zetherion AI am I free at 3pm?\n@Zetherion AI can I schedule a meeting tomorrow at 10?\n@Zetherion AI when am I available this afternoon?\n</code></pre> <p>Output: <pre><code>At 3:00 PM today:\n\u274c Busy - \"Sprint review\" (14:30-15:30)\n\nNext available slot: 15:30-17:00 (1.5 hours)\n</code></pre></p>"},{"location":"SKILLS/#work-hours","title":"Work Hours","text":"<pre><code>@Zetherion AI when are my work hours?\n@Zetherion AI what time do I usually start?\n@Zetherion AI set my work hours to 9am-5pm\n</code></pre>"},{"location":"SKILLS/#adding-events-awareness-mode","title":"Adding Events (Awareness Mode)","text":"<pre><code>@Zetherion AI remember I have a meeting at 2pm tomorrow\n@Zetherion AI note that I'm on PTO next Friday\n@Zetherion AI I have standup every day at 9am\n</code></pre>"},{"location":"SKILLS/#future-calendar-integration","title":"Future: Calendar Integration","text":"<p>Planned integrations (not yet implemented): - Google Calendar sync - Microsoft Outlook sync - Apple Calendar sync - iCal import</p>"},{"location":"SKILLS/#profile-skill","title":"Profile Skill","text":"<p>Manage user preferences and personal information.</p>"},{"location":"SKILLS/#profile-categories","title":"Profile Categories","text":"Category Examples Usage Identity Name, location, timezone Personalization Preferences Coding style, verbosity Response adaptation Schedule Work hours, availability Calendar awareness Projects Current work, technologies Context Relationships Team, manager, reports Communication Skills Languages, expertise Recommendations Goals Learning, deadlines Motivation Habits Shortcuts, patterns Efficiency"},{"location":"SKILLS/#commands_2","title":"Commands","text":""},{"location":"SKILLS/#viewing-profile","title":"Viewing Profile","text":"<pre><code>@Zetherion AI show my profile\n@Zetherion AI what do you know about me?\n@Zetherion AI show my preferences\n</code></pre> <p>Output: <pre><code>Your Profile:\n\n\ud83d\udccb Identity:\n  Name: James (confidence: 95%)\n  Location: Sydney, Australia (confidence: 80%)\n  Timezone: AEDT (confidence: 90%)\n\n\ud83d\udcbc Work:\n  Role: Software Engineer (confidence: 85%)\n  Team: Platform (confidence: 70%)\n  Manager: Sarah (confidence: 60%)\n\n\u2699\ufe0f Preferences:\n  Coding Style: Python preferred (confidence: 90%)\n  Verbosity: Concise (confidence: 75%)\n  Formality: Casual (confidence: 85%)\n\n\ud83c\udfaf Current Projects:\n  - Zetherion AI (confidence: 95%)\n  - API Migration (confidence: 70%)\n\nLast updated: 2 hours ago\n</code></pre></p>"},{"location":"SKILLS/#updating-profile","title":"Updating Profile","text":"<pre><code>@Zetherion AI update my name to James\n@Zetherion AI my timezone is AEDT\n@Zetherion AI I prefer detailed explanations\n@Zetherion AI set my coding style to Python\n</code></pre>"},{"location":"SKILLS/#deleting-information","title":"Deleting Information","text":"<pre><code>@Zetherion AI forget my location\n@Zetherion AI remove my manager from profile\n@Zetherion AI clear my profile (requires confirmation)\n</code></pre>"},{"location":"SKILLS/#confidence-tracking","title":"Confidence Tracking","text":"<p>Profile entries have confidence scores (0-1): - 0.9+: High confidence, auto-applied - 0.6-0.9: Medium confidence, may ask confirmation - &lt;0.6: Low confidence, always asks confirmation</p> <pre><code>@Zetherion AI show profile confidence\n@Zetherion AI which profile items are uncertain?\n</code></pre>"},{"location":"SKILLS/#data-export-gdpr","title":"Data Export (GDPR)","text":"<pre><code>@Zetherion AI export my data\n@Zetherion AI download my profile\n</code></pre> <p>Exports JSON file with all stored data.</p>"},{"location":"SKILLS/#skill-intents","title":"Skill Intents","text":"<p>The router classifies messages into intents to route to the correct skill.</p>"},{"location":"SKILLS/#intent-detection","title":"Intent Detection","text":"Intent Keywords/Patterns Skill <code>TASK_MANAGEMENT</code> add task, create todo, list tasks, complete, delete task Task Manager <code>CALENDAR_QUERY</code> schedule, free, available, meeting, calendar, work hours Calendar <code>PROFILE_QUERY</code> my profile, update preference, forget, export data Profile <code>SIMPLE_QUERY</code> what is, explain, tell me about Agent (direct) <code>COMPLEX_TASK</code> analyze, help me, debug, review Agent (complex) <code>MEMORY_STORE</code> remember, note that, save Memory <code>MEMORY_RECALL</code> search for, find, what did I say Memory"},{"location":"SKILLS/#intent-examples","title":"Intent Examples","text":"<pre><code># Task Management\n\"add task: review PR\" \u2192 TASK_MANAGEMENT\n\"what tasks do I have?\" \u2192 TASK_MANAGEMENT\n\"complete task 1\" \u2192 TASK_MANAGEMENT\n\n# Calendar\n\"am I free at 3?\" \u2192 CALENDAR_QUERY\n\"what's my schedule?\" \u2192 CALENDAR_QUERY\n\"show my work hours\" \u2192 CALENDAR_QUERY\n\n# Profile\n\"show my profile\" \u2192 PROFILE_QUERY\n\"update my name\" \u2192 PROFILE_QUERY\n\"export my data\" \u2192 PROFILE_QUERY\n</code></pre>"},{"location":"SKILLS/#skills-service-architecture","title":"Skills Service Architecture","text":"<p>Skills run as a separate Docker service for isolation and scalability.</p>"},{"location":"SKILLS/#service-endpoints","title":"Service Endpoints","text":"Endpoint Method Purpose <code>/health</code> GET Health check <code>/skills</code> GET List available skills <code>/handle</code> POST Handle skill request <code>/heartbeat</code> POST Trigger heartbeat actions <code>/status</code> GET Skill status and metrics"},{"location":"SKILLS/#request-format","title":"Request Format","text":"<pre><code>{\n  \"skill_id\": \"task_manager\",\n  \"intent\": \"TASK_MANAGEMENT\",\n  \"user_id\": \"123456789\",\n  \"message\": \"add task: review PR #123\",\n  \"context\": {\n    \"channel_id\": \"987654321\",\n    \"guild_id\": \"111222333\"\n  }\n}\n</code></pre>"},{"location":"SKILLS/#response-format","title":"Response Format","text":"<pre><code>{\n  \"status\": \"success\",\n  \"skill_id\": \"task_manager\",\n  \"response\": \"Created task: 'review PR #123' with HIGH priority\",\n  \"data\": {\n    \"task_id\": \"task_abc123\",\n    \"title\": \"review PR #123\",\n    \"priority\": \"HIGH\",\n    \"status\": \"TODO\"\n  }\n}\n</code></pre>"},{"location":"SKILLS/#configuration","title":"Configuration","text":"<pre><code># Skills service URL (Docker internal)\nSKILLS_SERVICE_URL=http://zetherion-ai-skills:8080\n\n# API authentication\nSKILLS_API_SECRET=your-secret-here\n\n# Request timeout\nSKILLS_REQUEST_TIMEOUT=30\n</code></pre>"},{"location":"SKILLS/#creating-custom-skills","title":"Creating Custom Skills","text":"<p>Extend Zetherion AI with custom skills.</p>"},{"location":"SKILLS/#skill-base-class","title":"Skill Base Class","text":"<pre><code>from zetherion_ai.skills.base import Skill, SkillMetadata, SkillRequest, SkillResponse\n\nclass MyCustomSkill(Skill):\n    \"\"\"Custom skill implementation.\"\"\"\n\n    @property\n    def metadata(self) -&gt; SkillMetadata:\n        return SkillMetadata(\n            id=\"my_custom_skill\",\n            name=\"My Custom Skill\",\n            description=\"Does something useful\",\n            version=\"1.0.0\",\n            intents=[\"MY_CUSTOM_INTENT\"],\n        )\n\n    async def handle(self, request: SkillRequest) -&gt; SkillResponse:\n        \"\"\"Handle incoming request.\"\"\"\n        # Parse message and perform action\n        result = await self.do_something(request.message)\n\n        return SkillResponse(\n            status=\"success\",\n            response=f\"Result: {result}\",\n        )\n\n    async def heartbeat(self) -&gt; list[HeartbeatAction]:\n        \"\"\"Return proactive actions (optional).\"\"\"\n        return []\n</code></pre>"},{"location":"SKILLS/#registering-skills","title":"Registering Skills","text":"<pre><code>from zetherion_ai.skills.registry import SkillRegistry\n\nregistry = SkillRegistry()\nregistry.register(MyCustomSkill())\n</code></pre>"},{"location":"SKILLS/#skill-permissions","title":"Skill Permissions","text":"<p>Skills can require specific permissions:</p> <pre><code>from zetherion_ai.skills.permissions import Permission, PermissionSet\n\nclass MySkill(Skill):\n    @property\n    def required_permissions(self) -&gt; PermissionSet:\n        return PermissionSet([\n            Permission.READ_MESSAGES,\n            Permission.SEND_MESSAGES,\n            Permission.MANAGE_TASKS,  # Custom permission\n        ])\n</code></pre>"},{"location":"SKILLS/#data-storage_1","title":"Data Storage","text":"<p>Skills use Qdrant collections for persistent storage:</p> <pre><code>async def store_data(self, data: dict):\n    await self.memory.store(\n        collection=\"my_skill_data\",\n        content=data[\"content\"],\n        metadata={\"user_id\": data[\"user_id\"]},\n    )\n\nasync def search_data(self, query: str):\n    return await self.memory.search(\n        collection=\"my_skill_data\",\n        query=query,\n        limit=10,\n    )\n</code></pre>"},{"location":"SKILLS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SKILLS/#skill-not-responding","title":"Skill Not Responding","text":"<pre><code># Check skills service health\ncurl http://localhost:8080/health\n\n# View skills service logs\ndocker-compose logs zetherion-ai-skills\n\n# Restart skills service\ndocker-compose restart zetherion-ai-skills\n</code></pre>"},{"location":"SKILLS/#intent-not-recognized","title":"Intent Not Recognized","text":"<p>The router may not recognize custom intents. Check: 1. Intent keywords in router configuration 2. Skill registration in registry 3. Message format matches expected patterns</p>"},{"location":"SKILLS/#taskevent-not-saved","title":"Task/Event Not Saved","text":"<p>Check Qdrant connection: <pre><code># Verify Qdrant is healthy\ncurl http://localhost:6333/healthz\n\n# Check collections exist\ncurl http://localhost:6333/collections\n</code></pre></p>"},{"location":"SKILLS/#additional-resources","title":"Additional Resources","text":"<ul> <li>Features Overview - All Phase 5+ features</li> <li>Architecture - System design</li> <li>Configuration - Environment variables</li> <li>Troubleshooting - Common issues</li> </ul> <p>Last Updated: 2026-02-07 Version: 3.0.0 (Skills Framework)</p>"},{"location":"STARTUP_WALKTHROUGH/","title":"Zetherion AI Startup Script Walkthrough","text":"<p>A comprehensive guide to understanding the <code>start.sh</code> script - what it does, why, and how it handles errors.</p>"},{"location":"STARTUP_WALKTHROUGH/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Execution Flow Diagram</li> <li>Phase-by-Phase Breakdown</li> <li>Decision Trees</li> <li>Timing Expectations</li> <li>Error Handling</li> <li>Environment Variables</li> </ol>"},{"location":"STARTUP_WALKTHROUGH/#overview","title":"Overview","text":"<p>The <code>start.sh</code> script is the single entry point for running Zetherion AI. It handles:</p> <ul> <li>\u2705 Dependency verification (Python, Docker)</li> <li>\u2705 Environment configuration (.env file)</li> <li>\u2705 Virtual environment management</li> <li>\u2705 Docker container orchestration</li> <li>\u2705 Ollama model download and memory management</li> <li>\u2705 Automatic Docker Desktop management</li> </ul> <p>Design Philosophy: - Zero-knowledge startup: Works for first-time users with minimal configuration - Idempotent: Safe to run multiple times - won't duplicate work - Fail-fast: Exits immediately on critical errors with clear guidance - Progressive enhancement: Detects capabilities and offers upgrades</p> <p>Typical runtime: - First run with Ollama: 5-10 minutes (model download) - First run with Gemini: 30-60 seconds (Docker startup) - Subsequent runs: 10-20 seconds (containers already exist)</p>"},{"location":"STARTUP_WALKTHROUGH/#execution-flow-diagram","title":"Execution Flow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 START: ./start.sh                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 1: Environment Validation                       \u2502\n\u2502  \u2022 Check Python 3.12+                                 \u2502\n\u2502  \u2022 Check Docker installed                             \u2502\n\u2502  \u2022 Check Docker daemon ready                          \u2502\n\u2502  \u2022 Launch Docker if needed                            \u2502\n\u2502  \u2022 Check .env file exists                             \u2502\n\u2502  \u2022 Validate required vars (DISCORD_TOKEN, GEMINI_KEY) \u2502\n\u2502  Duration: 5-90 seconds (if launching Docker)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 2: Router Backend Selection (if not set)       \u2502\n\u2502  \u2022 Prompt user: Gemini or Ollama?                     \u2502\n\u2502  \u2022 Save choice to .env                                \u2502\n\u2502  Duration: 5-10 seconds (user interaction)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 3: Python Environment                           \u2502\n\u2502  \u2022 Create/activate virtual environment                \u2502\n\u2502  \u2022 Install dependencies if missing                    \u2502\n\u2502  Duration: 5-60 seconds (depends on cache)            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 4: Qdrant Vector Database                       \u2502\n\u2502  \u2022 Check if container exists                          \u2502\n\u2502  \u2022 Create or start container                          \u2502\n\u2502  \u2022 Wait for health check (30s max)                    \u2502\n\u2502  Duration: 5-15 seconds                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u25ba ROUTER_BACKEND=gemini?\n                    \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502             \u2502 Yes\n                    \u2502             \u25bc\n                    \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502      \u2502 Skip Ollama phases  \u2502\n                    \u2502      \u2502 Go to Phase 8       \u2502\n                    \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502             \u2502\n                    \u2502 No (Ollama) \u2502\n                    \u25bc             \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 5: Ollama System Assessment (Ollama only)       \u2502\n\u2502  \u2022 Check if already assessed (.ollama_assessed)       \u2502\n\u2502  \u2022 Run hardware detection                             \u2502\n\u2502  \u2022 Recommend model based on RAM/CPU/GPU               \u2502\n\u2502  \u2022 Update .env with OLLAMA_ROUTER_MODEL &amp; DOCKER_MEM  \u2502\n\u2502  Duration: 10-20 seconds (user interaction)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 6: Docker Memory Check (Ollama only)            \u2502\n\u2502  \u2022 Read OLLAMA_DOCKER_MEMORY from .env               \u2502\n\u2502  \u2022 Check Docker Desktop total memory                  \u2502\n\u2502  \u2022 If insufficient, prompt user:                      \u2502\n\u2502    1. Auto-increase (calls increase-docker-memory.sh) \u2502\n\u2502    2. Choose smaller model (exit, re-run)            \u2502\n\u2502    3. Continue anyway (risky)                         \u2502\n\u2502  Duration: 30-90 seconds (if increasing memory)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 7: Ollama Container &amp; Model (Ollama only)       \u2502\n\u2502  \u2022 Create/start Ollama container                      \u2502\n\u2502  \u2022 Wait for API ready (30s max)                       \u2502\n\u2502  \u2022 Check if model downloaded                          \u2502\n\u2502  \u2022 Pull model if missing (~4.7GB, 3-7 minutes)        \u2502\n\u2502  Duration: 10 seconds - 10 minutes (first run)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 8: Configuration Summary                        \u2502\n\u2502  \u2022 Display all settings                               \u2502\n\u2502  \u2022 Show API keys (truncated)                          \u2502\n\u2502  \u2022 Show backend choice                                \u2502\n\u2502  Duration: 1 second                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Phase 9: Start Bot                                    \u2502\n\u2502  \u2022 Set PYTHONPATH                                     \u2502\n\u2502  \u2022 Launch: python -m zetherion_ai                       \u2502\n\u2502  \u2022 Run until Ctrl+C                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"STARTUP_WALKTHROUGH/#phase-by-phase-breakdown","title":"Phase-by-Phase Breakdown","text":""},{"location":"STARTUP_WALKTHROUGH/#phase-1-environment-validation","title":"Phase 1: Environment Validation","text":"<p>Purpose: Ensure all prerequisites are met before continuing.</p>"},{"location":"STARTUP_WALKTHROUGH/#11-python-version-check-lines-43-65","title":"1.1 Python Version Check (Lines 43-65)","text":"<pre><code># Check for Python 3.12 or 3.13 explicitly\nif command_exists python3.12; then\n    PYTHON_CMD=\"python3.12\"\nelif command_exists python3.13; then\n    PYTHON_CMD=\"python3.13\"\nelif command_exists python3; then\n    # Fallback: check if python3 is &gt;= 3.12\n    PYTHON_VERSION=$(python3 --version | cut -d' ' -f2 | cut -d'.' -f1,2)\n    if [[ $(echo \"$PYTHON_VERSION &gt;= 3.12\" | bc -l) -eq 1 ]]; then\n        PYTHON_CMD=\"python3\"\n    else\n        exit 1  # Version too old\n    fi\nelse\n    exit 1  # Not found\nfi\n</code></pre> <p>Decision Logic: - Prefer explicit versions (python3.12, python3.13) - Fall back to generic python3 if &gt;= 3.12 - Exit with error if none found</p> <p>Why 3.12+? Zetherion AI uses modern Python features (type hints, pattern matching) requiring 3.12+.</p> <p>Error Example: <pre><code>\u2717 Python 3.12+ required, found 3.11\n\u2139 Install with: brew install python@3.12\n</code></pre></p>"},{"location":"STARTUP_WALKTHROUGH/#12-docker-check-auto-launch-lines-67-157","title":"1.2 Docker Check &amp; Auto-Launch (Lines 67-157)","text":"<p>Step 1: Check if Docker CLI exists <pre><code>if ! command_exists docker; then\n    print_error \"Docker not found\"\n    exit 1\nfi\n</code></pre></p> <p>Step 2: Check if daemon is ready <pre><code>if ! docker info &gt;/dev/null 2&gt;&amp;1; then\n    # Daemon not ready - check if Docker Desktop is starting\n</code></pre></p> <p>Step 3: Determine Docker state <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Is Docker daemon ready?                 \u2502\n\u2502 (docker info succeeds)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 Yes         \u2502 No\n           \u25bc             \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Continue \u2502   \u2502 Is Docker Desktop  \u2502\n    \u2502          \u2502   \u2502 process running?   \u2502\n    \u2502          \u2502   \u2502 (pgrep -x \"Docker\")\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 Yes               \u2502 No\n                \u25bc                   \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Starting...   \u2502   \u2502 Launch Docker   \u2502\n        \u2502 Wait 90s max  \u2502   \u2502 Desktop         \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 Wait for daemon \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Step 4: Launch Docker if needed <pre><code># Verify Docker.app exists\nif [ ! -d \"/Applications/Docker.app\" ]; then\n    print_error \"Docker Desktop not found at /Applications/Docker.app\"\n    exit 1\nfi\n\n# Launch\nopen -a Docker\n\n# Initial wait (5 seconds) for process to spawn\nsleep 5\n\n# Quick check loop (4 attempts x 5 seconds = 20s total)\nfor attempt in {1..4}; do\n    if docker info &gt;/dev/null 2&gt;&amp;1; then\n        print_success \"Docker daemon is ready\"\n        break 2  # Success! Exit both loops\n    fi\n    sleep 5\ndone\n</code></pre></p> <p>Two-Phase Wait Strategy:</p> <ol> <li>Quick Phase (20 seconds): 4 attempts with 5-second intervals</li> <li>Optimized for fast machines / warm starts</li> <li> <p>Most machines ready in 10-20 seconds</p> </li> <li> <p>Extended Phase (90 seconds): Continues if quick phase fails</p> </li> <li>Handles slow machines / cold starts</li> <li>Shows progress every 10 seconds</li> </ol> <p>Why this approach? - Fast machines don't wait unnecessarily - Slow machines get enough time - Clear progress feedback to user</p> <p>Timing Breakdown: <pre><code>Fast machine (Docker already warm):\n  Launch: 2s + Quick phase: 5-10s = 7-12 seconds total\n\nSlow machine (Docker cold start):\n  Launch: 2s + Quick phase: 20s + Extended: 30s = 52 seconds total\n\nVery slow machine:\n  Launch: 2s + Quick phase: 20s + Extended: 60s = 82 seconds total\n</code></pre></p>"},{"location":"STARTUP_WALKTHROUGH/#13-env-file-validation-lines-159-183","title":"1.3 .env File Validation (Lines 159-183)","text":"<p>Check file exists: <pre><code>if [ ! -f .env ]; then\n    print_error \".env file not found\"\n    print_info \"Copy .env.example to .env and add your API keys\"\n    exit 1\nfi\n</code></pre></p> <p>Validate required variables: <pre><code>source .env  # Load variables\n\nMISSING_VARS=()\nif [ -z \"$DISCORD_TOKEN\" ]; then\n    MISSING_VARS+=(\"DISCORD_TOKEN\")\nfi\nif [ -z \"$GEMINI_API_KEY\" ]; then\n    MISSING_VARS+=(\"GEMINI_API_KEY\")\nfi\n\nif [ ${#MISSING_VARS[@]} -gt 0 ]; then\n    print_error \"Missing required environment variables: ${MISSING_VARS[*]}\"\n    exit 1\nfi\n</code></pre></p> <p>Why source early? We need env vars for subsequent phases (router selection, Ollama config).</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-2-router-backend-selection","title":"Phase 2: Router Backend Selection","text":"<p>Purpose: One-time choice between Gemini (cloud) and Ollama (local) routing.</p>"},{"location":"STARTUP_WALKTHROUGH/#trigger-condition-line-186","title":"Trigger Condition (Line 186)","text":"<pre><code>if [ -z \"$ROUTER_BACKEND\" ]; then\n    # Not set in .env - ask user\n</code></pre> <p>When does this happen? - First run (fresh .env from .env.example) - User manually removed ROUTER_BACKEND from .env</p> <p>Interactive Prompt (Lines 187-225):</p> <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  Router Backend Selection\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nZetherion AI can use two different backends for message routing:\n\n  1. Gemini (Google) - Cloud-based, fast, minimal setup\n     \u2022 Uses your existing Gemini API key\n     \u2022 No additional downloads\n     \u2022 Recommended for cloud-based workflows\n\n  2. Ollama (Local) - Privacy-focused, runs on your machine\n     \u2022 No data sent to external APIs for routing\n     \u2022 ~5GB model download (first time only)\n     \u2022 Recommended for privacy-conscious users\n\nWhich backend would you like to use? (1=Gemini, 2=Ollama) [1]:\n</code></pre> <p>Decision Logic: <pre><code>case \"$REPLY\" in\n    2)\n        ROUTER_BACKEND=\"ollama\"\n        ;;\n    1|\"\")  # Default to Gemini if user just presses Enter\n        ROUTER_BACKEND=\"gemini\"\n        ;;\n    *)\n        print_warning \"Invalid selection, defaulting to Gemini\"\n        ROUTER_BACKEND=\"gemini\"\n        ;;\nesac\n\n# Save to .env for future runs\necho \"ROUTER_BACKEND=$ROUTER_BACKEND\" &gt;&gt; .env\n</code></pre></p> <p>Why persist to .env? - User only chooses once - Subsequent runs skip this prompt - Can be changed by editing .env manually</p> <p>Typical Timing: 5-10 seconds (user reading and choosing)</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-3-python-environment","title":"Phase 3: Python Environment","text":"<p>Purpose: Isolate dependencies in a virtual environment.</p>"},{"location":"STARTUP_WALKTHROUGH/#31-virtual-environment-creation-lines-228-238","title":"3.1 Virtual Environment Creation (Lines 228-238)","text":"<pre><code>if [ ! -d \".venv\" ]; then\n    print_warning \"Virtual environment not found, creating...\"\n    $PYTHON_CMD -m venv .venv\n    print_success \"Virtual environment created\"\nfi\n\nsource .venv/bin/activate\n</code></pre> <p>Why check first? Avoid re-creating existing venv (wastes time).</p> <p>Timing: 3-5 seconds to create, &lt;1 second to activate</p>"},{"location":"STARTUP_WALKTHROUGH/#32-dependency-installation-lines-240-250","title":"3.2 Dependency Installation (Lines 240-250)","text":"<pre><code>if ! python -c \"import discord\" 2&gt;/dev/null; then\n    # discord.py is a core dependency - if it's not installed, nothing is\n    print_warning \"Dependencies not installed, installing...\"\n    pip install --upgrade pip\n    pip install -r requirements.txt\n    pip install -e .\n    print_success \"Dependencies installed\"\nelse\n    print_success \"Dependencies already installed\"\nfi\n</code></pre> <p>Optimization: Test for a single package instead of all packages. - Fast path: &lt;1 second (dependencies already installed) - Slow path: 30-60 seconds (first time)</p> <p>Why <code>pip install -e .</code>? - Installs Zetherion AI in \"editable\" mode - Code changes apply immediately (no reinstall needed)</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-4-qdrant-vector-database","title":"Phase 4: Qdrant Vector Database","text":"<p>Purpose: Start vector database for conversation memory and embeddings.</p>"},{"location":"STARTUP_WALKTHROUGH/#container-lifecycle-lines-252-270","title":"Container Lifecycle (Lines 252-270)","text":"<p>State Machine: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Does container exist?               \u2502\n\u2502 (docker ps -a | grep zetherion_ai-    \u2502\n\u2502  qdrant)                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Yes          \u2502 No\n       \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Is it        \u2502  \u2502 Create new      \u2502\n\u2502 running?     \u2502  \u2502 container:      \u2502\n\u2502 (docker ps)  \u2502  \u2502 docker run -d   \u2502\n\u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518  \u2502   --name ...    \u2502\n   \u2502 Yes   \u2502 No   \u2502   -p 6333:6333  \u2502\n   \u25bc       \u25bc      \u2502   -v ...        \u2502\n\u250c\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2510   \u2502   qdrant/qdrant \u2502\n\u2502Skip\u2502  \u2502Start\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Create Command: <pre><code>docker run -d \\\n    --name zetherion_ai-qdrant \\\n    -p 6333:6333 \\\n    -v \"$(pwd)/qdrant_storage:/qdrant/storage\" \\\n    qdrant/qdrant:latest\n</code></pre></p> <p>Why volume mount? Persist vector embeddings across container restarts.</p>"},{"location":"STARTUP_WALKTHROUGH/#health-check-lines-272-287","title":"Health Check (Lines 272-287)","text":"<pre><code>MAX_RETRIES=30  # 30 seconds max\nRETRY_COUNT=0\n\nwhile [ $RETRY_COUNT -lt $MAX_RETRIES ]; do\n    if curl -s http://localhost:6333/healthz &gt;/dev/null 2&gt;&amp;1; then\n        print_success \"Qdrant is ready\"\n        break\n    fi\n    RETRY_COUNT=$((RETRY_COUNT + 1))\n    sleep 1\ndone\n</code></pre> <p>Why curl healthz? - Qdrant exposes a health endpoint - More reliable than assuming \"started = ready\"</p> <p>Typical Timing: - Container already running: 1-2 seconds - Container starting: 3-5 seconds - First time (pulling image): 10-30 seconds</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-5-ollama-system-assessment-ollama-only","title":"Phase 5: Ollama System Assessment (Ollama Only)","text":"<p>Skip Condition: If <code>ROUTER_BACKEND != \"ollama\"</code>, phases 5-7 are skipped entirely.</p> <p>Purpose: Detect hardware and recommend optimal Ollama model.</p>"},{"location":"STARTUP_WALKTHROUGH/#assessment-trigger-lines-293-294","title":"Assessment Trigger (Lines 293-294)","text":"<pre><code>if [ ! -f \".ollama_assessed\" ] || [ -z \"$OLLAMA_ROUTER_MODEL\" ]; then\n    # Run assessment\n</code></pre> <p>When does this run? 1. First time using Ollama (<code>.ollama_assessed</code> doesn't exist) 2. User removed model from .env (<code>OLLAMA_ROUTER_MODEL</code> empty)</p> <p>When is it skipped? - <code>.ollama_assessed</code> marker exists AND <code>OLLAMA_ROUTER_MODEL</code> is set - User can force re-assessment: <code>rm .ollama_assessed &amp;&amp; ./start.sh</code></p>"},{"location":"STARTUP_WALKTHROUGH/#hardware-detection-lines-295-326","title":"Hardware Detection (Lines 295-326)","text":"<p>Script: <code>scripts/assess-system.py</code></p> <p>What it detects: <pre><code>{\n    'platform': 'Darwin',        # macOS\n    'machine': 'arm64',          # Apple Silicon\n    'cpu_count': 10,             # Physical cores\n    'cpu_threads': 10,           # Logical cores\n    'ram_gb': 16.0,              # Total RAM\n    'available_ram_gb': 8.5,     # Available now\n    'disk_free_gb': 250.0,       # Free disk space\n    'gpu': {\n        'type': 'apple_silicon', # or 'nvidia', 'none'\n        'name': 'Apple Silicon',\n        'unified_memory': True\n    }\n}\n</code></pre></p> <p>Recommendation Logic: <pre><code>if ram &lt; 6:\n    return phi3:mini (5GB Docker, 2.3GB model)\nelif ram &lt; 12 and gpu == 'none':\n    return llama3.1:8b (8GB Docker, 4.7GB model)\nelif gpu in ['nvidia', 'apple_silicon']:\n    return qwen2.5:7b (10GB Docker, 4.7GB model)  # GPU accelerated\nelif ram &gt;= 16:\n    return qwen2.5:7b (10GB Docker, 4.7GB model)  # High RAM\nelse:\n    return llama3.1:8b (8GB Docker, 4.7GB model)  # Default\n</code></pre></p> <p>Output: <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nZetherion AI System Assessment\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\ud83d\udda5\ufe0f  HARDWARE DETECTED:\n  Platform: Darwin (arm64)\n  CPU: 10 cores (10 threads)\n  RAM: 16.0 GB total\n  RAM Available: 8.5 GB\n  Disk Space: 250.0 GB free\n  GPU: Apple Silicon\n\n\ud83e\udd16 RECOMMENDED MODEL:\n  Model: qwen2.5:7b\n  Download Size: 4.7 GB\n  RAM Required: 8 GB minimum\n  Docker Memory: 10 GB (automatically configured)\n  Expected Speed: ~2-3s (CPU), ~500ms (GPU)\n  Quality: Best\n  Description: Best reasoning quality for routing\n\n\ud83d\udca1 RECOMMENDATION REASON:\n  GPU detected! You can use higher quality models with fast inference.\n\nWould you like to use the recommended model? (Y/n):\n</code></pre></p> <p>User Interaction: <pre><code>read -p \"Would you like to use the recommended model? (Y/n): \" -r\n\nif [[ ! $REPLY =~ ^[Nn]$ ]]; then\n    # User accepted (or just pressed Enter)\n    $PYTHON_CMD scripts/assess-system.py --update-env\n\n    # This updates .env with:\n    # OLLAMA_ROUTER_MODEL=qwen2.5:7b\n    # OLLAMA_DOCKER_MEMORY=10\n\n    source .env  # Reload to get updated values\n    touch .ollama_assessed  # Mark as assessed\nfi\n</code></pre></p> <p>Timing: 5-15 seconds (hardware detection + user reading + interaction)</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-6-docker-memory-check-ollama-only","title":"Phase 6: Docker Memory Check (Ollama Only)","text":"<p>Purpose: Ensure Docker Desktop has enough RAM allocated for the selected model.</p>"},{"location":"STARTUP_WALKTHROUGH/#memory-requirement-calculation-lines-343-352","title":"Memory Requirement Calculation (Lines 343-352)","text":"<pre><code># Get required memory from .env (set by assess-system.py)\nOLLAMA_DOCKER_MEMORY=\"${OLLAMA_DOCKER_MEMORY:-8}\"  # Default 8GB\n\n# Get current Docker allocation\nDOCKER_TOTAL_MEMORY=$(docker info 2&gt;/dev/null | grep \"Total Memory\" | awk '{print $3}')\nDOCKER_MEMORY_GB=$(echo \"$DOCKER_TOTAL_MEMORY\" | sed 's/GiB//')\n\n# Compare\nREQUIRED_MEMORY=$OLLAMA_DOCKER_MEMORY\nif (( $(echo \"$DOCKER_MEMORY_GB &lt; $REQUIRED_MEMORY\" | bc -l) )); then\n    # Insufficient memory!\n</code></pre> <p>Example Scenario: <pre><code>Selected model: qwen2.5:7b\nOLLAMA_DOCKER_MEMORY: 10 (from assess-system.py)\nCurrent Docker allocation: 4GB\n\nProblem: 4GB &lt; 10GB \u274c\n</code></pre></p>"},{"location":"STARTUP_WALKTHROUGH/#user-prompt-lines-353-409","title":"User Prompt (Lines 353-409)","text":"<pre><code>\u26a0 Docker has only 4GB allocated\n\u26a0 Your selected model requires 10GB\n\nWhat would you like to do?\n  1. Automatically increase Docker memory to 10GB (recommended)\n  2. Choose a smaller model that fits current Docker memory\n  3. Continue anyway (may fail)\n\nEnter choice (1/2/3) [1]:\n</code></pre> <p>Decision Tree: <pre><code>                User Choice\n                     \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502            \u2502            \u2502\n        \u25bc            \u25bc            \u25bc\n    Option 1     Option 2     Option 3\n  Auto-increase  Smaller    Continue\n                  model      anyway\n        \u2502            \u2502            \u2502\n        \u25bc            \u25bc            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Call increase-\u2502 \u2502 rm   \u2502  \u2502 Continue \u2502\n\u2502 docker-memory.\u2502 \u2502 .olla\u2502  \u2502 (risky)  \u2502\n\u2502 sh --yes      \u2502 \u2502 ma_  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502               \u2502 \u2502 asses\u2502\n\u2502 \u2022 Backup JSON \u2502 \u2502 sed  \u2502\n\u2502 \u2022 Update      \u2502 \u2502      \u2502\n\u2502   memoryMiB   \u2502 \u2502 Exit \u2502\n\u2502 \u2022 Restart     \u2502 \u2502 0    \u2502\n\u2502   Docker      \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 \u2022 Wait for    \u2502\n\u2502   daemon      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Option 1: Auto-increase (Default)</p> <p>Script: <code>scripts/increase-docker-memory.sh --yes</code></p> <p>What it does: 1. Backup settings: <pre><code>cp ~/Library/Group\\ Containers/group.com.docker/settings.json \\\n   ~/Library/Group\\ Containers/group.com.docker/settings.json.backup.20260205_143022\n</code></pre></p> <ol> <li> <p>Update JSON: <pre><code>import json\n\nwith open(settings_file, 'r') as f:\n    settings = json.load(f)\n\nsettings['memoryMiB'] = 10240  # 10GB in MiB\n\nwith open(settings_file, 'w') as f:\n    json.dump(settings, f, indent=2)\n</code></pre></p> </li> <li> <p>Restart Docker: <pre><code># Try AppleScript first\nosascript -e 'quit app \"Docker\"'\n\n# Fallback to killall if osascript fails\nif ! osascript -e 'quit app \"Docker\"' 2&gt;/dev/null; then\n    killall Docker\nfi\n\n# Wait for full shutdown (up to 20s)\nfor i in {1..20}; do\n    ! pgrep -x \"Docker\" &amp;&amp; break\n    sleep 1\ndone\n\n# Launch\nopen -a Docker\n\n# Wait for daemon (up to 60s)\nfor i in {1..60}; do\n    docker info &gt;/dev/null 2&gt;&amp;1 &amp;&amp; break\n    sleep 1\ndone\n</code></pre></p> </li> </ol> <p>Why AppleScript first? - More graceful shutdown - Allows Docker to save state - Fallback to <code>killall</code> if it fails</p> <p>Why wait loops with timeouts? - Prevent infinite hangs - Provide clear error messages - Show progress to user</p> <p>Timing: - Stop Docker: 5-10 seconds - Start Docker: 30-60 seconds - Total: 35-70 seconds</p> <p>Option 2: Smaller Model <pre><code>rm -f .ollama_assessed  # Remove marker\nprint_info \"Please run ./start.sh again to choose a smaller model\"\nexit 0\n</code></pre> - Clean exit - Next run triggers assessment again - User can choose <code>phi3:mini</code> (5GB) instead</p> <p>Option 3: Continue Anyway <pre><code>print_warning \"Continuing with insufficient memory. Model may crash.\"\n# Script continues, but Ollama likely to OOM\n</code></pre> - Not recommended - Useful for testing or if user knows better - Model will likely crash with OOM errors</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-7-ollama-container-model-ollama-only","title":"Phase 7: Ollama Container &amp; Model (Ollama Only)","text":"<p>Purpose: Start Ollama container and download the selected model.</p>"},{"location":"STARTUP_WALKTHROUGH/#container-management-lines-411-431","title":"Container Management (Lines 411-431)","text":"<p>State Machine (same as Qdrant): <pre><code>Container exists? \u2192 Yes \u2192 Running? \u2192 Yes \u2192 Skip\n                           \u2502          No  \u2192 Start\n                    No \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Create\n</code></pre></p> <p>Create Command: <pre><code>docker run -d \\\n    --name zetherion_ai-ollama \\\n    --memory=\"${OLLAMA_DOCKER_MEMORY}g\" \\        # e.g., 10g\n    --memory-swap=\"${OLLAMA_DOCKER_MEMORY}g\" \\  # Prevent swap usage\n    -p 11434:11434 \\\n    -v \"$(pwd)/ollama_models:/root/.ollama\" \\   # Persist models\n    ollama/ollama:latest\n</code></pre></p> <p>Key Parameters: - <code>--memory</code>: Container RAM limit (matches Docker Desktop allocation) - <code>--memory-swap</code>: Same as memory (prevents swapping to disk) - <code>-v</code>: Persist downloaded models across container restarts</p> <p>Why volume for models? - Models are large (2-5GB each) - Re-downloading every time wastes time and bandwidth - Survives <code>docker rm zetherion_ai-ollama</code></p>"},{"location":"STARTUP_WALKTHROUGH/#api-health-check-lines-433-448","title":"API Health Check (Lines 433-448)","text":"<pre><code>MAX_RETRIES=30\nwhile [ $RETRY_COUNT -lt $MAX_RETRIES ]; do\n    if curl -s http://localhost:11434/api/tags &gt;/dev/null 2&gt;&amp;1; then\n        break  # Ready!\n    fi\n    sleep 1\ndone\n</code></pre> <p>Why <code>/api/tags</code>? - Ollama's \"list models\" endpoint - Returns empty list if no models, but proves API is responsive</p> <p>Timing: - Already running: 1-2 seconds - Starting: 3-5 seconds - First time (pulling image): 20-40 seconds</p>"},{"location":"STARTUP_WALKTHROUGH/#model-download-lines-450-467","title":"Model Download (Lines 450-467)","text":"<p>Check if model exists: <pre><code>OLLAMA_MODEL=\"${OLLAMA_ROUTER_MODEL:-llama3.1:8b}\"\n\nif docker exec zetherion_ai-ollama ollama list | grep -q \"$OLLAMA_MODEL\"; then\n    print_success \"Model '$OLLAMA_MODEL' already available\"\nelse\n    # Not downloaded yet\n</code></pre></p> <p>Pull model: <pre><code>print_warning \"Model '$OLLAMA_MODEL' not found, downloading (this may take several minutes)...\"\nprint_info \"Model size: ~4.7GB - please be patient...\"\n\nif docker exec zetherion_ai-ollama ollama pull \"$OLLAMA_MODEL\"; then\n    print_success \"Model '$OLLAMA_MODEL' downloaded successfully\"\nelse\n    print_error \"Failed to download model '$OLLAMA_MODEL'\"\n    print_warning \"Continuing anyway - the bot will fall back to Gemini if the model isn't available\"\nfi\n</code></pre></p> <p>Download Progress Example: <pre><code>pulling manifest\npulling 8934d96d3f08... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.7 GB\npulling 8c17c2ebb0ea... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.0 KB\npulling 7c23fb36d801... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.8 KB\npulling 2e0493f67d0c... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   59 B\npulling fa304d675061... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   91 B\npulling 42ba7f8a01dd... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  557 B\nverifying sha256 digest\nwriting manifest\nsuccess\n</code></pre></p> <p>Timing: - Model already downloaded: 1-2 seconds - Downloading 2.3GB (phi3): 2-4 minutes (fast connection) - Downloading 4.7GB (llama3.1, qwen2.5): 3-7 minutes</p> <p>Graceful Failure: - If download fails, script continues - Bot will use Gemini as fallback - User can manually pull later: <code>docker exec zetherion_ai-ollama ollama pull &lt;model&gt;</code></p>"},{"location":"STARTUP_WALKTHROUGH/#phase-8-configuration-summary","title":"Phase 8: Configuration Summary","text":"<p>Purpose: Show user exactly what configuration will be used.</p> <p>Display (Lines 473-488): <pre><code>\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  Starting Zetherion AI Bot\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n\u2139 Configuration Summary:\n  \u2022 Python: Python 3.12.1\n  \u2022 Discord Token: DISCORD_TOKEN_START...\n  \u2022 Gemini API: GEMINI_API_KEY_START...\n  \u2022 Anthropic API: ANTHROPIC_API_KEY_S...\n  \u2022 OpenAI API: OPENAI_API_KEY_START...\n  \u2022 Qdrant: http://localhost:6333\n  \u2022 Router Backend: ollama\n  \u2022 Ollama: http://localhost:11434 (Model: qwen2.5:7b)\n  \u2022 File Logging: true (Directory: logs)\n  \u2022 Allowed Users: 123456789,987654321\n</code></pre></p> <p>Security: API keys truncated to first 20 characters only.</p> <p>Timing: &lt;1 second</p>"},{"location":"STARTUP_WALKTHROUGH/#phase-9-start-bot","title":"Phase 9: Start Bot","text":"<p>Final Step (Lines 490-497):</p> <pre><code>print_success \"All checks passed! Starting bot...\"\necho \"\"\necho -e \"${GREEN}Press Ctrl+C to stop the bot${NC}\"\necho \"\"\n\n# Set PYTHONPATH to include src directory\nPYTHONPATH=\"${PWD}/src:${PYTHONPATH}\" python -m zetherion_ai\n</code></pre> <p>Why set PYTHONPATH? - Zetherion AI source is in <code>src/zetherion_ai/</code> - Running as module (<code>-m zetherion_ai</code>) requires it to be importable - Adding <code>src/</code> to path makes this work</p> <p>What happens next? 1. Python loads <code>src/zetherion_ai/__main__.py</code> 2. Initializes logging, settings, memory systems 3. Connects to Discord 4. Bot runs until Ctrl+C</p>"},{"location":"STARTUP_WALKTHROUGH/#decision-trees","title":"Decision Trees","text":""},{"location":"STARTUP_WALKTHROUGH/#docker-launch-decision-tree","title":"Docker Launch Decision Tree","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Does `docker` command exist?        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Yes          \u2502 No\n       \u25bc              \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502Continue\u2502    \u2502 ERROR: Install      \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 Docker Desktop      \u2502\n                \u2502 EXIT 1              \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Is daemon ready? (docker info)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Yes          \u2502 No\n       \u25bc              \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502Continue\u2502    \u2502 Is Docker.app running\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502 (pgrep -x \"Docker\")? \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                       \u2502 Yes      \u2502 No\n                       \u25bc          \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 Wait for  \u2502  \u2502 Launch Docker\u2502\n               \u2502 daemon    \u2502  \u2502 Desktop      \u2502\n               \u2502 90s max   \u2502  \u2502 (open -a)    \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Wait 5s for  \u2502\n                              \u2502 process spawn\u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Quick loop:  \u2502\n                              \u2502 4 x 5s = 20s \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                                     \u25bc\n                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                              \u2502 Extended:    \u2502\n                              \u2502 90s total    \u2502\n                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                     \u2502\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502 Ready?                  \u2502\n                        \u25bc                         \u25bc\n                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2502Continue \u2502            \u2502 ERROR: Timed\u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502 out after   \u2502\n                                          \u2502 90 seconds  \u2502\n                                          \u2502 EXIT 1      \u2502\n                                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"STARTUP_WALKTHROUGH/#router-backend-selection-tree","title":"Router Backend Selection Tree","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Is ROUTER_BACKEND set in .env?     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Yes            \u2502 No\n       \u25bc                \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 Use it \u2502      \u2502 Prompt user:     \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 1=Gemini 2=Ollama\u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u25bc                 \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 User: 2  \u2502      \u2502 User: 1  \u2502\n            \u2502 or       \u2502      \u2502 or &lt;Enter\u2502\n            \u2502 &lt;invalid&gt;\u2502      \u2502 &gt;        \u2502\n            \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                 \u2502                  \u2502\n                 \u25bc                  \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 ROUTER_      \u2502    \u2502 ROUTER_     \u2502\n         \u2502 BACKEND=     \u2502    \u2502 BACKEND=    \u2502\n         \u2502 ollama       \u2502    \u2502 gemini      \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502                   \u2502\n                \u2502                   \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502 echo ROUTER_BACKEND\u2502\n               \u2502 to .env            \u2502\n               \u2502 (persist choice)   \u2502\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"STARTUP_WALKTHROUGH/#ollama-memory-management-tree","title":"Ollama Memory Management Tree","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ROUTER_BACKEND == \"ollama\"?           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 Yes            \u2502 No\n       \u25bc                \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502Continue\u2502      \u2502 Skip all \u2502\n  \u2502 to     \u2502      \u2502 Ollama   \u2502\n  \u2502 Ollama \u2502      \u2502 phases   \u2502\n  \u2502 phases \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Check Docker memory vs requirement    \u2502\n\u2502 docker info | grep \"Total Memory\"     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                \u2502\n       \u2502 Sufficient     \u2502 Insufficient\n       \u25bc                \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502Continue\u2502      \u2502 Prompt user:         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 1. Auto-increase     \u2502\n                  \u2502 2. Smaller model     \u2502\n                  \u2502 3. Continue anyway   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n             \u2502             \u2502             \u2502\n             \u25bc             \u25bc             \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Call   \u2502   \u2502 rm      \u2502   \u2502Continue\u2502\n        \u2502increase\u2502   \u2502.ollama_ \u2502   \u2502(risky) \u2502\n        \u2502-docker-\u2502   \u2502assessed \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502memory  \u2502   \u2502         \u2502\n        \u2502.sh     \u2502   \u2502EXIT 0   \u2502\n        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 Backup JSON \u2502\n     \u2502 Update mem  \u2502\n     \u2502 Restart     \u2502\n     \u2502 Docker      \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Success?       \u2502\n    \u25bc                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Continue\u2502      \u2502 ERROR    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 Give user\u2502\n                \u2502 options  \u2502\n                \u2502 EXIT 1   \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"STARTUP_WALKTHROUGH/#timing-expectations","title":"Timing Expectations","text":""},{"location":"STARTUP_WALKTHROUGH/#first-run-ollama-no-containers","title":"First Run (Ollama, No Containers)","text":"Phase Task Time 1.1 Python check 1s 1.2 Docker launch (cold) 60s 1.3 .env validation 1s 2 Router selection (user) 10s 3 Venv + deps install 60s 4 Qdrant pull + start 30s 5 System assessment (user) 15s 6 Docker memory increase 60s 7 Ollama pull + start 40s 7 Model download (4.7GB) 300s 8 Summary 1s TOTAL ~9 minutes"},{"location":"STARTUP_WALKTHROUGH/#first-run-gemini-no-containers","title":"First Run (Gemini, No Containers)","text":"Phase Task Time 1-2 Validation + router 72s 3 Venv + deps 60s 4 Qdrant pull + start 30s 5-7 (Skipped - Gemini) 0s 8 Summary 1s TOTAL ~3 minutes"},{"location":"STARTUP_WALKTHROUGH/#subsequent-runs-warm","title":"Subsequent Runs (Warm)","text":"Phase Task Time 1 Validation (Docker warm) 5s 2 (Skipped - already set) 0s 3 Venv activate 2s 4 Qdrant start 3s 5 (Skipped - assessed) 0s 6 (Skipped - sufficient) 2s 7 Ollama start 5s 8 Summary 1s TOTAL ~18 seconds"},{"location":"STARTUP_WALKTHROUGH/#cold-docker-start-worst-case","title":"Cold Docker Start (Worst Case)","text":"Phase Task Time 1.2 Docker launch 90s 4 Qdrant first start 10s 7 Ollama first start 10s TOTAL ADDED +110s"},{"location":"STARTUP_WALKTHROUGH/#error-handling","title":"Error Handling","text":""},{"location":"STARTUP_WALKTHROUGH/#exit-codes","title":"Exit Codes","text":"Code Meaning Example Trigger 0 Success Normal completion 1 Fatal error Missing dependency, .env invalid, timeout"},{"location":"STARTUP_WALKTHROUGH/#error-categories","title":"Error Categories","text":""},{"location":"STARTUP_WALKTHROUGH/#1-missing-prerequisites","title":"1. Missing Prerequisites","text":"<p><pre><code>\u2717 Python 3.12+ required, found 3.11\n\u2139 Install with: brew install python@3.12\n</code></pre> Action: Install suggested package, re-run</p>"},{"location":"STARTUP_WALKTHROUGH/#2-configuration-errors","title":"2. Configuration Errors","text":"<p><pre><code>\u2717 Missing required environment variables: DISCORD_TOKEN GEMINI_API_KEY\n\u2139 Please add them to your .env file\n</code></pre> Action: Edit .env, add missing keys, re-run</p>"},{"location":"STARTUP_WALKTHROUGH/#3-docker-errors","title":"3. Docker Errors","text":"<p><pre><code>\u2717 Docker daemon did not become ready after 90 seconds\n\u2139 Check Docker Desktop status in menu bar and try again\n\u2139 You may need to restart Docker Desktop manually\n</code></pre> Action: - Check Docker Desktop icon in menu bar - Look for error messages in Docker Desktop GUI - Try manually restarting: Docker menu \u2192 Restart - Check Console.app for Docker crashes</p>"},{"location":"STARTUP_WALKTHROUGH/#4-container-errors","title":"4. Container Errors","text":"<p><pre><code>\u2717 Qdrant failed to start\n</code></pre> Action: - Check if port already in use: <code>lsof -i :6333</code> - Check container logs: <code>docker logs zetherion_ai-qdrant</code> - Remove and retry: <code>docker rm -f zetherion_ai-qdrant &amp;&amp; ./start.sh</code></p>"},{"location":"STARTUP_WALKTHROUGH/#5-model-download-errors","title":"5. Model Download Errors","text":"<p><pre><code>\u2717 Failed to download model 'qwen2.5:7b'\n\u2139 You can manually pull it later with: docker exec zetherion_ai-ollama ollama pull qwen2.5:7b\n\u26a0 Continuing anyway - the bot will fall back to Gemini if the model isn't available\n</code></pre> Action: - Script continues (non-fatal) - Bot uses Gemini for routing instead - Manually pull later when network is better - Or choose smaller model: <code>rm .ollama_assessed &amp;&amp; ./start.sh</code></p>"},{"location":"STARTUP_WALKTHROUGH/#environment-variables","title":"Environment Variables","text":""},{"location":"STARTUP_WALKTHROUGH/#required-must-be-in-env","title":"Required (Must be in .env)","text":"Variable Example Purpose <code>DISCORD_TOKEN</code> <code>MTA...</code> Discord bot authentication <code>GEMINI_API_KEY</code> <code>AIza...</code> Google Gemini API (embeddings, routing)"},{"location":"STARTUP_WALKTHROUGH/#optional-configuration","title":"Optional Configuration","text":"Variable Default Purpose <code>ROUTER_BACKEND</code> <code>gemini</code> Router: <code>gemini</code> or <code>ollama</code> <code>ANTHROPIC_API_KEY</code> <code>None</code> Claude for complex tasks <code>OPENAI_API_KEY</code> <code>None</code> GPT for complex tasks <code>ALLOWED_USER_IDS</code> <code>[]</code> Discord user ID allowlist <code>LOG_TO_FILE</code> <code>true</code> Enable file logging <code>LOG_DIRECTORY</code> <code>logs</code> Log file location"},{"location":"STARTUP_WALKTHROUGH/#ollama-specific","title":"Ollama-Specific","text":"Variable Default Set By Purpose <code>OLLAMA_HOST</code> <code>ollama</code> <code>start.sh</code> Ollama container host <code>OLLAMA_PORT</code> <code>11434</code> Manual Ollama API port <code>OLLAMA_ROUTER_MODEL</code> <code>llama3.1:8b</code> <code>assess-system.py</code> Model to use <code>OLLAMA_DOCKER_MEMORY</code> <code>8</code> <code>assess-system.py</code> Docker RAM (GB) <code>OLLAMA_TIMEOUT</code> <code>30</code> Manual API timeout (seconds)"},{"location":"STARTUP_WALKTHROUGH/#auto-configured","title":"Auto-Configured","text":"Variable When Set By Whom <code>ROUTER_BACKEND</code> Phase 2 (first run) User prompt \u2192 <code>start.sh</code> <code>OLLAMA_HOST</code> Phase 7 (if <code>ollama</code>) <code>start.sh</code> (sets to <code>localhost</code>) <code>OLLAMA_ROUTER_MODEL</code> Phase 5 (if accepted) <code>assess-system.py --update-env</code> <code>OLLAMA_DOCKER_MEMORY</code> Phase 5 (if accepted) <code>assess-system.py --update-env</code>"},{"location":"STARTUP_WALKTHROUGH/#appendix-script-structure","title":"Appendix: Script Structure","text":""},{"location":"STARTUP_WALKTHROUGH/#helper-functions-lines-11-39","title":"Helper Functions (Lines 11-39)","text":"<pre><code>print_success() {\n    echo -e \"${GREEN}\u2713${NC} $1\"\n}\n\nprint_error() {\n    echo -e \"${RED}\u2717${NC} $1\"\n}\n\nprint_warning() {\n    echo -e \"${YELLOW}\u26a0${NC} $1\"\n}\n\nprint_info() {\n    echo -e \"${BLUE}\u2139${NC} $1\"\n}\n\nprint_header() {\n    echo -e \"${BLUE}\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550${NC}\"\n    echo -e \"${BLUE}  $1${NC}\"\n    echo -e \"${BLUE}\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550${NC}\"\n}\n\ncommand_exists() {\n    command -v \"$1\" &gt;/dev/null 2&gt;&amp;1\n}\n</code></pre> <p>Why standardize output? - Consistent UX - Colored output for clarity - Easy to scan for errors (red \u2717) vs success (green \u2713)</p>"},{"location":"STARTUP_WALKTHROUGH/#error-handling-pattern","title":"Error Handling Pattern","text":"<pre><code>set -e  # Exit on any error\n\n# For non-fatal errors:\nif ! some_command; then\n    print_warning \"Command failed, continuing...\"\nfi\n\n# For fatal errors:\nif ! critical_command; then\n    print_error \"Critical failure\"\n    print_info \"How to fix: ...\"\n    exit 1\nfi\n</code></pre>"},{"location":"STARTUP_WALKTHROUGH/#summary","title":"Summary","text":"<p>The startup script handles: - \u2705 Environment validation (Python, Docker, .env) - \u2705 User choices (router backend, model selection) - \u2705 Dependency management (venv, pip packages) - \u2705 Container orchestration (Qdrant, Ollama) - \u2705 Docker memory automation (detect, prompt, increase) - \u2705 Model downloads (Ollama pull) - \u2705 Clear feedback (colored output, progress messages) - \u2705 Graceful failures (timeouts, fallbacks, suggestions)</p> <p>First run: 3-9 minutes (depending on backend and network)</p> <p>Subsequent runs: 10-20 seconds (all containers already exist)</p> <p>User interaction points: 1. Router backend choice (first run) 2. Model recommendation acceptance (Ollama first run) 3. Docker memory increase approval (if needed)</p> <p>Exit points: - Fatal: Missing Python, Docker, .env keys - Soft: User chooses smaller model (can re-run immediately)</p>"},{"location":"TESTING-DEPLOYMENT/","title":"Testing &amp; Deployment Validation Guide","text":"<p>Complete testing guide for validating Zetherion AI deployment across all platforms.</p>"},{"location":"TESTING-DEPLOYMENT/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Pre-Testing Checklist</li> <li>Phase-by-Phase Validation</li> <li>Platform-Specific Testing</li> <li>Container Validation</li> <li>Hardware Assessment Testing</li> <li>Integration Testing</li> <li>Performance Benchmarking</li> <li>Security Testing</li> <li>Troubleshooting Test Failures</li> </ul>"},{"location":"TESTING-DEPLOYMENT/#overview","title":"Overview","text":"<p>This guide provides comprehensive testing procedures to validate the fully automated Docker deployment of Zetherion AI. All tests should pass before considering a deployment ready for production use.</p> <p>Testing Environments: - Windows 10/11 with PowerShell 5.1+ - macOS 10.15+ (Catalina or later) - Linux (Ubuntu 20.04+, Debian 11+, Fedora 35+)</p> <p>Required Tools: - Docker Desktop (or Docker Engine on Linux) - Git - curl or Invoke-WebRequest - jq (optional, for JSON parsing)</p>"},{"location":"TESTING-DEPLOYMENT/#pre-testing-checklist","title":"Pre-Testing Checklist","text":"<p>Before running any tests, verify:</p>"},{"location":"TESTING-DEPLOYMENT/#1-clean-environment","title":"1. Clean Environment","text":"<p>Windows: <pre><code># Stop any running containers\n.\\stop.ps1\n\n# Remove all containers and volumes (keeps config)\n.\\cleanup.ps1 -KeepConfig\n\n# Verify clean state\ndocker ps -a | Select-String \"zetherion\"  # Should return nothing\ndocker volume ls | Select-String \"zetherion\"  # Should return nothing\n</code></pre></p> <p>Unix (macOS/Linux): <pre><code># Stop any running containers\n./stop.sh\n\n# Remove all containers and volumes (keeps config)\n./cleanup.sh --keep-config\n\n# Verify clean state\ndocker ps -a | grep zetherion  # Should return nothing\ndocker volume ls | grep zetherion  # Should return nothing\n</code></pre></p>"},{"location":"TESTING-DEPLOYMENT/#2-prerequisites-verified","title":"2. Prerequisites Verified","text":"<ul> <li>\u2705 Docker Desktop installed and running</li> <li>\u2705 Git installed</li> <li>\u2705 Sufficient disk space (20GB+)</li> <li>\u2705 Internet connection active</li> <li>\u2705 API keys available (Discord, Gemini)</li> </ul>"},{"location":"TESTING-DEPLOYMENT/#3-test-configuration-ready","title":"3. Test Configuration Ready","text":"<p>Create a test <code>.env</code> file or have API keys ready for interactive setup:</p> <pre><code># Required for testing\nDISCORD_TOKEN=your_test_discord_token\nGEMINI_API_KEY=your_gemini_api_key\n\n# Optional (can be skipped in tests)\nANTHROPIC_API_KEY=\nOPENAI_API_KEY=\n\n# Test configuration\nROUTER_BACKEND=gemini  # Use Gemini for faster initial tests\n</code></pre>"},{"location":"TESTING-DEPLOYMENT/#phase-by-phase-validation","title":"Phase-by-Phase Validation","text":""},{"location":"TESTING-DEPLOYMENT/#phase-1-prerequisites-check","title":"Phase 1: Prerequisites Check","text":"<p>Test Objective: Verify prerequisite validation and auto-install prompts work correctly.</p>"},{"location":"TESTING-DEPLOYMENT/#test-11-docker-detection-already-installed","title":"Test 1.1: Docker Detection (Already Installed)","text":"<p>Windows: <pre><code>.\\start.ps1\n</code></pre></p> <p>Expected Output: <pre><code>[STEP 1/7] Checking prerequisites...\n[OK] Docker Desktop is running\n[OK] Git is installed\n[OK] Disk space: XXX GB available\n</code></pre></p> <p>Validation: - \u2705 Docker detected automatically - \u2705 Git detected automatically - \u2705 Disk space warning if &lt;20GB - \u2705 No errors, proceeds to Phase 2</p>"},{"location":"TESTING-DEPLOYMENT/#test-12-docker-not-running-manual-test","title":"Test 1.2: Docker Not Running (Manual Test)","text":"<p>Setup: 1. Stop Docker Desktop manually 2. Run start script</p> <p>Expected Behavior: - Script detects Docker not running - Attempts to start Docker Desktop - Waits up to 60 seconds for Docker daemon - Proceeds once Docker is ready OR errors with helpful message</p> <p>Validation: - \u2705 Docker Desktop launched automatically - \u2705 Script waits for daemon to be ready - \u2705 Proceeds after Docker starts</p>"},{"location":"TESTING-DEPLOYMENT/#test-13-low-disk-space-warning","title":"Test 1.3: Low Disk Space Warning","text":"<p>Manual Test (requires machine with &lt;20GB free):</p> <p>Expected Output: <pre><code>[WARNING] Low disk space: 15GB available\n[WARNING] Recommended: 20GB+ for optimal performance\n[INFO] Continue anyway? (y/N):\n</code></pre></p> <p>Validation: - \u2705 Warning displayed if &lt;20GB - \u2705 User can choose to continue or abort - \u2705 Script proceeds if user confirms</p>"},{"location":"TESTING-DEPLOYMENT/#phase-2-hardware-assessment","title":"Phase 2: Hardware Assessment","text":"<p>Test Objective: Verify hardware detection accurately identifies system resources.</p>"},{"location":"TESTING-DEPLOYMENT/#test-21-hardware-assessment-execution","title":"Test 2.1: Hardware Assessment Execution","text":"<p>Windows: <pre><code># Run start script and observe Phase 2 output\n.\\start.ps1\n</code></pre></p> <p>Expected Output: <pre><code>[STEP 2/7] Assessing hardware...\n[INFO] Building hardware assessment container...\n[INFO] CPU: [Detected CPU Model] (X cores, Y threads)\n[INFO] RAM: XGB total, YGB available\n[INFO] GPU: [Detected GPU] or \"None (CPU-only mode)\"\n[OK] Hardware assessment complete\n\nRecommended Ollama Model:\n  Model: llama3.1:8b (or appropriate for hardware)\n  Size: 4.7 GB download\n  Quality: High quality, comparable to cloud models\n  Speed: Fast (with GPU) or Medium (CPU-only)\n  Reason: [Explanation based on detected hardware]\n</code></pre></p> <p>Validation: - \u2705 CPU cores/threads detected correctly - \u2705 RAM total and available detected correctly - \u2705 GPU detected (if present) or \"None\" reported - \u2705 Model recommendation matches hardware capability - \u2705 No errors during assessment</p>"},{"location":"TESTING-DEPLOYMENT/#test-22-hardware-assessment-accuracy","title":"Test 2.2: Hardware Assessment Accuracy","text":"<p>Manual Verification:</p> <p>Windows: <pre><code># Check actual hardware\nGet-WmiObject Win32_Processor | Select-Object Name, NumberOfCores, NumberOfLogicalProcessors\nGet-WmiObject Win32_ComputerSystem | Select-Object @{Name=\"RAM (GB)\";Expression={[math]::Round($_.TotalPhysicalMemory/1GB,2)}}\nGet-WmiObject Win32_VideoController | Select-Object Name, AdapterRAM\n</code></pre></p> <p>macOS: <pre><code># Check actual hardware\nsysctl -n machdep.cpu.brand_string\nsysctl -n hw.ncpu\nsysctl -n hw.memsize | awk '{print $1/1024/1024/1024 \" GB\"}'\nsystem_profiler SPDisplaysDataType | grep \"Chipset Model\"\n</code></pre></p> <p>Linux: <pre><code># Check actual hardware\nlscpu | grep \"Model name\"\nlscpu | grep \"^CPU(s):\"\nfree -h | grep Mem\nlspci | grep -i vga\n</code></pre></p> <p>Compare with script output: - \u2705 CPU model matches - \u2705 Core/thread count matches - \u2705 RAM amount within 10% tolerance - \u2705 GPU model matches (if present)</p>"},{"location":"TESTING-DEPLOYMENT/#test-23-model-recommendation-logic","title":"Test 2.3: Model Recommendation Logic","text":"<p>Expected Recommendations by Hardware:</p> Hardware Expected Model Size &lt;8GB RAM, no GPU gemini (cloud) 0GB 8-16GB RAM, no GPU llama3.1:8b 4.7GB 16-32GB RAM, no GPU qwen2.5:14b 9.0GB 8GB VRAM GPU llama3.1:8b 4.7GB 12GB+ VRAM GPU qwen2.5:14b 9.0GB 24GB+ VRAM GPU qwen2.5:32b 18GB <p>Validation: - \u2705 Recommendation matches table above - \u2705 Reason provided explains the recommendation - \u2705 Alternative models listed</p>"},{"location":"TESTING-DEPLOYMENT/#phase-3-configuration-setup","title":"Phase 3: Configuration Setup","text":"<p>Test Objective: Verify interactive .env generation works correctly.</p>"},{"location":"TESTING-DEPLOYMENT/#test-31-first-run-interactive-setup","title":"Test 3.1: First-Run Interactive Setup","text":"<p>Prerequisite: No <code>.env</code> file exists</p> <p>Windows: <pre><code># Remove .env if it exists\nRemove-Item .env -ErrorAction SilentlyContinue\n\n# Run start script\n.\\start.ps1\n</code></pre></p> <p>Expected Prompts: <pre><code>[STEP 3/7] Configuration setup...\n[INFO] No .env file found. Let's set it up!\n\nDiscord Bot Token (required):\n&gt; [User enters token]\n\nGemini API Key (required for embeddings):\n&gt; [User enters key]\n\nAnthropic API Key (optional, press Enter to skip):\n&gt; [User enters key or presses Enter]\n\nOpenAI API Key (optional, press Enter to skip):\n&gt; [User presses Enter]\n\nRouter Backend:\n  1. Gemini (cloud-based, fast, minimal resources)\n  2. Ollama (local, private, ~5GB download)\nChoose [1/2]: 2\n\n[INFO] Based on your hardware (16GB RAM, RTX 3060), we recommend:\n  Recommended: llama3.1:8b\nUse recommended model 'llama3.1:8b'? (Y/n): Y\n\n[OK] Configuration saved to .env\n</code></pre></p> <p>Validation: - \u2705 All required prompts displayed - \u2705 Optional prompts allow Enter to skip - \u2705 Router backend selection works - \u2705 Model recommendation shown (if Ollama selected) - \u2705 <code>.env</code> file created with correct values - \u2705 API keys not displayed in logs</p>"},{"location":"TESTING-DEPLOYMENT/#test-32-existing-configuration-skip-setup","title":"Test 3.2: Existing Configuration (Skip Setup)","text":"<p>Prerequisite: <code>.env</code> file exists</p> <p>Windows: <pre><code># .env already exists from previous test\n.\\start.ps1\n</code></pre></p> <p>Expected Output: <pre><code>[STEP 3/7] Configuration setup...\n[OK] Configuration file found\n</code></pre></p> <p>Validation: - \u2705 No prompts displayed - \u2705 Existing <code>.env</code> used - \u2705 Script proceeds to Phase 4</p>"},{"location":"TESTING-DEPLOYMENT/#test-33-invalid-api-key-format","title":"Test 3.3: Invalid API Key Format","text":"<p>Manual Test:</p> <ol> <li>Modify interactive setup script to enter invalid key</li> <li>Enter malformed Discord token (e.g., \"invalid_token\")</li> </ol> <p>Expected Behavior: <pre><code>[ERROR] Invalid Discord token format\n[ERROR] Discord tokens should start with \"MT\" or \"MQ\" and contain at least 50 characters\n[INFO] Discord Bot Token (required):\n&gt; [Prompts again]\n</code></pre></p> <p>Validation: - \u2705 Invalid format detected - \u2705 Helpful error message shown - \u2705 Re-prompts for correct input</p>"},{"location":"TESTING-DEPLOYMENT/#phase-4-docker-build-deploy","title":"Phase 4: Docker Build &amp; Deploy","text":"<p>Test Objective: Verify distroless images build correctly and containers start.</p>"},{"location":"TESTING-DEPLOYMENT/#test-41-distroless-image-build","title":"Test 4.1: Distroless Image Build","text":"<p>Windows: <pre><code># Start script initiates build\n.\\start.ps1\n</code></pre></p> <p>Expected Output: <pre><code>[STEP 4/7] Building Docker images...\n[INFO] Building zetherion-ai-bot...\n[+] Building 45.2s (18/18) FINISHED\n[INFO] Building zetherion-ai-skills...\n[+] Building 38.7s (16/16) FINISHED\n[OK] Images built successfully\n</code></pre></p> <p>Validation: - \u2705 Multi-stage build completes without errors - \u2705 Builder stage installs dependencies - \u2705 Runtime stage uses distroless base - \u2705 Both bot and skills images build successfully</p>"},{"location":"TESTING-DEPLOYMENT/#test-42-container-startup","title":"Test 4.2: Container Startup","text":"<p>Expected Output: <pre><code>[INFO] Starting Docker containers...\n[INFO] Starting docker-compose...\nCreating network \"personalbot_default\" ... done\nCreating volume \"personalbot_qdrant_storage\" ... done\nCreating volume \"personalbot_ollama_models\" ... done\nCreating zetherion-ai-qdrant ... done\nCreating zetherion-ai-ollama ... done\nCreating zetherion-ai-skills ... done\nCreating zetherion-ai-bot ... done\n[OK] Containers started\n</code></pre></p> <p>Validation: - \u2705 All containers created - \u2705 Networks created - \u2705 Volumes created - \u2705 No error messages</p>"},{"location":"TESTING-DEPLOYMENT/#test-43-health-check-wait","title":"Test 4.3: Health Check Wait","text":"<p>Expected Output: <pre><code>[INFO] Waiting for services to become healthy...\n[INFO] Waiting for zetherion-ai-qdrant... (0/120s)\n[INFO] Waiting for zetherion-ai-qdrant... (5/120s)\n[OK] zetherion-ai-qdrant is healthy\n[INFO] Waiting for zetherion-ai-skills... (0/120s)\n[OK] zetherion-ai-skills is healthy\n[INFO] Waiting for zetherion-ai-bot... (0/120s)\n[OK] zetherion-ai-bot is healthy\n</code></pre></p> <p>Validation: - \u2705 Health checks executed - \u2705 All services become healthy within 120 seconds - \u2705 Progress updates shown - \u2705 No timeouts</p>"},{"location":"TESTING-DEPLOYMENT/#phase-5-model-download-ollama-only","title":"Phase 5: Model Download (Ollama Only)","text":"<p>Test Objective: Verify Ollama model downloads correctly (if Ollama backend selected).</p>"},{"location":"TESTING-DEPLOYMENT/#test-51-first-time-model-download","title":"Test 5.1: First-Time Model Download","text":"<p>Prerequisite: Ollama backend selected, model not yet downloaded</p> <p>Expected Output: <pre><code>[STEP 5/7] Downloading Ollama model (first time only)...\n[INFO] Checking if model 'llama3.1:8b' exists...\n[INFO] Model not found. Downloading llama3.1:8b (4.7GB)...\n[INFO] This may take 5-7 minutes depending on internet speed...\npulling manifest\npulling 8eeb52dfb3bb... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.7 GB\npulling 73b313b5552d... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.5 KB\npulling 0ba8f0e314b4... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  12 KB\npulling 56bb8bd477a5... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   96 B\npulling 1a4c3c319823... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  485 B\nverifying sha256 digest\nwriting manifest\nsuccess\n[OK] Model 'llama3.1:8b' downloaded successfully\n</code></pre></p> <p>Validation: - \u2705 Model download initiated - \u2705 Progress bar displayed - \u2705 Download completes successfully - \u2705 Model verified</p>"},{"location":"TESTING-DEPLOYMENT/#test-52-existing-model-skip-download","title":"Test 5.2: Existing Model (Skip Download)","text":"<p>Prerequisite: Model already downloaded from previous test</p> <p>Windows: <pre><code># Stop and restart to test existing model check\n.\\stop.ps1\n.\\start.ps1\n</code></pre></p> <p>Expected Output: <pre><code>[STEP 5/7] Checking Ollama model...\n[INFO] Checking if model 'llama3.1:8b' exists...\n[OK] Model 'llama3.1:8b' already downloaded\n</code></pre></p> <p>Validation: - \u2705 Existing model detected - \u2705 Download skipped - \u2705 Fast startup (~30 seconds)</p>"},{"location":"TESTING-DEPLOYMENT/#test-53-model-download-failure-recovery","title":"Test 5.3: Model Download Failure Recovery","text":"<p>Manual Test (simulate network failure):</p> <ol> <li>Disconnect internet during model download</li> <li>Observe error handling</li> </ol> <p>Expected Behavior: <pre><code>[ERROR] Failed to download model 'llama3.1:8b'\n[ERROR] Error: connection timeout\n[INFO] Please check your internet connection and try again\n[INFO] To retry manually:\n  docker exec zetherion-ai-ollama ollama pull llama3.1:8b\n</code></pre></p> <p>Validation: - \u2705 Error caught gracefully - \u2705 Helpful error message - \u2705 Manual retry instructions provided</p>"},{"location":"TESTING-DEPLOYMENT/#phase-6-verification","title":"Phase 6: Verification","text":"<p>Test Objective: Verify all services are running and accessible.</p>"},{"location":"TESTING-DEPLOYMENT/#test-61-qdrant-health-check","title":"Test 6.1: Qdrant Health Check","text":"<p>Expected Output: <pre><code>[STEP 6/7] Verifying deployment...\n[INFO] Testing Qdrant connection...\n[OK] Qdrant is healthy (http://localhost:6333/healthz)\n</code></pre></p> <p>Manual Verification: <pre><code># Windows\nInvoke-WebRequest http://localhost:6333/healthz\n\n# Unix\ncurl http://localhost:6333/healthz\n</code></pre></p> <p>Expected Response: <pre><code>{\n  \"title\": \"Qdrant - Vector Search Engine\",\n  \"version\": \"v1.7.4\"\n}\n</code></pre></p> <p>Validation: - \u2705 Qdrant responds to health check - \u2705 HTTP 200 status code - \u2705 Version info returned</p>"},{"location":"TESTING-DEPLOYMENT/#test-62-ollama-health-check-if-enabled","title":"Test 6.2: Ollama Health Check (If Enabled)","text":"<p>Expected Output: <pre><code>[INFO] Testing Ollama connection...\n[OK] Ollama is healthy (http://localhost:11434/api/tags)\n[OK] Model 'llama3.1:8b' loaded\n</code></pre></p> <p>Manual Verification: <pre><code># Windows\nInvoke-WebRequest http://localhost:11434/api/tags | ConvertFrom-Json\n\n# Unix\ncurl http://localhost:11434/api/tags | jq\n</code></pre></p> <p>Expected Response: <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"llama3.1:8b\",\n      \"modified_at\": \"2026-02-07T10:30:00Z\",\n      \"size\": 4661211648\n    }\n  ]\n}\n</code></pre></p> <p>Validation: - \u2705 Ollama responds to API request - \u2705 Model listed in response - \u2705 Model size matches expected</p>"},{"location":"TESTING-DEPLOYMENT/#test-63-container-status-display","title":"Test 6.3: Container Status Display","text":"<p>Expected Output: <pre><code>[INFO] Container status:\nzetherion-ai-qdrant   Up 1 minute (healthy)\nzetherion-ai-ollama   Up 1 minute (healthy)\nzetherion-ai-skills   Up 1 minute (healthy)\nzetherion-ai-bot      Up 1 minute (healthy)\n</code></pre></p> <p>Validation: - \u2705 All 4 containers listed - \u2705 All show \"Up\" status - \u2705 All show \"(healthy)\" status - \u2705 Uptime displayed</p>"},{"location":"TESTING-DEPLOYMENT/#phase-7-success","title":"Phase 7: Success","text":"<p>Test Objective: Verify success message and next steps displayed.</p> <p>Expected Output: <pre><code>============================================================\n  \u2713 Zetherion AI is now running!\n============================================================\n\nNext Steps:\n  1. View logs:        docker-compose logs -f\n  2. Check status:     .\\status.ps1 (or ./status.sh)\n  3. Stop bot:         .\\stop.ps1 (or ./stop.sh)\n\n  4. Invite bot to Discord:\n     https://discord.com/developers/applications\n\nDeployment successful!\n</code></pre></p> <p>Validation: - \u2705 Success banner displayed - \u2705 Next steps clearly listed - \u2705 Platform-appropriate commands shown - \u2705 Discord invite link provided</p>"},{"location":"TESTING-DEPLOYMENT/#platform-specific-testing","title":"Platform-Specific Testing","text":""},{"location":"TESTING-DEPLOYMENT/#windows-testing","title":"Windows Testing","text":""},{"location":"TESTING-DEPLOYMENT/#test-w1-powershell-execution-policy","title":"Test W1: PowerShell Execution Policy","text":"<p>Test with Restricted Policy: <pre><code># Set restricted policy (simulates fresh Windows)\nSet-ExecutionPolicy -Scope Process -ExecutionPolicy Restricted\n\n# Try to run start script\n.\\start.ps1\n</code></pre></p> <p>Expected Error: <pre><code>.\\start.ps1 : File .\\start.ps1 cannot be loaded because running scripts is disabled on this system.\n</code></pre></p> <p>Resolution Test: <pre><code># Script should provide helpful message\nSet-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass\n.\\start.ps1\n</code></pre></p> <p>Validation: - \u2705 Error message clear and helpful - \u2705 Documentation includes resolution steps - \u2705 Script runs after policy change</p>"},{"location":"TESTING-DEPLOYMENT/#test-w2-administrator-privileges","title":"Test W2: Administrator Privileges","text":"<p>Test without Admin: <pre><code># Run PowerShell as regular user (not admin)\n.\\start.ps1\n</code></pre></p> <p>Expected Error: <pre><code>[ERROR] This script requires Administrator privileges\n[ERROR] Right-click PowerShell and select \"Run as Administrator\"\n</code></pre></p> <p>Validation: - \u2705 Admin check works - \u2705 Clear error message - \u2705 Script exits gracefully</p>"},{"location":"TESTING-DEPLOYMENT/#test-w3-wsl-2-dependency","title":"Test W3: WSL 2 Dependency","text":"<p>Test Docker Desktop Start:</p> <p>Expected Behavior: - If WSL 2 not installed, Docker Desktop installation prompts for it - Script waits for Docker Desktop to be ready - Clear error if WSL 2 issues detected</p> <p>Validation: - \u2705 WSL 2 requirement documented - \u2705 Helpful error messages if WSL 2 missing - \u2705 Links to WSL 2 installation guide</p>"},{"location":"TESTING-DEPLOYMENT/#macos-testing","title":"macOS Testing","text":""},{"location":"TESTING-DEPLOYMENT/#test-m1-homebrew-auto-install","title":"Test M1: Homebrew Auto-Install","text":"<p>Test on Mac without Homebrew:</p> <ol> <li>Check if Homebrew installed: <code>which brew</code></li> <li>If not, start script should offer to install</li> </ol> <p>Expected Prompt: <pre><code>[INFO] Homebrew not found\n[INFO] Homebrew is required to install Docker Desktop\n[INFO] Install Homebrew? (Y/n):\n</code></pre></p> <p>Validation: - \u2705 Homebrew detection works - \u2705 Installation prompt clear - \u2705 Homebrew installs successfully - \u2705 Script continues after installation</p>"},{"location":"TESTING-DEPLOYMENT/#test-m2-docker-desktop-installation-macos","title":"Test M2: Docker Desktop Installation (macOS)","text":"<p>Expected Command: <pre><code>brew install --cask docker\n</code></pre></p> <p>Validation: - \u2705 Docker Desktop installed via Homebrew - \u2705 Application launches automatically - \u2705 Script waits for Docker daemon - \u2705 Proceeds after Docker ready</p>"},{"location":"TESTING-DEPLOYMENT/#test-m3-apple-silicon-vs-intel","title":"Test M3: Apple Silicon vs Intel","text":"<p>Test on both architectures:</p> <ul> <li>Apple Silicon (M1/M2/M3): Should use arm64 images</li> <li>Intel: Should use amd64 images</li> </ul> <p>Validation: <pre><code># Check architecture in use\ndocker inspect zetherion-ai-bot | jq '.[0].Architecture'\n</code></pre></p> <p>Expected: - \u2705 Apple Silicon: \"arm64\" - \u2705 Intel: \"amd64\" - \u2705 Both architectures work correctly</p>"},{"location":"TESTING-DEPLOYMENT/#linux-testing","title":"Linux Testing","text":""},{"location":"TESTING-DEPLOYMENT/#test-l1-docker-installation-detection","title":"Test L1: Docker Installation Detection","text":"<p>Test on fresh Ubuntu/Debian:</p> <pre><code># Check if Docker installed\nwhich docker\n\n# If not, script should provide instructions\n./start.sh\n</code></pre> <p>Expected Output: <pre><code>[ERROR] Docker is not installed\n[INFO] Please install Docker using one of these methods:\n[INFO]   Ubuntu/Debian: sudo apt-get update &amp;&amp; sudo apt-get install docker-ce docker-ce-cli containerd.io\n[INFO]   Fedora: sudo dnf install docker-ce\n[INFO]   See: https://docs.docker.com/engine/install/\n</code></pre></p> <p>Validation: - \u2705 Docker detection works - \u2705 Platform-specific install instructions provided - \u2705 Clear error message</p>"},{"location":"TESTING-DEPLOYMENT/#test-l2-docker-permission-issues","title":"Test L2: Docker Permission Issues","text":"<p>Test without docker group membership:</p> <pre><code># Remove user from docker group (if added)\nsudo gpasswd -d $USER docker\n\n# Try to run start script\n./start.sh\n</code></pre> <p>Expected Error: <pre><code>docker: permission denied while trying to connect to the Docker daemon socket\n</code></pre></p> <p>Expected Helpful Message: <pre><code>[ERROR] Docker permission denied\n[INFO] Add your user to the docker group:\n  sudo usermod -aG docker $USER\n  newgrp docker\n[INFO] Then try again\n</code></pre></p> <p>Validation: - \u2705 Permission error detected - \u2705 Clear resolution steps - \u2705 Documentation includes this scenario</p>"},{"location":"TESTING-DEPLOYMENT/#test-l3-systemd-docker-service","title":"Test L3: systemd Docker Service","text":"<p>Test Docker service start:</p> <pre><code># Check Docker service status\nsudo systemctl status docker\n\n# If not running, script should detect and start\n./start.sh\n</code></pre> <p>Expected Behavior: - Script detects Docker service not running - Attempts to start: <code>sudo systemctl start docker</code> - Waits for service to be ready - Proceeds with deployment</p> <p>Validation: - \u2705 Service status detected - \u2705 Automatic start attempted - \u2705 Clear error if start fails</p>"},{"location":"TESTING-DEPLOYMENT/#container-validation","title":"Container Validation","text":""},{"location":"TESTING-DEPLOYMENT/#distroless-image-verification","title":"Distroless Image Verification","text":""},{"location":"TESTING-DEPLOYMENT/#test-c1-image-size","title":"Test C1: Image Size","text":"<p>Verify distroless images are smaller:</p> <pre><code># Check image sizes\ndocker images | grep zetherion-ai\n</code></pre> <p>Expected Results: - <code>zetherion-ai-bot:latest</code> \u2248 180-220MB (distroless) - <code>zetherion-ai-skills:latest</code> \u2248 180-220MB (distroless) - Significantly smaller than python:3.11-slim (~500MB)</p> <p>Validation: - \u2705 Images under 250MB each - \u2705 Smaller than traditional Python images</p>"},{"location":"TESTING-DEPLOYMENT/#test-c2-no-shell-access","title":"Test C2: No Shell Access","text":"<p>Verify distroless has no shell:</p> <pre><code># Attempt to exec shell (should fail)\ndocker exec -it zetherion-ai-bot /bin/sh\n</code></pre> <p>Expected Error: <pre><code>OCI runtime exec failed: exec: \"/bin/sh\": stat /bin/sh: no such file or directory\n</code></pre></p> <p>Validation: - \u2705 Shell access denied (security feature) - \u2705 Cannot execute arbitrary commands - \u2705 Distroless working as intended</p>"},{"location":"TESTING-DEPLOYMENT/#test-c3-python-execution-only","title":"Test C3: Python Execution Only","text":"<p>Verify only Python works:</p> <pre><code># Python should work\ndocker exec zetherion-ai-bot python3.11 --version\n\n# Shell commands should NOT work\ndocker exec zetherion-ai-bot ls\n</code></pre> <p>Expected Results: - Python version displayed (e.g., \"Python 3.11.7\") - <code>ls</code> command fails (command not found)</p> <p>Validation: - \u2705 Python interpreter accessible - \u2705 System utilities not available (security)</p>"},{"location":"TESTING-DEPLOYMENT/#test-c4-non-root-user","title":"Test C4: Non-Root User","text":"<p>Verify container runs as non-root:</p> <pre><code># Check container user\ndocker exec zetherion-ai-bot python3.11 -c \"import os; print(f'UID: {os.getuid()}, GID: {os.getgid()}')\"\n</code></pre> <p>Expected Output: <pre><code>UID: 65532, GID: 65532\n</code></pre></p> <p>Validation: - \u2705 UID is 65532 (nonroot user) - \u2705 Not running as root (UID 0) - \u2705 Security best practice</p>"},{"location":"TESTING-DEPLOYMENT/#container-health-checks","title":"Container Health Checks","text":""},{"location":"TESTING-DEPLOYMENT/#test-c5-health-check-execution","title":"Test C5: Health Check Execution","text":"<p>Monitor health checks:</p> <pre><code># Watch health status change from \"starting\" to \"healthy\"\nwatch -n 1 'docker ps --format \"table {{.Names}}\\t{{.Status}}\"'\n</code></pre> <p>Expected Progression: <pre><code># Initial (0-30s)\nzetherion-ai-bot    Up 5 seconds (health: starting)\n\n# After health check passes (30-60s)\nzetherion-ai-bot    Up 45 seconds (healthy)\n</code></pre></p> <p>Validation: - \u2705 Health status changes from \"starting\" to \"healthy\" - \u2705 Happens within 60 seconds - \u2705 All containers show \"healthy\"</p>"},{"location":"TESTING-DEPLOYMENT/#test-c6-health-check-failure-detection","title":"Test C6: Health Check Failure Detection","text":"<p>Simulate health check failure:</p> <pre><code># Stop Qdrant (bot health check will fail)\ndocker stop zetherion-ai-qdrant\n\n# Wait 30 seconds for health check to detect failure\nsleep 30\n\n# Check bot status\ndocker ps --format \"table {{.Names}}\\t{{.Status}}\" | grep zetherion-ai-bot\n</code></pre> <p>Expected Status: <pre><code>zetherion-ai-bot    Up 2 minutes (unhealthy)\n</code></pre></p> <p>Validation: - \u2705 Unhealthy status detected - \u2705 Container still running (not crashed) - \u2705 Can be restarted to recover</p>"},{"location":"TESTING-DEPLOYMENT/#hardware-assessment-testing","title":"Hardware Assessment Testing","text":""},{"location":"TESTING-DEPLOYMENT/#cpu-detection-tests","title":"CPU Detection Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-h1-cpu-model-detection","title":"Test H1: CPU Model Detection","text":"<p>Windows: <pre><code># Compare script output with actual CPU\n$scriptCpu = (.\\start.ps1 2&gt;&amp;1 | Select-String \"CPU:\").Line\n$actualCpu = (Get-WmiObject Win32_Processor).Name\nWrite-Host \"Script: $scriptCpu\"\nWrite-Host \"Actual: $actualCpu\"\n</code></pre></p> <p>Unix: <pre><code># Compare script output with actual CPU\nscript_cpu=$(./start.sh 2&gt;&amp;1 | grep \"CPU:\")\nactual_cpu=$(lscpu | grep \"Model name\" | awk -F: '{print $2}' | xargs)\necho \"Script: $script_cpu\"\necho \"Actual: $actual_cpu\"\n</code></pre></p> <p>Validation: - \u2705 CPU model name matches - \u2705 Core count matches - \u2705 Thread count matches</p>"},{"location":"TESTING-DEPLOYMENT/#ram-detection-tests","title":"RAM Detection Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-h2-ram-amount-detection","title":"Test H2: RAM Amount Detection","text":"<p>Validation: <pre><code># Check detected RAM vs actual\ndocker run --rm zetherion-ai-assess:distroless | jq '.ram_total_gb, .ram_available_gb'\n\n# Compare with system info\nfree -g  # Linux\n# or\nvm_stat  # macOS\n</code></pre></p> <p>Expected: - Detected RAM within 10% of actual - Available RAM reasonable (50-90% of total)</p> <p>Validation: - \u2705 Total RAM detected accurately - \u2705 Available RAM reasonable - \u2705 Values in GB, not bytes</p>"},{"location":"TESTING-DEPLOYMENT/#gpu-detection-tests","title":"GPU Detection Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-h3-nvidia-gpu-detection","title":"Test H3: NVIDIA GPU Detection","text":"<p>Prerequisites: NVIDIA GPU installed</p> <pre><code># Check detected GPU\ndocker run --rm --gpus all zetherion-ai-assess:distroless | jq '.gpu'\n\n# Compare with nvidia-smi\nnvidia-smi --query-gpu=name,memory.total --format=csv\n</code></pre> <p>Expected Output: <pre><code>{\n  \"vendor\": \"NVIDIA\",\n  \"model\": \"GeForce RTX 3060\",\n  \"vram_gb\": 12\n}\n</code></pre></p> <p>Validation: - \u2705 NVIDIA GPU detected - \u2705 Model name matches nvidia-smi - \u2705 VRAM amount accurate</p>"},{"location":"TESTING-DEPLOYMENT/#test-h4-amd-gpu-detection","title":"Test H4: AMD GPU Detection","text":"<p>Prerequisites: AMD GPU installed</p> <pre><code># Check detected GPU\ndocker run --rm zetherion-ai-assess:distroless | jq '.gpu'\n\n# Compare with lspci\nlspci | grep VGA\n</code></pre> <p>Expected Output: <pre><code>{\n  \"vendor\": \"AMD\",\n  \"model\": \"Radeon RX 6800\",\n  \"vram_gb\": 16\n}\n</code></pre></p> <p>Validation: - \u2705 AMD GPU detected - \u2705 Model name matches lspci - \u2705 VRAM amount accurate</p>"},{"location":"TESTING-DEPLOYMENT/#test-h5-no-gpu-cpu-only-detection","title":"Test H5: No GPU (CPU-Only) Detection","text":"<p>Prerequisites: No dedicated GPU, or integrated graphics only</p> <pre><code># Check detected GPU\ndocker run --rm zetherion-ai-assess:distroless | jq '.gpu'\n</code></pre> <p>Expected Output: <pre><code>{\n  \"vendor\": null,\n  \"model\": null,\n  \"vram_gb\": 0\n}\n</code></pre></p> <p>Validation: - \u2705 No GPU detected (null values) - \u2705 Model recommendation adjusts to CPU-only - \u2705 Gemini or small models recommended</p>"},{"location":"TESTING-DEPLOYMENT/#integration-testing","title":"Integration Testing","text":""},{"location":"TESTING-DEPLOYMENT/#end-to-end-workflow-tests","title":"End-to-End Workflow Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-i1-fresh-installation-gemini-backend","title":"Test I1: Fresh Installation (Gemini Backend)","text":"<p>Test Steps: 1. Clean environment: <code>./cleanup.sh</code> or <code>.\\cleanup.ps1</code> 2. Remove .env: <code>rm .env</code> or <code>Remove-Item .env</code> 3. Run start script 4. Select Gemini backend 5. Wait for completion 6. Test Discord bot</p> <p>Expected Timeline: - Phase 1-3: ~30 seconds - Phase 4 (build): ~2 minutes - Phase 6-7: ~30 seconds - Total: ~3 minutes</p> <p>Validation: - \u2705 Completes in &lt;5 minutes - \u2705 All phases succeed - \u2705 Bot responds in Discord</p>"},{"location":"TESTING-DEPLOYMENT/#test-i2-fresh-installation-ollama-backend","title":"Test I2: Fresh Installation (Ollama Backend)","text":"<p>Test Steps: 1. Clean environment 2. Remove .env 3. Run start script 4. Select Ollama backend 5. Choose llama3.1:8b model 6. Wait for model download 7. Test Discord bot</p> <p>Expected Timeline: - Phases 1-4: ~2.5 minutes - Phase 5 (model download): ~5-7 minutes - Phases 6-7: ~30 seconds - Total: ~8-10 minutes</p> <p>Validation: - \u2705 Completes in &lt;12 minutes - \u2705 Model downloads successfully - \u2705 Bot responds in Discord</p>"},{"location":"TESTING-DEPLOYMENT/#test-i3-restart-with-existing-configuration","title":"Test I3: Restart with Existing Configuration","text":"<p>Test Steps: 1. Stop bot: <code>./stop.sh</code> or <code>.\\stop.ps1</code> 2. Start bot: <code>./start.sh</code> or <code>.\\start.ps1</code></p> <p>Expected Timeline: - Total: ~30 seconds</p> <p>Validation: - \u2705 No configuration prompts - \u2705 Containers start quickly - \u2705 Bot operational within 30s</p>"},{"location":"TESTING-DEPLOYMENT/#discord-bot-integration","title":"Discord Bot Integration","text":""},{"location":"TESTING-DEPLOYMENT/#test-i4-bot-responds-to-mentions","title":"Test I4: Bot Responds to Mentions","text":"<p>Prerequisites: Bot invited to Discord server</p> <p>Test Steps: 1. Ensure bot is online (green status) 2. Send message: <code>@Zetherion AI hello</code> 3. Wait for response</p> <p>Expected Response: <pre><code>Hello! I'm Zetherion AI, your AI assistant. How can I help you today?\n</code></pre></p> <p>Validation: - \u2705 Bot shows online status - \u2705 Bot responds within 3 seconds - \u2705 Response is coherent</p>"},{"location":"TESTING-DEPLOYMENT/#test-i5-memorysearch-commands","title":"Test I5: Memory/Search Commands","text":"<p>Test Steps: 1. Store memory: <code>@Zetherion AI remember I prefer Python for coding</code> 2. Wait for confirmation 3. Search memory: <code>@Zetherion AI search for my coding preferences</code></p> <p>Expected Behavior: - Memory stored in Qdrant - Search returns stored information</p> <p>Validation: - \u2705 Memory stored successfully - \u2705 Search retrieves correct information - \u2705 Qdrant database working</p>"},{"location":"TESTING-DEPLOYMENT/#status-script-testing","title":"Status Script Testing","text":""},{"location":"TESTING-DEPLOYMENT/#test-i6-statusps1-statussh-accuracy","title":"Test I6: status.ps1 / status.sh Accuracy","text":"<p>Test Steps: <pre><code># Run status script\n./status.sh  # or .\\status.ps1\n</code></pre></p> <p>Expected Output: <pre><code>============================================================\n  Zetherion AI Status\n============================================================\n\n[OK] Qdrant is running and healthy\n    Collections: 1\n    Vectors: 42\n\n[OK] Ollama is running and healthy\n    Models: 1\n      - llama3.1:8b\n\n[OK] Skills service is running and healthy\n\n[OK] Bot is running and healthy\n    Uptime: 0d 0h 15m 32s\n\n[OK] Zetherion AI is fully operational\n\nContainer Summary:\nzetherion-ai-qdrant   Up 15 minutes (healthy)\nzetherion-ai-ollama   Up 15 minutes (healthy)\nzetherion-ai-skills   Up 15 minutes (healthy)\nzetherion-ai-bot      Up 15 minutes (healthy)\n</code></pre></p> <p>Validation: - \u2705 All services show [OK] - \u2705 Collection and vector counts displayed - \u2705 Loaded models listed - \u2705 Uptime accurate - \u2705 Container summary matches docker ps</p>"},{"location":"TESTING-DEPLOYMENT/#stop-script-testing","title":"Stop Script Testing","text":""},{"location":"TESTING-DEPLOYMENT/#test-i7-graceful-shutdown","title":"Test I7: Graceful Shutdown","text":"<p>Test Steps: <pre><code># Stop all services\n./stop.sh  # or .\\stop.ps1\n</code></pre></p> <p>Expected Output: <pre><code>============================================================\n  Stopping Zetherion AI\n============================================================\n\n[INFO] Stopping Docker containers...\nStopping zetherion-ai-bot ... done\nStopping zetherion-ai-skills ... done\nStopping zetherion-ai-ollama ... done\nStopping zetherion-ai-qdrant ... done\n\n[OK] All containers stopped\n[INFO] Data preserved in Docker volumes\n[INFO] Run ./start.sh to restart\n</code></pre></p> <p>Validation: - \u2705 All containers stopped - \u2705 30-second timeout respected - \u2705 No errors - \u2705 Volumes preserved</p>"},{"location":"TESTING-DEPLOYMENT/#test-i8-data-persistence-after-stop","title":"Test I8: Data Persistence After Stop","text":"<p>Test Steps: 1. Store memory in Discord bot 2. Stop bot: <code>./stop.sh</code> 3. Start bot: <code>./start.sh</code> 4. Search for stored memory</p> <p>Validation: - \u2705 Memory persists after restart - \u2705 Qdrant data preserved - \u2705 Ollama model not re-downloaded</p>"},{"location":"TESTING-DEPLOYMENT/#cleanup-script-testing","title":"Cleanup Script Testing","text":""},{"location":"TESTING-DEPLOYMENT/#test-i9-cleanup-with-keep-data","title":"Test I9: Cleanup with Keep Data","text":"<p>Test Steps: <pre><code># Stop and cleanup, keeping data\n./cleanup.sh --keep-data  # or .\\cleanup.ps1 -KeepData\n</code></pre></p> <p>Expected Behavior: - Containers removed - Images removed - Volumes KEPT - .env KEPT</p> <p>Validation: <pre><code># Check containers (should be none)\ndocker ps -a | grep zetherion  # Empty\n\n# Check volumes (should exist)\ndocker volume ls | grep zetherion  # Shows volumes\n\n# Check .env (should exist)\nls -la .env  # File exists\n</code></pre></p> <p>Validation: - \u2705 Containers removed - \u2705 Volumes preserved - \u2705 .env preserved</p>"},{"location":"TESTING-DEPLOYMENT/#test-i10-complete-cleanup","title":"Test I10: Complete Cleanup","text":"<p>Test Steps: <pre><code># Complete removal (with confirmation)\n./cleanup.sh  # or .\\cleanup.ps1\n# Type 'yes' when prompted\n</code></pre></p> <p>Expected Behavior: - Containers removed - Images removed - Volumes removed - .env removed - Complete fresh slate</p> <p>Validation: <pre><code># Everything should be gone\ndocker ps -a | grep zetherion  # Empty\ndocker volume ls | grep zetherion  # Empty\ndocker images | grep zetherion  # Empty\nls .env  # File not found\n</code></pre></p> <p>Validation: - \u2705 All containers removed - \u2705 All volumes removed - \u2705 All images removed - \u2705 .env removed - \u2705 Clean environment</p>"},{"location":"TESTING-DEPLOYMENT/#performance-benchmarking","title":"Performance Benchmarking","text":""},{"location":"TESTING-DEPLOYMENT/#startup-time-benchmarks","title":"Startup Time Benchmarks","text":""},{"location":"TESTING-DEPLOYMENT/#test-p1-first-run-performance-gemini","title":"Test P1: First-Run Performance (Gemini)","text":"<p>Measurement: <pre><code># Windows\nMeasure-Command { .\\start.ps1 }\n\n# Unix\ntime ./start.sh\n</code></pre></p> <p>Expected Times: | Phase | Expected Duration | |-------|------------------| | Phase 1: Prerequisites | 5-10s | | Phase 2: Hardware Assessment | 10-20s | | Phase 3: Interactive Setup | 30-60s (user input) | | Phase 4: Build &amp; Deploy | 90-120s | | Phase 5: N/A (Gemini) | 0s | | Phase 6: Verification | 10-20s | | Total | 3-4 minutes |</p> <p>Validation: - \u2705 Completes within 5 minutes - \u2705 No phase takes excessively long - \u2705 Build time reasonable</p>"},{"location":"TESTING-DEPLOYMENT/#test-p2-first-run-performance-ollama","title":"Test P2: First-Run Performance (Ollama)","text":"<p>Expected Times: | Phase | Expected Duration | |-------|------------------| | Phase 1: Prerequisites | 5-10s | | Phase 2: Hardware Assessment | 10-20s | | Phase 3: Interactive Setup | 30-60s (user input) | | Phase 4: Build &amp; Deploy | 90-120s | | Phase 5: Model Download | 300-420s (5-7 min) | | Phase 6: Verification | 10-20s | | Total | 8-10 minutes |</p> <p>Validation: - \u2705 Completes within 12 minutes - \u2705 Model download shows progress - \u2705 Download speed reasonable for internet connection</p>"},{"location":"TESTING-DEPLOYMENT/#test-p3-subsequent-startup-performance","title":"Test P3: Subsequent Startup Performance","text":"<p>Measurement: <pre><code># Stop and restart\n./stop.sh\ntime ./start.sh\n</code></pre></p> <p>Expected Times: | Phase | Expected Duration | |-------|------------------| | Phase 1: Prerequisites | 5s | | Phase 2: Hardware Assessment | 0s (skipped) | | Phase 3: Config Check | 1s | | Phase 4: Container Start | 15-20s | | Phase 5: Model Check | 2s (already exists) | | Phase 6: Verification | 5s | | Total | 30-35 seconds |</p> <p>Validation: - \u2705 Completes within 45 seconds - \u2705 Fast startup from cached images - \u2705 No unnecessary steps</p>"},{"location":"TESTING-DEPLOYMENT/#resource-usage-benchmarks","title":"Resource Usage Benchmarks","text":""},{"location":"TESTING-DEPLOYMENT/#test-p4-memory-usage","title":"Test P4: Memory Usage","text":"<p>Measurement: <pre><code># Monitor memory usage\ndocker stats --no-stream zetherion-ai-bot zetherion-ai-qdrant zetherion-ai-ollama zetherion-ai-skills\n</code></pre></p> <p>Expected Memory Usage: | Container | Expected RAM | Notes | |-----------|--------------|-------| | zetherion-ai-bot | 200-400MB | Varies with activity | | zetherion-ai-qdrant | 100-200MB | Increases with vectors | | zetherion-ai-ollama | 5-8GB | Model in memory | | zetherion-ai-skills | 200-300MB | Varies with requests | | Total (Ollama) | 6-9GB | | | Total (Gemini) | 0.5-1GB | No Ollama |</p> <p>Validation: - \u2705 Memory usage within expected ranges - \u2705 No memory leaks over 24 hours - \u2705 Total under hardware recommendations</p>"},{"location":"TESTING-DEPLOYMENT/#test-p5-cpu-usage","title":"Test P5: CPU Usage","text":"<p>Measurement: <pre><code># Monitor CPU usage\ndocker stats --no-stream --format \"table {{.Name}}\\t{{.CPUPerc}}\"\n</code></pre></p> <p>Expected CPU Usage (Idle): | Container | Expected CPU | Notes | |-----------|--------------|-------| | zetherion-ai-bot | 0-2% | Idle | | zetherion-ai-qdrant | 0-1% | Idle | | zetherion-ai-ollama | 0% | Idle | | zetherion-ai-skills | 0-1% | Idle |</p> <p>Expected CPU Usage (Active - Query): | Container | Expected CPU | Notes | |-----------|--------------|-------| | zetherion-ai-bot | 10-30% | Processing | | zetherion-ai-ollama | 200-400% | Inference (multi-core) |</p> <p>Validation: - \u2705 Low CPU usage when idle - \u2705 CPU spikes during queries expected - \u2705 Returns to idle after query</p>"},{"location":"TESTING-DEPLOYMENT/#test-p6-disk-usage","title":"Test P6: Disk Usage","text":"<p>Measurement: <pre><code># Check volume sizes\ndocker system df -v\n</code></pre></p> <p>Expected Disk Usage: | Component | Expected Size | Notes | |-----------|---------------|-------| | zetherion-ai-bot image | 180-220MB | Distroless | | zetherion-ai-skills image | 180-220MB | Distroless | | qdrant_storage volume | 100MB-1GB | Grows with vectors | | ollama_models volume | 5-20GB | Model dependent | | Total (llama3.1:8b) | 6-8GB | | | Total (qwen2.5:32b) | 20-25GB | Large model |</p> <p>Validation: - \u2705 Distroless images under 250MB each - \u2705 Volume sizes reasonable - \u2705 Total under documented requirements</p>"},{"location":"TESTING-DEPLOYMENT/#response-time-benchmarks","title":"Response Time Benchmarks","text":""},{"location":"TESTING-DEPLOYMENT/#test-p7-bot-response-time-gemini-backend","title":"Test P7: Bot Response Time (Gemini Backend)","text":"<p>Measurement: <pre><code># Time Discord bot response\n# Send: \"@Zetherion AI hello\"\n# Note timestamp of send and response\n</code></pre></p> <p>Expected Response Times: | Query Type | Expected Time | Notes | |------------|---------------|-------| | Simple (hello) | 1-3s | Fast | | Memory search | 1-2s | Vector search | | Complex query | 3-7s | API call + processing |</p> <p>Validation: - \u2705 Simple queries under 3 seconds - \u2705 Complex queries under 10 seconds - \u2705 Consistent response times</p>"},{"location":"TESTING-DEPLOYMENT/#test-p8-bot-response-time-ollama-backend","title":"Test P8: Bot Response Time (Ollama Backend)","text":"<p>Expected Response Times: | Query Type | Expected Time | Notes | |------------|---------------|-------| | Simple (hello) | 2-5s | Local inference | | Memory search | 2-4s | Vector + local | | Complex query | 5-15s | Longer inference |</p> <p>Validation: - \u2705 Simple queries under 7 seconds - \u2705 Complex queries under 20 seconds - \u2705 Faster with GPU acceleration</p>"},{"location":"TESTING-DEPLOYMENT/#security-testing","title":"Security Testing","text":""},{"location":"TESTING-DEPLOYMENT/#container-security-tests","title":"Container Security Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-s1-distroless-base-image-verification","title":"Test S1: Distroless Base Image Verification","text":"<p>Verify base image: <pre><code># Check base image\ndocker inspect zetherion-ai-bot | jq '.[0].Config.Image'\n</code></pre></p> <p>Expected Output: <pre><code>\"gcr.io/distroless/python3-debian12:nonroot\"\n</code></pre></p> <p>Validation: - \u2705 Using distroless base - \u2705 Using nonroot variant - \u2705 No shell present</p>"},{"location":"TESTING-DEPLOYMENT/#test-s2-vulnerability-scanning","title":"Test S2: Vulnerability Scanning","text":"<p>Run security scan: <pre><code># Scan bot image for vulnerabilities\ndocker scan zetherion-ai-bot:latest\n\n# Or use Trivy\ntrivy image zetherion-ai-bot:latest\n</code></pre></p> <p>Expected Results: - No HIGH or CRITICAL vulnerabilities - Significantly fewer vulnerabilities than python:3.11-slim - Most findings in base distroless image (maintained by Google)</p> <p>Validation: - \u2705 Zero CRITICAL vulnerabilities - \u2705 HIGH vulnerabilities under 5 - \u2705 Fewer CVEs than standard Python images</p>"},{"location":"TESTING-DEPLOYMENT/#test-s3-non-root-user-verification","title":"Test S3: Non-Root User Verification","text":"<p>Verify all containers run as non-root: <pre><code># Check each container's user\nfor container in zetherion-ai-bot zetherion-ai-skills; do\n  echo \"$container:\"\n  docker exec $container python3.11 -c \"import os; print(f'  UID: {os.getuid()}')\"\ndone\n</code></pre></p> <p>Expected Output: <pre><code>zetherion-ai-bot:\n  UID: 65532\nzetherion-ai-skills:\n  UID: 65532\n</code></pre></p> <p>Validation: - \u2705 UID 65532 (nonroot) - \u2705 Not UID 0 (root) - \u2705 Security best practice</p>"},{"location":"TESTING-DEPLOYMENT/#encryption-tests","title":"Encryption Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-s4-encryption-enabled","title":"Test S4: Encryption Enabled","text":"<p>Prerequisites: Enable encryption in .env: <pre><code>ENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=test-passphrase-for-testing-only\n</code></pre></p> <p>Test Steps: 1. Restart bot with encryption enabled 2. Store memory: <code>@Zetherion AI remember my secret: test123</code> 3. Check Qdrant data (should be encrypted)</p> <p>Validation: <pre><code># Query Qdrant directly (data should be encrypted)\ncurl http://localhost:6333/collections/memories/points/scroll | jq\n</code></pre></p> <p>Expected: - Payload data encrypted (not plaintext \"test123\") - Metadata may be visible (timestamps, user IDs) - Bot can decrypt and retrieve correctly</p> <p>Validation: - \u2705 Data encrypted in Qdrant - \u2705 Bot can decrypt and retrieve - \u2705 Raw data not readable in database</p>"},{"location":"TESTING-DEPLOYMENT/#api-key-security-tests","title":"API Key Security Tests","text":""},{"location":"TESTING-DEPLOYMENT/#test-s5-api-keys-not-in-logs","title":"Test S5: API Keys Not in Logs","text":"<p>Test Steps: 1. View container logs 2. Check for exposed API keys</p> <p>Check Logs: <pre><code># Check bot logs\ndocker-compose logs zetherion-ai-bot | grep -i \"api\" | grep -i \"key\"\n\n# Check start script output\n./start.sh 2&gt;&amp;1 | grep -E \"(DISCORD_TOKEN|GEMINI_API_KEY|ANTHROPIC_API_KEY)\"\n</code></pre></p> <p>Expected Result: - No full API keys visible - At most, first 4 characters shown (e.g., \"sk-an****\") - No keys in startup output</p> <p>Validation: - \u2705 API keys redacted in logs - \u2705 Keys not echoed during setup - \u2705 .env file not displayed</p>"},{"location":"TESTING-DEPLOYMENT/#test-s6-env-file-permissions","title":"Test S6: .env File Permissions","text":"<p>Check file permissions:</p> <p>Unix: <pre><code>ls -la .env\n</code></pre></p> <p>Expected: <pre><code>-rw------- 1 user user 1234 Feb 07 10:30 .env\n</code></pre></p> <p>Validation: - \u2705 Owner read/write only (600) - \u2705 Not readable by other users - \u2705 Security best practice</p> <p>Windows: <pre><code>icacls .env\n</code></pre></p> <p>Expected: <pre><code>.env DOMAIN\\User:(R,W)\n</code></pre></p> <p>Validation: - \u2705 Only owner has access - \u2705 Other users denied</p>"},{"location":"TESTING-DEPLOYMENT/#troubleshooting-test-failures","title":"Troubleshooting Test Failures","text":""},{"location":"TESTING-DEPLOYMENT/#common-test-failures","title":"Common Test Failures","text":""},{"location":"TESTING-DEPLOYMENT/#failure-docker-desktop-not-starting","title":"Failure: Docker Desktop Not Starting","text":"<p>Symptoms: - Script times out waiting for Docker - \"Docker daemon not responding\"</p> <p>Resolution Steps: 1. Check WSL 2 (Windows): <code>wsl --status</code> 2. Update WSL: <code>wsl --update</code> 3. Restart Docker Desktop manually 4. Check Docker Desktop settings (Resources) 5. Restart computer if needed</p>"},{"location":"TESTING-DEPLOYMENT/#failure-health-checks-timeout","title":"Failure: Health Checks Timeout","text":"<p>Symptoms: - Containers start but never become \"healthy\" - Timeout after 120 seconds</p> <p>Resolution Steps: 1. Check Docker resource allocation (Memory) 2. View container logs: <code>docker-compose logs &lt;service&gt;</code> 3. Verify ports not in use: <code>netstat -an | grep 6333</code> 4. Try clean rebuild: <code>./cleanup.sh &amp;&amp; ./start.sh --force-rebuild</code></p>"},{"location":"TESTING-DEPLOYMENT/#failure-model-download-hangs","title":"Failure: Model Download Hangs","text":"<p>Symptoms: - Model download stuck at X% - No progress for 5+ minutes</p> <p>Resolution Steps: 1. Check internet connection 2. Check disk space: <code>df -h</code> or <code>Get-PSDrive</code> 3. Stop and retry:    <pre><code>docker exec zetherion-ai-ollama pkill ollama\ndocker exec zetherion-ai-ollama ollama pull llama3.1:8b\n</code></pre></p>"},{"location":"TESTING-DEPLOYMENT/#failure-bot-wont-respond-in-discord","title":"Failure: Bot Won't Respond in Discord","text":"<p>Symptoms: - Bot shows online but doesn't respond - No errors in logs</p> <p>Resolution Steps: 1. Verify Message Content Intent enabled 2. Check bot has permissions in channel 3. Test with direct mention: <code>@Zetherion AI hello</code> 4. Check logs: <code>docker-compose logs -f zetherion-ai-bot</code> 5. Verify Discord token valid</p>"},{"location":"TESTING-DEPLOYMENT/#debugging-commands","title":"Debugging Commands","text":""},{"location":"TESTING-DEPLOYMENT/#view-detailed-container-logs","title":"View Detailed Container Logs","text":"<pre><code># All services\ndocker-compose logs -f --tail 100\n\n# Specific service\ndocker-compose logs -f zetherion-ai-bot\n\n# With timestamps\ndocker-compose logs -f --timestamps zetherion-ai-bot\n\n# Last 50 lines only\ndocker-compose logs --tail 50 zetherion-ai-bot\n</code></pre>"},{"location":"TESTING-DEPLOYMENT/#inspect-container-configuration","title":"Inspect Container Configuration","text":"<pre><code># Full container inspect\ndocker inspect zetherion-ai-bot | jq\n\n# Check environment variables\ndocker inspect zetherion-ai-bot | jq '.[0].Config.Env'\n\n# Check health check config\ndocker inspect zetherion-ai-bot | jq '.[0].State.Health'\n\n# Check volume mounts\ndocker inspect zetherion-ai-bot | jq '.[0].Mounts'\n</code></pre>"},{"location":"TESTING-DEPLOYMENT/#test-service-connectivity","title":"Test Service Connectivity","text":"<pre><code># Test Qdrant\ncurl http://localhost:6333/healthz\n\n# Test Qdrant collections\ncurl http://localhost:6333/collections\n\n# Test Ollama (if enabled)\ncurl http://localhost:11434/api/tags\n\n# Test Ollama generate\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Hello!\",\n  \"stream\": false\n}'\n</code></pre>"},{"location":"TESTING-DEPLOYMENT/#check-resource-usage","title":"Check Resource Usage","text":"<pre><code># Real-time stats\ndocker stats\n\n# Disk usage\ndocker system df -v\n\n# Network inspection\ndocker network inspect personalbot_default\n\n# Volume inspection\ndocker volume inspect zetherion-ai_qdrant_storage\n</code></pre>"},{"location":"TESTING-DEPLOYMENT/#test-report-template","title":"Test Report Template","text":"<p>Use this template to document test results:</p> <pre><code># Zetherion AI Deployment Test Report\n\n**Date:** YYYY-MM-DD\n**Platform:** Windows 11 / macOS 14 / Ubuntu 22.04\n**Tester:** [Your Name]\n**Version:** [Git commit hash or tag]\n\n## Hardware Configuration\n- CPU: [Model, cores, threads]\n- RAM: [Total, available]\n- GPU: [Model, VRAM] or \"None\"\n- Disk: [Total, free]\n\n## Test Results Summary\n\n| Phase | Tests | Passed | Failed | Notes |\n|-------|-------|--------|--------|-------|\n| Phase 1: Prerequisites | 3 | 3 | 0 | |\n| Phase 2: Hardware Assessment | 5 | 5 | 0 | |\n| Phase 3: Configuration | 3 | 3 | 0 | |\n| Phase 4: Build &amp; Deploy | 4 | 4 | 0 | |\n| Phase 5: Model Download | 3 | 3 | 0 | |\n| Phase 6: Verification | 3 | 3 | 0 | |\n| Platform-Specific | 9 | 9 | 0 | |\n| Container Validation | 6 | 6 | 0 | |\n| Integration Tests | 10 | 10 | 0 | |\n| Performance Tests | 8 | 8 | 0 | |\n| Security Tests | 6 | 6 | 0 | |\n| **TOTAL** | **60** | **60** | **0** | |\n\n## Detailed Results\n\n### Phase 1: Prerequisites Check\n- \u2705 Test 1.1: Docker Detection - Passed\n- \u2705 Test 1.2: Docker Auto-Start - Passed\n- \u2705 Test 1.3: Disk Space Warning - Passed\n\n[Continue for all phases...]\n\n## Performance Metrics\n\n- **First-Run Time (Gemini):** 3m 45s\n- **First-Run Time (Ollama):** 9m 12s\n- **Subsequent Startup:** 32s\n- **Build Time:** 1m 58s\n- **Model Download Time:** 6m 24s\n\n## Failed Tests\n\n[List any failures with details]\n\n## Recommendations\n\n[Any suggestions for improvements]\n\n## Sign-Off\n\n- [ ] All critical tests passed\n- [ ] No security vulnerabilities found\n- [ ] Performance within acceptable range\n- [ ] Documentation accurate\n- [ ] Ready for deployment\n\n**Tester Signature:** _______________________\n**Date:** _______________________\n</code></pre> <p>Last Updated: 2026-02-07 Version: 3.0.0 (Fully Automated Docker Deployment)</p>"},{"location":"TESTING/","title":"Testing Guide","text":"<p>Complete guide to Zetherion AI's three-layer testing approach.</p>"},{"location":"TESTING/#test-types","title":"Test Types","text":"<p>Zetherion AI uses a three-layer testing pyramid for comprehensive coverage:</p> <ol> <li>Unit Tests - Fast, isolated tests with mocked dependencies (Discord bot, Agent, Router, Memory)</li> <li>Integration Tests - Full stack tests with Docker services (bypasses Discord API)</li> <li>Discord E2E Tests - True end-to-end tests using real Discord API</li> <li>Pre-commit Hooks - Automated linting and type checking before commits</li> </ol> <pre><code>              \ud83d\udd3a\n           Discord E2E\n        (Real Discord API)\n      Slowest | Most Realistic\n    \u2571                           \u2572\n   \u2571   Integration Tests          \u2572\n  \u2571 (Docker + Services + Agent)    \u2572\n \u2571    Medium Speed | Component      \u2572\n\u2571          Integration               \u2572\n\u2572_______________________________________\u2571\n \u2572           Unit Tests               \u2571\n  \u2572   (Mocked, Fast Feedback)        \u2571\n   \u2572________________________________\u2571\n    Fastest | Most Isolated | Largest\n\n---\n\n## Unit Tests\n\n### Running Unit Tests\n\n```bash\n# Run all unit tests (excluding integration tests)\npytest -m \"not integration\"\n\n# Run with coverage\npytest --cov=src/zetherion_ai --cov-report=html\n\n# Run specific test file\npytest tests/test_agent_core.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"TESTING/#writing-unit-tests","title":"Writing Unit Tests","text":"<p>Unit tests go in <code>tests/</code> directory:</p> <pre><code>import pytest\nfrom zetherion_ai.agent.core import Agent\n\ndef test_agent_initialization():\n    \"\"\"Test that agent initializes correctly.\"\"\"\n    agent = Agent(memory=mock_memory)\n    assert agent is not None\n</code></pre>"},{"location":"TESTING/#integration-tests","title":"Integration Tests","text":"<p>Integration tests start the entire Docker environment and simulate real user interactions.</p>"},{"location":"TESTING/#prerequisites","title":"Prerequisites","text":"<ol> <li>Docker must be running</li> <li>Environment variables must be set in <code>.env</code>:</li> <li><code>GEMINI_API_KEY</code> (required)</li> <li><code>DISCORD_TOKEN</code> (required)</li> <li>Other API keys (optional, but recommended for full testing)</li> </ol>"},{"location":"TESTING/#running-integration-tests","title":"Running Integration Tests","text":"<pre><code># Easy way - use the provided script\n./scripts/run-integration-tests.sh\n\n# Manual way\npytest tests/integration/test_e2e.py -v -s -m integration\n</code></pre>"},{"location":"TESTING/#what-integration-tests-do","title":"What Integration Tests Do","text":"<ol> <li>Start Docker Compose with test project name</li> <li>Wait for services to be healthy (Qdrant, Zetherion AI)</li> <li>Run test scenarios:</li> <li>Simple questions</li> <li>Memory storage and recall</li> <li>Complex tasks</li> <li>Conversation context</li> <li>Help commands</li> <li>Service health checks</li> <li>Clean up - Stop and remove containers</li> </ol>"},{"location":"TESTING/#integration-test-output","title":"Integration Test Output","text":"<pre><code>\ud83d\udc33 Starting Docker Compose environment...\n\u23f3 Waiting for services to be healthy...\n\u2705 Qdrant is healthy\n\u2705 Zetherion AI is running\n\ntest_simple_question PASSED\ntest_memory_store_and_recall PASSED\ntest_complex_task PASSED\ntest_conversation_context PASSED\ntest_help_command PASSED\ntest_docker_services_running PASSED\ntest_qdrant_collections_exist PASSED\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  All Integration Tests Passed! \u2713\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n</code></pre>"},{"location":"TESTING/#skipping-integration-tests","title":"Skipping Integration Tests","text":"<p>Integration tests can be slow (2-3 minutes). To skip them:</p> <pre><code># Skip in pytest\npytest -m \"not integration\"\n\n# Skip via environment variable\nexport SKIP_INTEGRATION_TESTS=true\npytest\n</code></pre>"},{"location":"TESTING/#discord-e2e-tests","title":"Discord E2E Tests","text":"<p>NEW: True end-to-end tests that send real messages through Discord API.</p> <p>Location: <code>tests/integration/test_discord_e2e.py</code> What: Tests real Discord bot message handling with actual Discord API Speed: Fast (~1 minute once bot is running) Marker: <code>discord_e2e</code></p>"},{"location":"TESTING/#whats-different-from-integration-tests","title":"What's Different from Integration Tests?","text":"Feature Integration Tests Discord E2E Tests Discord API \u274c Mocked (MockDiscordBot) \u2705 Real Discord messages Agent Logic \u2705 Full stack tested \u2705 Full stack tested Services \u2705 Qdrant + Ollama \u2705 Qdrant + Ollama Use Case Verify agent/router/memory Verify Discord bot integration"},{"location":"TESTING/#setup-requirements","title":"Setup Requirements","text":""},{"location":"TESTING/#1-create-test-bot-in-discord-developer-portal","title":"1. Create Test Bot in Discord Developer Portal","text":"<ol> <li>Go to https://discord.com/developers/applications</li> <li>Click \"New Application\" \u2192 Name: \"Zetherion AI Test Bot\"</li> <li>Navigate to \"Bot\" tab</li> <li>Click \"Reset Token\" \u2192 Copy token (you'll need this)</li> <li>Enable Required Privileged Gateway Intents:</li> <li>\u2705 Message Content Intent - Allows bot to read message content</li> <li>\u2705 Server Members Intent - Allows bot to see member information</li> <li>Configure Bot Permissions:</li> <li>\u2705 View Channels - See channels in the server</li> <li>\u2705 Send Messages - Send responses to users</li> <li>\u2705 Read Message History - Read previous messages</li> <li>\u2705 Use Application Commands - Enable slash commands</li> <li>\u2705 Mention Everyone, Here, and All Roles - For @mentions in responses</li> <li>Save changes</li> </ol>"},{"location":"TESTING/#2-create-test-discord-server-channel","title":"2. Create Test Discord Server &amp; Channel","text":"<ol> <li>Create a new Discord server (or use existing test server)</li> <li>Generate Bot Invite URL (choose one method):</li> </ol> <p>Method A - Manual URL: <pre><code>https://discord.com/api/oauth2/authorize?client_id=YOUR_CLIENT_ID&amp;permissions=274878285888&amp;scope=bot%20applications.commands\n</code></pre>    - Replace <code>YOUR_CLIENT_ID</code> with your bot's Application ID (from General Information tab)    - <code>permissions=274878285888</code> = View Channels + Send Messages + Read Message History + Use Application Commands + Mention Everyone</p> <p>Method B - Discord Developer Portal:    - In Discord Developer Portal, go to \"OAuth2\" \u2192 \"URL Generator\"    - Select scopes: <code>bot</code> and <code>applications.commands</code>    - Select bot permissions:      - View Channels      - Send Messages      - Read Message History      - Use Application Commands      - Mention Everyone, Here, and All Roles    - Copy generated URL at bottom of page</p> <ol> <li>Invite bot to test server using the generated URL</li> <li>Create a dedicated test channel (e.g., <code>#bot-testing</code>)</li> <li>Get Channel ID:</li> <li>Enable Developer Mode in Discord: Settings \u2192 Advanced \u2192 Developer Mode</li> <li>Right-click the test channel \u2192 \"Copy Channel ID\"</li> </ol>"},{"location":"TESTING/#3-configure-environment-variables","title":"3. Configure Environment Variables","text":"<p>Add to your <code>.env</code> file:</p> <pre><code># Discord E2E Testing (separate from main bot)\nTEST_DISCORD_BOT_TOKEN=your_test_bot_token_here\nTEST_DISCORD_CHANNEL_ID=1234567890123456789\n\n# Allow bot-to-bot messages (required for E2E tests)\nALLOW_BOT_MESSAGES=true\n</code></pre> <p>\u26a0\ufe0f Important: - Use a separate test bot, not your production bot! - Never commit <code>TEST_DISCORD_BOT_TOKEN</code> to git - Test bot should only have access to test servers - <code>ALLOW_BOT_MESSAGES=true</code> is required for Discord E2E tests to work   - By default, Zetherion AI ignores messages from other bots (to prevent bot-to-bot spam)   - Setting this to <code>true</code> allows the test bot to send messages to your production bot   - Keep this <code>false</code> in production unless you specifically need bot-to-bot communication</p>"},{"location":"TESTING/#running-discord-e2e-tests","title":"Running Discord E2E Tests","text":"<pre><code># Easy way - use the provided script\n./scripts/run-discord-e2e-tests.sh\n\n# Manual way\npytest tests/integration/test_discord_e2e.py -v -s -m discord_e2e\n\n# Skip Discord E2E tests (if not configured)\npytest tests/ -m \"not discord_e2e\" -v\n</code></pre>"},{"location":"TESTING/#what-discord-e2e-tests-cover","title":"What Discord E2E Tests Cover","text":"<ol> <li>\u2705 Message Handling - Bot receives and processes messages</li> <li>\u2705 Response Generation - Bot sends responses back to Discord</li> <li>\u2705 Memory Operations - Store and recall through Discord</li> <li>\u2705 Mentions - Bot responds to @mentions</li> <li>\u2705 Slash Commands - Commands registered and functional</li> <li>\u2705 Complex Queries - Multi-turn conversations</li> </ol>"},{"location":"TESTING/#example-discord-e2e-test","title":"Example Discord E2E Test","text":"<pre><code>@pytest.mark.discord_e2e\n@pytest.mark.asyncio\nasync def test_bot_responds_to_message(discord_test_client):\n    \"\"\"Test bot responds to a real Discord message.\"\"\"\n    # Send actual message through Discord API\n    test_message = await discord_test_client.send_message(\n        \"Hello Zetherion AI, what is 2+2?\"\n    )\n\n    # Wait for bot response (real Discord event)\n    response = await discord_test_client.wait_for_bot_response(\n        test_message, timeout=30.0\n    )\n\n    assert response is not None\n    assert len(response.content) &gt; 0\n\n    # Cleanup test messages\n    await discord_test_client.delete_message(test_message)\n    await discord_test_client.delete_message(response)\n</code></pre>"},{"location":"TESTING/#troubleshooting-discord-e2e-tests","title":"Troubleshooting Discord E2E Tests","text":"<p>Error: <code>TEST_DISCORD_BOT_TOKEN not set</code> <pre><code># Add to .env\necho \"TEST_DISCORD_BOT_TOKEN=your_token\" &gt;&gt; .env\necho \"TEST_DISCORD_CHANNEL_ID=123456789\" &gt;&gt; .env\n</code></pre></p> <p>Error: Bot not responding <pre><code># Check bot is online in Discord\n# Check bot logs\ndocker logs zetherion_ai-bot\n</code></pre></p> <p>Verify bot has required permissions in test channel: 1. Right-click test channel \u2192 \"Edit Channel\" \u2192 \"Permissions\" 2. Find your test bot in the permissions list 3. Ensure these permissions are enabled (green checkmarks):    - \u2705 View Channels    - \u2705 Send Messages    - \u2705 Read Message History    - \u2705 Use Application Commands 4. If permissions are missing, add them and try again</p> <p>Verify bot intents are enabled: 1. Go to Discord Developer Portal \u2192 Your Application \u2192 Bot 2. Scroll to \"Privileged Gateway Intents\" 3. Ensure both are enabled:    - \u2705 Message Content Intent    - \u2705 Server Members Intent 4. Save changes and restart bot if you made changes</p> <p>Error: Timeout waiting for response <pre><code># Possible causes:\n# 1. Bot not in test server - reinvite bot\n# 2. Bot lacks permissions - check channel permissions\n# 3. Rate limiting - wait 60 seconds between test runs\n# 4. Discord API issues - check https://discordstatus.com\n</code></pre></p>"},{"location":"TESTING/#required-permissions-summary","title":"Required Permissions Summary","text":"<p>Privileged Gateway Intents (Bot tab in Developer Portal): | Intent | Required | Purpose | |--------|----------|---------| | Message Content Intent | \u2705 Yes | Read message content for processing | | Server Members Intent | \u2705 Yes | Access member information |</p> <p>Bot Permissions (OAuth2 \u2192 URL Generator): | Permission | Required | Purpose | |-----------|----------|---------| | View Channels | \u2705 Yes | See test channel | | Send Messages | \u2705 Yes | Send responses | | Read Message History | \u2705 Yes | Read previous messages for context | | Use Application Commands | \u2705 Yes | Enable slash commands (/ask, /remember, etc.) | | Mention Everyone, Here, and All Roles | \u26a0\ufe0f Optional | Allow @mentions in responses |</p> <p>Permission Integer for OAuth URL: <code>274878285888</code></p>"},{"location":"TESTING/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Git hooks run automatically before commits and pushes.</p>"},{"location":"TESTING/#setup","title":"Setup","text":"<pre><code>./scripts/setup-git-hooks.sh\n</code></pre>"},{"location":"TESTING/#what-runs-on-commit","title":"What Runs on Commit","text":"<p>Pre-commit hook runs: - Ruff - Linting and formatting - Mypy - Type checking - Bandit - Security scanning - YAML/TOML checks - Trailing whitespace removal - Private key detection</p>"},{"location":"TESTING/#what-runs-on-push","title":"What Runs on Push","text":"<p>Pre-push hook runs: - All pre-commit checks - Unit tests (excluding integration)</p>"},{"location":"TESTING/#bypass-hooks-not-recommended","title":"Bypass Hooks (Not Recommended)","text":"<pre><code># Skip pre-commit hook\ngit commit --no-verify -m \"message\"\n\n# Skip pre-push hook\ngit push --no-verify\n</code></pre>"},{"location":"TESTING/#continuous-integration-github-actions","title":"Continuous Integration (GitHub Actions)","text":"<p>When you push to GitHub, the full CI pipeline runs:</p> <ol> <li>Lint &amp; Format (Ruff)</li> <li>Type Check (Mypy)</li> <li>Security Scan (Bandit)</li> <li>Unit Tests (Python 3.12 &amp; 3.13)</li> <li>Docker Build (Verify images build)</li> <li>Integration Tests (MockDiscordBot + full agent stack)</li> <li>Discord E2E Tests (Real Discord API - only if secrets configured)</li> </ol> <p>See .github/workflows/ci.yml for details.</p>"},{"location":"TESTING/#github-actions-secrets-setup","title":"GitHub Actions Secrets Setup","text":"<p>To enable Discord E2E tests in CI, add these secrets to your GitHub repository:</p> <ol> <li>Go to your repository on GitHub</li> <li>Navigate to Settings \u2192 Secrets and variables \u2192 Actions</li> <li>Click New repository secret and add:</li> </ol> Secret Name Description Example <code>DISCORD_TOKEN</code> Main bot token (production) <code>MTIzNDU2...</code> <code>GEMINI_API_KEY</code> Google AI API key <code>AIza...</code> <code>TEST_DISCORD_BOT_TOKEN</code> Test bot token (separate bot) <code>MTIzNDU2...</code> <code>TEST_DISCORD_CHANNEL_ID</code> Test channel ID <code>1234567890123456789</code> <code>ANTHROPIC_API_KEY</code> Anthropic API key (optional) <code>sk-ant-api03-...</code> <code>OPENAI_API_KEY</code> OpenAI API key (optional) <code>sk-...</code> <p>Important: - Use a separate test bot for <code>TEST_DISCORD_BOT_TOKEN</code>, not your production bot - Create a dedicated test server/channel for <code>TEST_DISCORD_CHANNEL_ID</code> - If <code>TEST_DISCORD_BOT_TOKEN</code> or <code>TEST_DISCORD_CHANNEL_ID</code> are not set, Discord E2E tests will be gracefully skipped - Test bot must have same permissions as production bot:   - Privileged Intents: Message Content + Server Members   - Bot Permissions: View Channels, Send Messages, Read Message History, Use Application Commands   - See Required Permissions Summary below for details</p>"},{"location":"TESTING/#ci-pipeline-behavior","title":"CI Pipeline Behavior","text":"<p>Discord E2E Tests: - \u2705 Run: If both <code>TEST_DISCORD_BOT_TOKEN</code> and <code>TEST_DISCORD_CHANNEL_ID</code> secrets are configured - \u23ed\ufe0f Skip: If either secret is missing (graceful skip, CI still passes) - \ud83d\udeab Fail: If tests run but fail (e.g., bot doesn't respond, assertions fail)</p> <p>Integration Tests: - \u2705 Run: Always (uses MockDiscordBot, doesn't require real Discord API) - \u23ed\ufe0f Skip: Only if commit message contains <code>[skip integration]</code></p>"},{"location":"TESTING/#test-coverage","title":"Test Coverage","text":""},{"location":"TESTING/#generate-coverage-report","title":"Generate Coverage Report","text":"<pre><code># HTML report\npytest --cov=src/zetherion_ai --cov-report=html\nopen htmlcov/index.html\n\n# Terminal report\npytest --cov=src/zetherion_ai --cov-report=term-missing\n\n# XML report (for CI)\npytest --cov=src/zetherion_ai --cov-report=xml\n</code></pre>"},{"location":"TESTING/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Minimum: 70% overall coverage</li> <li>Target: 85%+ for core modules</li> <li>Critical paths: 95%+ (authentication, security, data handling)</li> </ul>"},{"location":"TESTING/#debugging-test-failures","title":"Debugging Test Failures","text":""},{"location":"TESTING/#view-integration-test-logs","title":"View Integration Test Logs","text":"<p>If integration tests fail, check Docker logs:</p> <pre><code># View Zetherion AI logs\ndocker compose -p zetherion_ai-test logs zetherion_ai\n\n# View Qdrant logs\ndocker compose -p zetherion_ai-test logs qdrant\n\n# Follow logs in real-time\ndocker compose -p zetherion_ai-test logs -f\n</code></pre>"},{"location":"TESTING/#common-issues","title":"Common Issues","text":"<p>Issue: \"Services failed to become healthy\" - Solution: Check if ports 6333 or 8000 are already in use - Solution: Verify .env has valid API keys - Solution: Increase timeout in test (default 120s)</p> <p>Issue: \"Missing required environment variables\" - Solution: Copy <code>.env.example</code> to <code>.env</code> and fill in values</p> <p>Issue: \"Docker is not running\" - Solution: Start Docker Desktop</p> <p>Issue: Tests pass but bot doesn't respond in Discord - Solution: Integration tests use mock bot, not real Discord - Solution: For real Discord testing, use <code>./start.sh</code> instead</p>"},{"location":"TESTING/#test-markers","title":"Test Markers","text":"<p>Use pytest markers to run specific test categories:</p> <pre><code># Run only integration tests\npytest -m integration\n\n# Run everything except integration tests\npytest -m \"not integration\"\n\n# Run slow tests\npytest -m slow\n\n# Run fast tests only\npytest -m \"not slow and not integration\"\n</code></pre>"},{"location":"TESTING/#available-markers","title":"Available Markers","text":"<ul> <li><code>integration</code> - Service integration tests with Docker (MockDiscordBot)</li> <li><code>discord_e2e</code> - True E2E tests with real Discord API</li> <li><code>slow</code> - Tests that take &gt;5 seconds</li> <li>Custom markers can be added in <code>pyproject.toml</code></li> </ul> <pre><code># Run only Discord E2E tests\npytest -m discord_e2e\n\n# Run everything except Discord E2E\npytest -m \"not discord_e2e\"\n\n# Run integration but not Discord E2E\npytest -m \"integration and not discord_e2e\"\n</code></pre>"},{"location":"TESTING/#best-practices","title":"Best Practices","text":"<ol> <li>Run unit tests frequently during development</li> <li>Run integration tests before creating PRs</li> <li>Don't commit with failing tests</li> <li>Don't bypass hooks without good reason</li> <li>Keep tests fast - mock external APIs in unit tests</li> <li>Add tests for new features</li> <li>Update tests when changing behavior</li> </ol>"},{"location":"TESTING/#quick-reference","title":"Quick Reference","text":"<pre><code># Fast feedback loop during development\npytest -m \"not integration\" --tb=short\n\n# Full local test suite\npytest\n\n# Pre-commit check (manual)\npre-commit run --all-files\n\n# Integration tests\n./scripts/run-integration-tests.sh\n\n# Coverage report\npytest --cov=src/zetherion_ai --cov-report=html &amp;&amp; open htmlcov/index.html\n</code></pre>"},{"location":"TESTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"TESTING/#test-discovery-issues","title":"Test Discovery Issues","text":"<p>If pytest can't find tests:</p> <pre><code># Ensure PYTHONPATH includes src/\nexport PYTHONPATH=\"${PWD}/src:${PYTHONPATH}\"\n\n# Or use pytest.ini/pyproject.toml configuration\n</code></pre>"},{"location":"TESTING/#import-errors-in-tests","title":"Import Errors in Tests","text":"<pre><code># Install in editable mode\npip install -e .\n\n# Or use test dependencies\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"TESTING/#docker-cleanup","title":"Docker Cleanup","text":"<p>If test containers don't clean up:</p> <pre><code># Manual cleanup\ndocker compose -p zetherion_ai-test down -v\n\n# Nuclear option (removes ALL stopped containers)\ndocker system prune -a\n</code></pre>"},{"location":"TESTING/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"TESTING/#unit-test-template","title":"Unit Test Template","text":"<pre><code>\"\"\"Tests for new feature.\"\"\"\n\nimport pytest\nfrom zetherion_ai.feature import NewFeature\n\n\n@pytest.fixture\ndef feature():\n    \"\"\"Fixture for feature instance.\"\"\"\n    return NewFeature()\n\n\ndef test_new_feature(feature):\n    \"\"\"Test new feature behavior.\"\"\"\n    result = feature.do_something()\n    assert result == expected\n</code></pre>"},{"location":"TESTING/#integration-test-template","title":"Integration Test Template","text":"<pre><code>\"\"\"Integration test for new feature.\"\"\"\n\nimport pytest\n\n\n@pytest.mark.asyncio\n@pytest.mark.integration\nasync def test_new_feature_e2e(mock_bot):\n    \"\"\"Test new feature end-to-end.\"\"\"\n    response = await mock_bot.simulate_message(\"test message\")\n    assert \"expected\" in response.lower()\n</code></pre>"},{"location":"TESTING/#related-documentation","title":"Related Documentation","text":"<ul> <li>CI/CD Pipeline - GitHub Actions workflow</li> <li>Contributing - Development guidelines</li> <li>Architecture - System design</li> </ul>"},{"location":"TROUBLESHOOTING/","title":"Zetherion AI Troubleshooting Guide","text":"<p>Common issues and their solutions for running Zetherion AI.</p>"},{"location":"TROUBLESHOOTING/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Discord Errors</li> <li>Configuration Issues</li> <li>Docker Issues</li> <li>Ollama Issues</li> <li>Qdrant Connection Issues</li> <li>Python Version Issues</li> <li>API Key Issues</li> <li>Performance Issues</li> </ul>"},{"location":"TROUBLESHOOTING/#discord-errors","title":"Discord Errors","text":""},{"location":"TROUBLESHOOTING/#error-privilegedintentsrequired","title":"Error: <code>PrivilegedIntentsRequired</code>","text":"<p>Full Error: <pre><code>discord.errors.PrivilegedIntentsRequired: Shard ID None is requesting privileged intents\nthat have not been explicitly enabled in the developer portal.\n</code></pre></p> <p>Cause: The bot needs the Message Content Intent to read messages, but it's not enabled in Discord.</p> <p>Solution: 1. Go to https://discord.com/developers/applications 2. Select your bot application 3. Go to Bot tab 4. Scroll to Privileged Gateway Intents 5. Enable \"MESSAGE CONTENT INTENT\" (toggle it ON) 6. Click Save Changes 7. Restart the bot: <code>./stop.sh &amp;&amp; ./start.sh</code></p> <p>Why Required: Discord requires explicit permission to read message content for privacy/security.</p>"},{"location":"TROUBLESHOOTING/#bot-not-responding-in-server","title":"Bot Not Responding in Server","text":"<p>Symptoms: - Bot is online but doesn't respond to mentions - Slash commands work but messages don't</p> <p>Solutions:</p> <ol> <li>Check Bot Permissions:</li> <li>Bot needs <code>Send Messages</code>, <code>Read Messages</code>, <code>Embed Links</code> permissions</li> <li> <p>Right-click the bot in server \u2192 Manage \u2192 Check role permissions</p> </li> <li> <p>Verify Message Content Intent:</p> </li> <li> <p>See PrivilegedIntentsRequired above</p> </li> <li> <p>Check if Bot is Being Mentioned:</p> </li> <li> <p>Bot only responds to DMs or when mentioned: <code>@BotName your message</code></p> </li> <li> <p>Check Allowlist:</p> </li> <li>If <code>ALLOWED_USER_IDS</code> is set in <code>.env</code>, your user ID must be included</li> <li>To allow all users: <code>ALLOWED_USER_IDS=</code> (leave empty)</li> </ol>"},{"location":"TROUBLESHOOTING/#slash-commands-not-appearing","title":"Slash Commands Not Appearing","text":"<p>Symptoms: - Can't see <code>/ask</code>, <code>/remember</code>, <code>/search</code> commands</p> <p>Solutions:</p> <ol> <li>Wait for Sync:</li> <li>Commands can take up to 1 hour to appear globally</li> <li> <p>Restart Discord app to force refresh</p> </li> <li> <p>Check Bot Scope:</p> </li> <li>When you invited the bot, did you select <code>applications.commands</code> scope?</li> <li> <p>If not, generate new invite URL with both <code>bot</code> and <code>applications.commands</code></p> </li> <li> <p>Reinvite Bot:</p> </li> <li>Go to OAuth2 \u2192 URL Generator in Discord Developer Portal</li> <li>Select: <code>bot</code> + <code>applications.commands</code></li> <li> <p>Use new URL to invite bot</p> </li> <li> <p>Check Logs: <pre><code># Look for \"commands_synced\" message\n./status.sh\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#configuration-issues","title":"Configuration Issues","text":""},{"location":"TROUBLESHOOTING/#error-error-parsing-value-for-field-allowed_user_ids","title":"Error: <code>error parsing value for field \"allowed_user_ids\"</code>","text":"<p>Full Error: <pre><code>pydantic_settings.sources.SettingsError: error parsing value for field \"allowed_user_ids\"\n</code></pre></p> <p>Cause: Invalid format for <code>ALLOWED_USER_IDS</code> in <code>.env</code> file.</p> <p>Solution: <pre><code># Correct formats:\nALLOWED_USER_IDS=                        # Allow all users (empty)\nALLOWED_USER_IDS=123456789               # Single user\nALLOWED_USER_IDS=123456789,987654321     # Multiple users (comma-separated, no spaces)\n\n# Wrong formats:\nALLOWED_USER_IDS=123456789, 987654321    # Extra spaces after comma\nALLOWED_USER_IDS=[123456789]             # JSON format (not supported)\n</code></pre></p> <p>Get Your Discord User ID: 1. Enable Developer Mode in Discord: Settings \u2192 Advanced \u2192 Developer Mode 2. Right-click your username anywhere 3. Select \"Copy User ID\" 4. Paste into <code>.env</code>: <code>ALLOWED_USER_IDS=your_id_here</code></p>"},{"location":"TROUBLESHOOTING/#missing-environment-variables","title":"Missing Environment Variables","text":"<p>Error: <pre><code>Field required [type=missing]\n</code></pre></p> <p>Solution:</p> <ol> <li>Check Required Variables:</li> <li><code>DISCORD_TOKEN</code> (required)</li> <li> <p><code>GEMINI_API_KEY</code> (required)</p> </li> <li> <p>Optional Variables:</p> </li> <li><code>ANTHROPIC_API_KEY</code> (for Claude)</li> <li><code>OPENAI_API_KEY</code> (for GPT-4)</li> <li><code>ALLOWED_USER_IDS</code> (defaults to allow all)</li> <li><code>QDRANT_HOST</code> (defaults to \"qdrant\")</li> <li> <p><code>QDRANT_PORT</code> (defaults to 6333)</p> </li> <li> <p>Verify <code>.env</code> File Exists: <pre><code>ls -la .env\n# If missing:\ncp .env.example .env\n</code></pre></p> </li> <li> <p>Check for Trailing Spaces: <pre><code># Bad:\nDISCORD_TOKEN=abc123\n\n# Good:\nDISCORD_TOKEN=abc123\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#qdrant-connection-issues","title":"Qdrant Connection Issues","text":""},{"location":"TROUBLESHOOTING/#error-connection-refused-to-qdrant","title":"Error: Connection Refused to Qdrant","text":"<p>Full Error: <pre><code>ConnectionRefusedError: [Errno 61] Connection refused\nhttpcore._exceptions.ConnectError: [Errno 61] Connection refused\n</code></pre></p> <p>Cause: Bot can't connect to Qdrant vector database.</p> <p>Solutions:</p> <ol> <li> <p>Check Qdrant is Running: <pre><code>docker ps | grep qdrant\n# Should show zetherion_ai-qdrant running\n</code></pre></p> </li> <li> <p>Start Qdrant if Stopped: <pre><code>docker start zetherion_ai-qdrant\n# Or use the start script:\n./start.sh\n</code></pre></p> </li> <li> <p>Check <code>QDRANT_HOST</code> Setting:</p> </li> </ol> <p>For Local Development (./start.sh): <pre><code># In .env:\nQDRANT_HOST=localhost\nQDRANT_PORT=6333\n</code></pre></p> <p>For Docker Compose: <pre><code># In .env:\nQDRANT_HOST=qdrant\nQDRANT_PORT=6333\n</code></pre></p> <ol> <li> <p>Verify Qdrant Health: <pre><code>curl http://localhost:6333/healthz\n# Should return: healthy\n</code></pre></p> </li> <li> <p>Check Port Availability: <pre><code>lsof -i :6333\n# Should show Docker using port 6333\n</code></pre></p> </li> <li> <p>Restart Qdrant: <pre><code>docker restart zetherion_ai-qdrant\n# Wait 10 seconds\ncurl http://localhost:6333/healthz\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#qdrant-data-persistence","title":"Qdrant Data Persistence","text":"<p>Symptoms: - Bot loses all memories after restart - Collections disappear</p> <p>Cause: Qdrant data not being persisted to disk.</p> <p>Solution:</p> <ol> <li> <p>Check Volume Mount: <pre><code>docker inspect zetherion_ai-qdrant | grep -A 5 Mounts\n# Should show volume mounted to /qdrant/storage\n</code></pre></p> </li> <li> <p>Verify Data Directory: <pre><code>ls -la qdrant_storage/\n# Should contain Qdrant database files\n</code></pre></p> </li> <li> <p>Recreate with Proper Volume: <pre><code>docker stop zetherion_ai-qdrant\ndocker rm zetherion_ai-qdrant\n./start.sh  # Will recreate with volume\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#python-version-issues","title":"Python Version Issues","text":""},{"location":"TROUBLESHOOTING/#error-package-requires-a-different-python","title":"Error: <code>Package requires a different Python</code>","text":"<p>Full Error: <pre><code>ERROR: Package 'zetherion_ai' requires a different Python: 3.11.6 not in '&gt;=3.12'\n</code></pre></p> <p>Cause: Zetherion AI requires Python 3.12+, but you have an older version.</p> <p>Solutions:</p> <ol> <li> <p>Install Python 3.12+ (macOS with Homebrew): <pre><code>brew install python@3.12\n</code></pre></p> </li> <li> <p>Verify Installation: <pre><code>python3.12 --version\n# Should show: Python 3.12.x\n</code></pre></p> </li> <li> <p>Recreate Virtual Environment: <pre><code>rm -rf .venv\npython3.12 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npip install -e .\n</code></pre></p> </li> <li> <p>Use start.sh (Automatic): <pre><code>./start.sh\n# Automatically finds Python 3.12+\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#multiple-python-versions","title":"Multiple Python Versions","text":"<p>Symptoms: - <code>python3 --version</code> shows old version - But <code>python3.12</code> exists</p> <p>Solution: The start script handles this automatically. If you need to manually use Python 3.12:</p> <pre><code># Always use explicit version:\npython3.12 -m venv .venv\nsource .venv/bin/activate\n\n# Or use the start script:\n./start.sh\n</code></pre>"},{"location":"TROUBLESHOOTING/#api-key-issues","title":"API Key Issues","text":""},{"location":"TROUBLESHOOTING/#invalid-discord-token","title":"Invalid Discord Token","text":"<p>Error: <pre><code>discord.errors.LoginFailure: Improper token has been passed\n</code></pre></p> <p>Solutions:</p> <ol> <li>Regenerate Token:</li> <li>Go to Discord Developer Portal \u2192 Bot tab</li> <li>Click \"Reset Token\"</li> <li>Copy new token immediately</li> <li> <p>Update <code>.env</code>: <code>DISCORD_TOKEN=new_token_here</code></p> </li> <li> <p>Check for Extra Spaces: <pre><code># Bad:\nDISCORD_TOKEN= abc123\nDISCORD_TOKEN=abc123\n\n# Good:\nDISCORD_TOKEN=abc123\n</code></pre></p> </li> <li> <p>Verify Token Format:</p> </li> <li>Three parts separated by dots (Base64-encoded)</li> <li>Format: <code>&lt;FIRST_PART&gt;.&lt;SECOND_PART&gt;.&lt;THIRD_PART&gt;</code></li> <li>Each part contains alphanumeric characters, hyphens, and underscores</li> <li>Real tokens are much longer (70+ characters total)</li> <li>No quotes needed in <code>.env</code></li> <li>Never share your actual token!</li> </ol>"},{"location":"TROUBLESHOOTING/#invalid-gemini-api-key","title":"Invalid Gemini API Key","text":"<p>Error: <pre><code>google.api_core.exceptions.PermissionDenied: 403 API key not valid\n</code></pre></p> <p>Solutions:</p> <ol> <li>Verify API Key:</li> <li>Go to https://aistudio.google.com/app/apikey</li> <li>Check if key exists and is enabled</li> <li> <p>Generate new key if needed</p> </li> <li> <p>Check Gemini API Quotas:</p> </li> <li>Free tier has limits</li> <li> <p>Check quota at: https://aistudio.google.com/app/apikey</p> </li> <li> <p>Enable Gemini API:</p> </li> <li>Some Google accounts need to enable the API first</li> <li>Visit: https://makersuite.google.com/</li> </ol>"},{"location":"TROUBLESHOOTING/#rate-limiting","title":"Rate Limiting","text":"<p>Error: <pre><code>429 Too Many Requests\nanthropic.RateLimitError: rate_limit_error\n</code></pre></p> <p>Cause: Too many API requests in short time.</p> <p>Solutions:</p> <ol> <li>Wait Before Retrying:</li> <li>The bot has automatic retry with backoff</li> <li> <p>Wait 1-2 minutes before trying again</p> </li> <li> <p>Reduce Usage:</p> </li> <li>Gemini Flash: 15 requests/minute (free tier)</li> <li>Claude: Depends on your tier</li> <li> <p>OpenAI: Depends on your tier</p> </li> <li> <p>Check API Dashboard:</p> </li> <li>Anthropic: https://console.anthropic.com/</li> <li> <p>OpenAI: https://platform.openai.com/usage</p> </li> <li> <p>Upgrade API Tier:</p> </li> <li>Most rate limits are per-tier</li> <li>Free \u2192 Paid tier often increases limits significantly</li> </ol>"},{"location":"TROUBLESHOOTING/#performance-issues","title":"Performance Issues","text":""},{"location":"TROUBLESHOOTING/#slow-response-times","title":"Slow Response Times","text":"<p>Symptoms: - Bot takes 10+ seconds to respond - Timeouts in Discord</p> <p>Solutions:</p> <ol> <li>Check Which Model is Being Used:</li> <li>Simple queries \u2192 Gemini Flash (fast)</li> <li> <p>Complex tasks \u2192 Claude/GPT-4 (slower)</p> </li> <li> <p>Adjust Router Threshold:</p> </li> <li>Edit <code>src/zetherion_ai/agent/router.py</code></li> <li> <p>Line 136: Change <code>confidence &gt; 0.7</code> to <code>0.8</code> to use Flash more often</p> </li> <li> <p>Check Qdrant Performance: <pre><code>curl http://localhost:6333/metrics\n# Check for slow queries\n</code></pre></p> </li> <li> <p>Reduce Memory Context:</p> </li> <li>Edit <code>src/zetherion_ai/agent/core.py</code></li> <li>Line ~145: Reduce <code>memory_limit</code> and <code>history_limit</code></li> </ol>"},{"location":"TROUBLESHOOTING/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptoms: - Bot using multiple GB of RAM - System slowing down</p> <p>Solutions:</p> <ol> <li> <p>Check Qdrant Memory: <pre><code>docker stats zetherion_ai-qdrant\n</code></pre></p> </li> <li> <p>Limit Qdrant Memory: <pre><code># Stop and recreate with memory limit:\ndocker stop zetherion_ai-qdrant\ndocker rm zetherion_ai-qdrant\ndocker run -d \\\n  --name zetherion_ai-qdrant \\\n  -p 6333:6333 \\\n  --memory=\"2g\" \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant:latest\n</code></pre></p> </li> <li> <p>Clear Old Memories: <pre><code># Delete qdrant_storage and restart:\nrm -rf qdrant_storage\n./start.sh\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#docker-issues","title":"Docker Issues","text":""},{"location":"TROUBLESHOOTING/#docker-not-running","title":"Docker Not Running","text":"<p>Error: <pre><code>Cannot connect to the Docker daemon\n</code></pre></p> <p>Solution:</p> <p>Automatic (Recommended): <pre><code>./start.sh\n# The script will detect Docker is not running and:\n# 1. Launch Docker Desktop automatically\n# 2. Wait for daemon to be ready (up to 90 seconds)\n# 3. Continue with setup\n</code></pre></p> <p>Manual: 1. Open Docker Desktop application 2. Wait for it to fully start (green icon in menu bar) 3. Verify: <code>docker ps</code> 4. Retry: <code>./start.sh</code></p> <p>If Docker Desktop won't start: - Check Activity Monitor for stuck Docker processes - Try: <code>killall Docker</code> then relaunch - Check Console.app for Docker error logs - Reinstall Docker Desktop if necessary</p>"},{"location":"TROUBLESHOOTING/#docker-daemon-not-ready-after-launch","title":"Docker Daemon Not Ready After Launch","text":"<p>Symptoms: - Docker Desktop GUI is open - <code>docker info</code> returns connection error - startup script times out waiting for daemon</p> <p>Causes: - Docker still initializing (can take 30-60 seconds on cold start) - Docker settings corrupted - Insufficient system resources</p> <p>Solutions:</p> <ol> <li> <p>Wait Longer: <pre><code># The start script waits up to 90 seconds\n# If you manually started Docker, wait before running commands:\nfor i in {1..60}; do\n  docker info &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"Ready!\" &amp;&amp; break\n  echo \"Waiting... ($i/60)\"\n  sleep 1\ndone\n</code></pre></p> </li> <li> <p>Check Docker Desktop Status:</p> </li> <li>Look at menu bar icon</li> <li>Should show green icon with no error messages</li> <li> <p>If yellow or red, click for details</p> </li> <li> <p>Restart Docker Desktop: <pre><code>osascript -e 'quit app \"Docker\"'\nsleep 5\nopen -a Docker\n</code></pre></p> </li> <li> <p>Check Available Resources: <pre><code># macOS:\nvm_stat | head -3\ndf -h /  # Disk space\n\n# Ensure you have:\n# - At least 2GB free RAM\n# - At least 10GB free disk space\n</code></pre></p> </li> <li> <p>Reset Docker (if stuck):</p> </li> <li>Docker Desktop \u2192 Troubleshoot \u2192 Reset to factory defaults</li> <li>WARNING: Deletes all containers/images</li> <li>Run <code>./start.sh</code> to rebuild</li> </ol>"},{"location":"TROUBLESHOOTING/#docker-desktop-memory-allocation","title":"Docker Desktop Memory Allocation","text":"<p>How much memory should Docker have?</p> <p>Depends on router backend:</p> <p>Gemini (Cloud): - Minimum: 2GB (for Qdrant only) - Recommended: 4GB (safe buffer)</p> <p>Ollama (Local): - Depends on model selected - See recommendations:   - <code>phi3:mini</code>: 5GB   - <code>llama3.1:8b</code>: 8GB   - <code>qwen2.5:7b</code>: 10GB   - <code>mistral:7b</code>: 7GB</p> <p>Automated Management: <pre><code># The startup script handles this automatically:\n./start.sh\n\n# It will:\n# 1. Detect your selected model\n# 2. Check Docker's current allocation\n# 3. Prompt to increase if needed\n# 4. Automatically update Docker settings\n# 5. Restart Docker if required\n</code></pre></p> <p>Manual Check: <pre><code># Check current allocation:\ndocker info | grep \"Total Memory\"\n\n# Check what's required:\ngrep OLLAMA_DOCKER_MEMORY .env\n</code></pre></p> <p>See also: Docker Architecture for detailed explanation.</p>"},{"location":"TROUBLESHOOTING/#port-already-in-use","title":"Port Already in Use","text":"<p>Error: <pre><code>Error starting userland proxy: listen tcp 0.0.0.0:6333: bind: address already in use\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Find What's Using Port 6333: <pre><code>lsof -i :6333\n</code></pre></p> </li> <li> <p>Kill the Process: <pre><code># Get PID from lsof, then:\nkill &lt;PID&gt;\n</code></pre></p> </li> <li> <p>Use Different Port: <pre><code># In .env:\nQDRANT_PORT=6334\n\n# Recreate Qdrant:\ndocker rm -f zetherion_ai-qdrant\ndocker run -d \\\n  --name zetherion_ai-qdrant \\\n  -p 6334:6333 \\\n  -v $(pwd)/qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant:latest\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#ollama-issues","title":"Ollama Issues","text":""},{"location":"TROUBLESHOOTING/#ollama-container-fails-with-out-of-memory","title":"Ollama Container Fails with \"Out of Memory\"","text":"<p>Error: <pre><code>Error: llama runner process has terminated: signal: killed\nServer error: '500 Internal Server Error'\n</code></pre></p> <p>Cause: Docker Desktop doesn't have enough RAM allocated for the Ollama model.</p> <p>Solution:</p> <p>The startup script should handle this automatically, but if you encounter it manually:</p> <ol> <li> <p>Check Docker Desktop Memory: <pre><code>docker info | grep \"Total Memory\"\n# Example output: Total Memory: 4 GiB\n</code></pre></p> </li> <li> <p>Increase Docker Desktop Memory: <pre><code># Automated approach (recommended):\ncd scripts\n./increase-docker-memory.sh\n\n# This will:\n# - Detect required memory from .env\n# - Update Docker Desktop settings\n# - Restart Docker automatically\n</code></pre></p> </li> <li> <p>Manual Approach (if script fails):</p> </li> <li>Open Docker Desktop</li> <li>Go to Settings \u2192 Resources \u2192 Advanced</li> <li>Increase Memory slider to match your model's requirement:<ul> <li><code>phi3:mini</code> \u2192 5GB minimum</li> <li><code>llama3.1:8b</code> \u2192 8GB minimum</li> <li><code>qwen2.5:7b</code> \u2192 10GB minimum</li> <li><code>mistral:7b</code> \u2192 7GB minimum</li> </ul> </li> <li>Click Apply &amp; Restart</li> <li> <p>Wait for Docker to restart (30-60 seconds)</p> </li> <li> <p>Verify the Change: <pre><code>docker info | grep \"Total Memory\"\n# Should show your new allocation\n</code></pre></p> </li> </ol> <p>See also: Docker Architecture for understanding Docker Desktop vs container memory.</p>"},{"location":"TROUBLESHOOTING/#ollama-model-download-fails","title":"Ollama Model Download Fails","text":"<p>Error: <pre><code>Failed to download model 'qwen2.5:7b'\npulling manifest: Get \"https://registry.ollama.ai/v2/library/qwen2.5/manifests/7b\": EOF\n</code></pre></p> <p>Causes: - Network connectivity issues - Registry temporarily unavailable - Insufficient disk space</p> <p>Solutions:</p> <ol> <li> <p>Check Internet Connection: <pre><code>curl -I https://ollama.ai\n# Should return: HTTP/2 200\n</code></pre></p> </li> <li> <p>Check Disk Space: <pre><code>df -h .\n# Ensure at least 10GB free for large models\n</code></pre></p> </li> <li> <p>Manually Pull Model: <pre><code># If automatic pull failed, try manually:\ndocker exec zetherion_ai-ollama ollama pull qwen2.5:7b\n\n# For smaller model:\ndocker exec zetherion_ai-ollama ollama pull phi3:mini\n</code></pre></p> </li> <li> <p>Use Different Model: <pre><code># Switch to smaller model:\nrm .ollama_assessed\n# Edit .env to prefer smaller model\necho \"OLLAMA_ROUTER_MODEL=phi3:mini\" &gt;&gt; .env\n./start.sh\n</code></pre></p> </li> <li> <p>Fallback to Gemini:</p> </li> <li>The bot automatically falls back to Gemini if Ollama model unavailable</li> <li>Change router backend in <code>.env</code>:      <pre><code>ROUTER_BACKEND=gemini\n</code></pre></li> </ol>"},{"location":"TROUBLESHOOTING/#ollama-connection-error","title":"Ollama Connection Error","text":"<p>Error: <pre><code>[Errno 8] nodename nor servname provided, or not known\nhttpx.ConnectError: [Errno 61] Connection refused\n</code></pre></p> <p>Cause: Bot can't connect to Ollama container.</p> <p>Solutions:</p> <ol> <li> <p>Check Ollama Container is Running: <pre><code>docker ps | grep ollama\n# Should show: zetherion_ai-ollama\n</code></pre></p> </li> <li> <p>Start Ollama Container: <pre><code>docker start zetherion_ai-ollama\n# Or use the start script:\n./start.sh\n</code></pre></p> </li> <li> <p>Check <code>OLLAMA_HOST</code> Setting:</p> </li> </ol> <p>For Local Development (./start.sh): <pre><code># In .env:\nOLLAMA_HOST=localhost\nOLLAMA_PORT=11434\n</code></pre></p> <p>For Docker Compose: <pre><code># In .env:\nOLLAMA_HOST=ollama\nOLLAMA_PORT=11434\n</code></pre></p> <ol> <li> <p>Verify Ollama API: <pre><code>curl http://localhost:11434/api/tags\n# Should return JSON with list of models\n</code></pre></p> </li> <li> <p>Check Container Logs: <pre><code>docker logs zetherion_ai-ollama\n# Look for errors or OOM messages\n</code></pre></p> </li> </ol>"},{"location":"TROUBLESHOOTING/#ollama-slow-response-times","title":"Ollama Slow Response Times","text":"<p>Symptoms: - Ollama takes 30+ seconds to respond - Slower than expected inference</p> <p>Solutions:</p> <ol> <li> <p>Check CPU Usage: <pre><code>docker stats zetherion_ai-ollama\n# Look at CPU% - should be 100-400% during inference\n</code></pre></p> </li> <li> <p>Verify Model Size: <pre><code>docker exec zetherion_ai-ollama ollama list\n# Smaller models (phi3:mini) are faster than large ones\n</code></pre></p> </li> <li> <p>Switch to Smaller Model: <pre><code># Remove assessment marker to choose again:\nrm .ollama_assessed\n./start.sh\n# Select phi3:mini or mistral:7b when prompted\n</code></pre></p> </li> <li> <p>Check Docker Memory: <pre><code>docker stats zetherion_ai-ollama\n# MEM USAGE should be well below LIMIT\n# If at limit, model is swapping (very slow)\n</code></pre></p> </li> <li> <p>Add More Docker Memory: <pre><code># See \"Ollama Container Fails with Out of Memory\" above\n./scripts/increase-docker-memory.sh\n</code></pre></p> </li> <li> <p>Use GPU Acceleration (if available):</p> </li> <li>Requires NVIDIA GPU with Docker GPU support</li> <li>Or Apple Silicon Mac (uses Metal automatically)</li> <li>Check logs for \"GPU detected\" message</li> </ol>"},{"location":"TROUBLESHOOTING/#docker-desktop-wont-start-after-memory-increase","title":"Docker Desktop Won't Start After Memory Increase","text":"<p>Symptoms: - Ran <code>increase-docker-memory.sh</code> - Docker Desktop GUI opens but daemon never becomes ready - Stuck at \"Docker Desktop is starting...\"</p> <p>Possible Causes: - Requested more RAM than system has available - Other applications using too much memory - Docker settings corrupted</p> <p>Solutions:</p> <ol> <li> <p>Check System RAM Availability: <pre><code># macOS:\nvm_stat | head -2\n# Look at \"Pages free\"\n</code></pre></p> </li> <li> <p>Restore Backup Settings: <pre><code># increase-docker-memory.sh creates backups\ncd ~/Library/Group\\ Containers/group.com.docker/\nls -lt settings.json.backup.*\n\n# Restore most recent backup:\ncp settings.json.backup.20260205_143022 settings.json\n</code></pre></p> </li> <li> <p>Manually Reduce Memory: <pre><code># Edit settings file:\nvim ~/Library/Group\\ Containers/group.com.docker/settings.json\n\n# Find \"memoryMiB\" and set to safe value:\n{\n  \"memoryMiB\": 6144,  // 6GB - usually safe\n  ...\n}\n</code></pre></p> </li> <li> <p>Restart Docker: <pre><code># Quit Docker Desktop\nosascript -e 'quit app \"Docker\"'\nsleep 5\n\n# Relaunch\nopen -a Docker\n\n# Wait for daemon\nfor i in {1..60}; do\n  docker info &gt;/dev/null 2&gt;&amp;1 &amp;&amp; echo \"Ready!\" &amp;&amp; break\n  sleep 1\ndone\n</code></pre></p> </li> <li> <p>Reset Docker Desktop (Last Resort):</p> </li> <li>Open Docker Desktop</li> <li>Go to Troubleshoot \u2192 Reset to factory defaults</li> <li>WARNING: This deletes all containers and images</li> <li> <p>After reset, run <code>./start.sh</code> to recreate everything</p> </li> <li> <p>Close Other Memory-Intensive Apps:</p> </li> <li>Quit Chrome, Slack, IDEs, etc.</li> <li>Check Activity Monitor for memory hogs</li> <li>Try increasing Docker memory again with more free RAM</li> </ol>"},{"location":"TROUBLESHOOTING/#switch-between-gemini-and-ollama","title":"Switch Between Gemini and Ollama","text":"<p>How to change router backend after initial setup:</p> <p>Switch to Ollama: <pre><code># 1. Edit .env\nsed -i '' 's/ROUTER_BACKEND=gemini/ROUTER_BACKEND=ollama/' .env\n\n# 2. Run assessment (if not done before)\nrm .ollama_assessed  # Force re-assessment\n./start.sh\n</code></pre></p> <p>Switch to Gemini: <pre><code># 1. Edit .env\nsed -i '' 's/ROUTER_BACKEND=ollama/ROUTER_BACKEND=gemini/' .env\n\n# 2. Restart bot\n./stop.sh &amp;&amp; ./start.sh\n</code></pre></p> <p>Or manually edit <code>.env</code>: <pre><code># Change this line:\nROUTER_BACKEND=gemini  # or ollama\n</code></pre></p>"},{"location":"TROUBLESHOOTING/#getting-help","title":"Getting Help","text":""},{"location":"TROUBLESHOOTING/#collecting-debug-information","title":"Collecting Debug Information","text":"<p>Before asking for help, gather this information:</p> <pre><code># 1. Check status\n./status.sh\n\n# 2. Get bot logs (last 50 lines)\ndocker logs zetherion_ai-qdrant --tail 50\n\n# 3. Check Python version\npython3 --version\n\n# 4. Check Docker version\ndocker --version\n\n# 5. Test Qdrant\ncurl http://localhost:6333/healthz\n\n# 6. Check disk space\ndf -h .\n</code></pre>"},{"location":"TROUBLESHOOTING/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code># In .env:\nLOG_LEVEL=DEBUG\n\n# Restart:\n./stop.sh &amp;&amp; ./start.sh\n</code></pre>"},{"location":"TROUBLESHOOTING/#report-an-issue","title":"Report an Issue","text":"<p>Include: - OS version (macOS/Linux) - Python version - Full error message (last 20 lines) - Steps to reproduce - What you've already tried</p>"},{"location":"TROUBLESHOOTING/#quick-reference","title":"Quick Reference","text":""},{"location":"TROUBLESHOOTING/#common-commands","title":"Common Commands","text":"<pre><code># Start bot\n./start.sh\n\n# Check status\n./status.sh\n\n# Stop bot\n./stop.sh\n\n# View logs\ntail -f logs/zetherion_ai.log  # if logging to file\n\n# Restart bot\n./stop.sh &amp;&amp; ./start.sh\n\n# Check Qdrant health\ncurl http://localhost:6333/healthz\n\n# List Docker containers\ndocker ps -a\n\n# View bot process\nps aux | grep zetherion_ai\n</code></pre>"},{"location":"TROUBLESHOOTING/#configuration-checklist","title":"Configuration Checklist","text":"<ul> <li>[ ] Discord Token in <code>.env</code></li> <li>[ ] Gemini API Key in <code>.env</code></li> <li>[ ] Message Content Intent enabled in Discord</li> <li>[ ] Bot invited with <code>applications.commands</code> scope</li> <li>[ ] Router backend selected (<code>ROUTER_BACKEND</code> in <code>.env</code>)</li> <li>[ ] Qdrant running on correct host/port</li> <li>[ ] Ollama container running (if using Ollama backend)</li> <li>[ ] Docker Desktop has sufficient memory (check <code>docker info</code>)</li> <li>[ ] Python 3.12+ installed</li> <li>[ ] Docker Desktop running</li> <li>[ ] Correct <code>QDRANT_HOST</code> for your setup</li> <li>[ ] Correct <code>OLLAMA_HOST</code> for your setup (if using Ollama)</li> </ul>"},{"location":"TROUBLESHOOTING/#still-having-issues","title":"Still Having Issues?","text":"<ol> <li>Check the README.md for setup instructions</li> <li>Enable debug logging: <code>LOG_LEVEL=DEBUG</code> in <code>.env</code></li> <li>Search existing GitHub issues</li> <li>Create new issue with debug information</li> </ol>"},{"location":"WINDOWS_DEPLOYMENT/","title":"Windows Deployment Guide","text":"<p>Simple, fully automated Docker deployment for Zetherion AI on Windows 10/11.</p>"},{"location":"WINDOWS_DEPLOYMENT/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>One command deployment - no Python installation required:</p>"},{"location":"WINDOWS_DEPLOYMENT/#prerequisites","title":"Prerequisites","text":"<ol> <li>Administrator PowerShell (required for installation)</li> <li>Press <code>Win + X</code></li> <li> <p>Select \"Windows PowerShell (Admin)\" or \"Terminal (Admin)\"</p> </li> <li> <p>That's it! Script handles Docker and Git installation if needed.</p> </li> </ol>"},{"location":"WINDOWS_DEPLOYMENT/#installation","title":"Installation","text":"<pre><code># 1. Clone repository (or download ZIP from GitHub)\ngit clone https://github.com/jimtin/zetherion-ai.git\ncd zetherion-ai\n\n# 2. Run automated deployment\n.\\start.ps1\n</code></pre> <p>First run: ~3-9 minutes (depending on Ollama vs Gemini) Subsequent runs: ~30 seconds (containers cached)</p>"},{"location":"WINDOWS_DEPLOYMENT/#what-happens-automatically","title":"What Happens Automatically","text":""},{"location":"WINDOWS_DEPLOYMENT/#phase-1-prerequisites-check","title":"Phase 1: Prerequisites Check","text":"<ul> <li>\u2705 Checks if Docker Desktop installed</li> <li>If not found: Prompts to install via winget</li> <li>Auto-downloads and installs Docker Desktop</li> <li>\u2705 Checks if Git installed</li> <li>If not found: Prompts to install via winget</li> <li>\u2705 Checks Docker daemon running</li> <li>If not: Auto-launches Docker Desktop</li> <li>Waits up to 60 seconds for Docker to start</li> <li>\u2705 Validates disk space (warns if &lt;20GB)</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#phase-2-hardware-assessment","title":"Phase 2: Hardware Assessment","text":"<ul> <li>\u2705 Detects CPU model, core count, thread count</li> <li>\u2705 Checks system RAM and available memory</li> <li>\u2705 Detects GPU (NVIDIA, AMD, integrated)</li> <li>\u2705 Recommends optimal Ollama model for your hardware</li> <li>\u2705 Displays hardware summary</li> </ul> <p>Example output: <pre><code>System Hardware:\n  CPU: Intel Core i7-12700K (12 cores, 20 threads)\n  RAM: 32 GB total, 24 GB available\n  GPU: NVIDIA GeForce RTX 3060 (12GB)\n\nRecommended Ollama Model:\n  Model: qwen2.5:14b\n  Size: 9.0 GB download\n  Quality: High quality, comparable to cloud models\n  Speed: Fast (with GPU acceleration)\n  Reason: Powerful GPU detected, high-quality model recommended\n</code></pre></p>"},{"location":"WINDOWS_DEPLOYMENT/#phase-3-configuration-setup","title":"Phase 3: Configuration Setup","text":"<ul> <li>\u2705 Checks if <code>.env</code> exists</li> <li>\u2705 If not found: Interactive setup wizard</li> <li>Prompts for Discord Bot Token (required)</li> <li>Prompts for Gemini API Key (required)</li> <li>Prompts for Anthropic API Key (optional)</li> <li>Prompts for OpenAI API Key (optional)</li> <li>Asks router backend choice (Gemini or Ollama)</li> <li>If Ollama: Shows hardware-recommended model, allows override</li> <li>\u2705 Validates API key formats</li> <li>\u2705 Generates <code>.env</code> file</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#phase-4-docker-build-deploy","title":"Phase 4: Docker Build &amp; Deploy","text":"<ul> <li>\u2705 Builds distroless Docker images (~2 minutes)</li> <li>Bot container (~50MB runtime)</li> <li>Skills service container</li> <li>Hardware assessment container</li> <li>\u2705 Starts all services via docker-compose</li> <li>\u2705 Waits for health checks (up to 2 minutes)</li> <li>Qdrant vector database</li> <li>Skills service</li> <li>Bot container</li> <li>Ollama (if selected)</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#phase-5-model-download-if-ollama-selected","title":"Phase 5: Model Download (if Ollama selected)","text":"<ul> <li>\u2705 Checks if model already downloaded</li> <li>\u2705 If not: Downloads recommended model</li> <li><code>llama3.1:8b</code>: ~4.7GB (~5-7 minutes)</li> <li><code>qwen2.5:14b</code>: ~9.0GB (~7-10 minutes)</li> <li><code>qwen2.5:32b</code>: ~18GB (~15-20 minutes)</li> <li>\u2705 Shows download progress</li> <li>\u2705 Verifies model loaded successfully</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#phase-6-verification","title":"Phase 6: Verification","text":"<ul> <li>\u2705 Tests Qdrant connection (http://localhost:6333/healthz)</li> <li>\u2705 Tests Ollama connection (http://localhost:11434/api/tags)</li> <li>\u2705 Displays container status</li> <li>\u2705 Shows running containers with health status</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#phase-7-success","title":"Phase 7: Success","text":"<pre><code>============================================================\n  Zetherion AI is now running!\n============================================================\n\nNext Steps:\n  1. View logs:        docker-compose logs -f\n  2. Check status:     .\\status.ps1\n  3. Stop bot:         .\\stop.ps1\n\n  4. Invite bot to Discord:\n     https://discord.com/developers/applications\n\nDeployment successful!\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#management-commands","title":"Management Commands","text":""},{"location":"WINDOWS_DEPLOYMENT/#check-status","title":"Check Status","text":"<pre><code>.\\status.ps1\n</code></pre> <p>Shows: - \u2705 Qdrant health and collection count - \u2705 Ollama health and loaded models (if enabled) - \u2705 Skills service health - \u2705 Bot health and uptime - \u2705 Overall operational status - \u2705 Container summary table</p>"},{"location":"WINDOWS_DEPLOYMENT/#stop-bot","title":"Stop Bot","text":"<pre><code>.\\stop.ps1\n</code></pre> <ul> <li>Gracefully stops all containers (30-second timeout)</li> <li>Data preserved: Volumes (database, models) kept</li> <li>Quick restart possible</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#complete-cleanup","title":"Complete Cleanup","text":"<pre><code># Complete removal (prompts for confirmation)\n.\\cleanup.ps1\n\n# Keep data but remove containers\n.\\cleanup.ps1 -KeepData\n\n# Keep config but remove everything else\n.\\cleanup.ps1 -KeepConfig\n\n# Also remove old local Python artifacts\n.\\cleanup.ps1 -RemoveOldVersion\n</code></pre> <p>Cleanup options: - <code>KeepData</code>: Preserve Qdrant database and Ollama models - <code>KeepConfig</code>: Preserve <code>.env</code> file - <code>RemoveOldVersion</code>: Clean up old local Python installation (if exists)</p>"},{"location":"WINDOWS_DEPLOYMENT/#view-logs","title":"View Logs","text":"<pre><code># All services\ndocker-compose logs -f\n\n# Specific service\ndocker-compose logs -f zetherion-ai-bot\n\n# Last 50 lines\ndocker-compose logs --tail 50 zetherion-ai-bot\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#restart-services","title":"Restart Services","text":"<pre><code># Quick restart (no rebuild)\ndocker-compose restart\n\n# Full restart with rebuild\n.\\stop.ps1\n.\\start.ps1 --force-rebuild\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"WINDOWS_DEPLOYMENT/#minimum-gemini-backend","title":"Minimum (Gemini Backend)","text":"<ul> <li>Windows 10/11 (64-bit)</li> <li>8GB RAM</li> <li>20GB free disk space</li> <li>Any modern CPU (2+ cores)</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#recommended-ollama-backend","title":"Recommended (Ollama Backend)","text":"<ul> <li>Windows 11 (64-bit)</li> <li>16GB RAM (32GB for larger models)</li> <li>30GB free SSD space</li> <li>8+ core CPU or NVIDIA GPU</li> </ul> <p>See Hardware Recommendations for detailed specs.</p>"},{"location":"WINDOWS_DEPLOYMENT/#configuration","title":"Configuration","text":""},{"location":"WINDOWS_DEPLOYMENT/#edit-configuration","title":"Edit Configuration","text":"<pre><code># Open .env in Notepad\nnotepad .env\n\n# Or use your preferred editor\ncode .env  # VS Code\n</code></pre> <p>Required Settings: <pre><code>DISCORD_TOKEN=your_discord_token_here\nGEMINI_API_KEY=your_gemini_api_key_here\n</code></pre></p> <p>Router Backend: <pre><code># Cloud-based (default, fastest setup)\nROUTER_BACKEND=gemini\n\n# Local AI (privacy-focused)\nROUTER_BACKEND=ollama\nOLLAMA_ROUTER_MODEL=llama3.1:8b\n</code></pre></p> <p>Optional Settings: <pre><code>ANTHROPIC_API_KEY=your_anthropic_key  # For Claude\nOPENAI_API_KEY=your_openai_key        # For GPT-4\nALLOWED_USER_IDS=123456789,987654321  # User allowlist\nRATE_LIMIT_MESSAGES=10                 # Messages per minute\n</code></pre></p> <p>See Configuration Guide for complete reference.</p>"},{"location":"WINDOWS_DEPLOYMENT/#restart-after-config-changes","title":"Restart After Config Changes","text":"<pre><code>.\\stop.ps1\n.\\start.ps1\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":""},{"location":"WINDOWS_DEPLOYMENT/#docker-desktop-wont-start","title":"Docker Desktop Won't Start","text":"<p>Issue: Docker daemon not responding after 60 seconds</p> <p>Solutions: 1. Check WSL 2 is installed:    <pre><code>wsl --status\n</code></pre> 2. Update WSL:    <pre><code>wsl --update\n</code></pre> 3. Restart computer and try again</p>"},{"location":"WINDOWS_DEPLOYMENT/#script-requires-administrator","title":"Script Requires Administrator","text":"<p>Issue: \"This script requires Administrator privileges\"</p> <p>Solution: Right-click PowerShell \u2192 \"Run as Administrator\"</p>"},{"location":"WINDOWS_DEPLOYMENT/#execution-policy-error","title":"Execution Policy Error","text":"<p>Issue: \"Cannot be loaded because running scripts is disabled\"</p> <p>Solution: <pre><code># Allow scripts for current session\nSet-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process\n\n# Then run start.ps1\n.\\start.ps1\n</code></pre></p>"},{"location":"WINDOWS_DEPLOYMENT/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Issue: Container crashes with \"Out of memory\"</p> <p>Solutions: 1. Open Docker Desktop 2. Settings \u2192 Resources \u2192 Memory 3. Increase memory allocation:    - Gemini: 4GB minimum    - Ollama (<code>llama3.1:8b</code>): 8GB    - Ollama (<code>qwen2.5:14b</code>): 12GB 4. Click \"Apply &amp; Restart\"</p>"},{"location":"WINDOWS_DEPLOYMENT/#model-download-fails","title":"Model Download Fails","text":"<p>Issue: Ollama model download times out or fails</p> <p>Solutions: 1. Check internet connection 2. Verify disk space:    <pre><code>Get-PSDrive C | Select-Object Used,Free\n</code></pre> 3. Retry download manually:    <pre><code>docker exec zetherion-ai-ollama ollama pull llama3.1:8b\n</code></pre></p>"},{"location":"WINDOWS_DEPLOYMENT/#port-already-in-use","title":"Port Already in Use","text":"<p>Issue: \"port is already allocated\"</p> <p>Solutions: 1. Find process using port:    <pre><code>netstat -ano | findstr :6333\nnetstat -ano | findstr :8080\nnetstat -ano | findstr :11434\n</code></pre> 2. Stop conflicting process or change ports in <code>docker-compose.yml</code></p>"},{"location":"WINDOWS_DEPLOYMENT/#container-health-check-failing","title":"Container Health Check Failing","text":"<p>Issue: Services don't become healthy within 2 minutes</p> <p>Solutions: 1. Check Docker resource allocation (see \"Out of Memory\" above) 2. View container logs:    <pre><code>docker-compose logs zetherion-ai-bot\n</code></pre> 3. Check for errors in logs 4. Try clean rebuild:    <pre><code>.\\cleanup.ps1\n.\\start.ps1 --force-rebuild\n</code></pre></p>"},{"location":"WINDOWS_DEPLOYMENT/#updating-zetherion-ai","title":"Updating Zetherion AI","text":""},{"location":"WINDOWS_DEPLOYMENT/#update-to-latest-version","title":"Update to Latest Version","text":"<pre><code># Stop bot\n.\\stop.ps1\n\n# Pull latest code\ngit pull origin main\n\n# Rebuild and restart\n.\\start.ps1 --force-rebuild\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#update-specific-components","title":"Update Specific Components","text":"<p>Update Docker images only: <pre><code>docker-compose pull\ndocker-compose up -d\n</code></pre></p> <p>Update Ollama model: <pre><code>docker exec zetherion-ai-ollama ollama pull llama3.1:8b\n</code></pre></p>"},{"location":"WINDOWS_DEPLOYMENT/#security-best-practices","title":"Security Best Practices","text":""},{"location":"WINDOWS_DEPLOYMENT/#secure-env-file","title":"Secure .env File","text":"<pre><code># Set file permissions (owner only)\nicacls .env /inheritance:r /grant:r \"$env:USERNAME:(R,W)\"\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#enable-user-allowlist","title":"Enable User Allowlist","text":"<pre><code># In .env file\nALLOWED_USER_IDS=your_discord_id_here\n</code></pre> <p>Get your Discord ID: 1. Enable Developer Mode (Discord Settings \u2192 Advanced) 2. Right-click your username \u2192 Copy User ID</p>"},{"location":"WINDOWS_DEPLOYMENT/#rotate-api-keys","title":"Rotate API Keys","text":"<ul> <li>Discord: Every 6 months</li> <li>AI Providers: Every 3 months</li> <li>Encryption: Every 6-12 months</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#enable-encryption","title":"Enable Encryption","text":"<pre><code>ENCRYPTION_ENABLED=true\nENCRYPTION_PASSPHRASE=&lt;strong-random-passphrase&gt;\n</code></pre> <p>Generate passphrase: <pre><code># Generate secure random passphrase\n-join ((65..90) + (97..122) + (48..57) | Get-Random -Count 32 | % {[char]$_})\n</code></pre></p> <p>See Security Guide for comprehensive security documentation.</p>"},{"location":"WINDOWS_DEPLOYMENT/#performance-optimization","title":"Performance Optimization","text":""},{"location":"WINDOWS_DEPLOYMENT/#for-faster-startup","title":"For Faster Startup","text":"<pre><code>ROUTER_BACKEND=gemini  # Cloud-based, no model download\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#for-better-quality","title":"For Better Quality","text":"<pre><code>ROUTER_BACKEND=ollama\nOLLAMA_ROUTER_MODEL=qwen2.5:14b  # Requires 16GB+ RAM\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#for-lower-costs","title":"For Lower Costs","text":"<pre><code>ROUTER_BACKEND=gemini  # Free tier (1,500 requests/day)\nANTHROPIC_API_KEY=     # Leave empty (use Gemini for all queries)\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#gpu-acceleration","title":"GPU Acceleration","text":"<ul> <li>Install NVIDIA Container Toolkit</li> <li>Ollama automatically detects and uses GPU</li> <li>5-10x faster inference vs CPU</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#advanced-features","title":"Advanced Features","text":""},{"location":"WINDOWS_DEPLOYMENT/#custom-model-selection","title":"Custom Model Selection","text":"<pre><code># List available models\ndocker exec zetherion-ai-ollama ollama list\n\n# Pull specific model\ndocker exec zetherion-ai-ollama ollama pull mistral:7b\n\n# Update .env\nOLLAMA_ROUTER_MODEL=mistral:7b\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#remote-qdrant-instance","title":"Remote Qdrant Instance","text":"<pre><code>QDRANT_HOST=qdrant.example.com\nQDRANT_PORT=6333\nQDRANT_USE_TLS=true\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#custom-docker-compose","title":"Custom Docker Compose","text":"<pre><code># Use custom compose file\ndocker-compose -f docker-compose.yml -f docker-compose.custom.yml up -d\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#monitoring","title":"Monitoring","text":""},{"location":"WINDOWS_DEPLOYMENT/#resource-usage","title":"Resource Usage","text":"<pre><code># View Docker resource usage\ndocker stats\n\n# Specific container\ndocker stats zetherion-ai-bot\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#log-monitoring","title":"Log Monitoring","text":"<pre><code># Continuous log monitoring\ndocker-compose logs -f --tail 100\n</code></pre>"},{"location":"WINDOWS_DEPLOYMENT/#health-endpoints","title":"Health Endpoints","text":"<ul> <li>Qdrant: http://localhost:6333/dashboard</li> <li>Ollama: http://localhost:11434/api/tags</li> </ul>"},{"location":"WINDOWS_DEPLOYMENT/#getting-help","title":"Getting Help","text":"<p>Before asking for help: 1. Check Troubleshooting section 2. View logs: <code>docker-compose logs</code> 3. Check status: <code>.\\status.ps1</code></p> <p>Where to get help: - Installation Guide - Detailed setup instructions - Configuration Guide - All environment variables - Hardware Guide - Optimize for your system - GitHub Issues - Bug reports - GitHub Discussions - Questions</p> <p>When reporting issues, include: - Windows version (<code>winver</code>) - Docker Desktop version (<code>docker --version</code>) - Output of <code>.\\status.ps1</code> - Relevant error messages from <code>docker-compose logs</code></p>"},{"location":"WINDOWS_DEPLOYMENT/#additional-resources","title":"Additional Resources","text":"<ul> <li>Security Guide - Distroless containers and encryption</li> <li>Testing Guide - Running tests and CI/CD</li> <li>Architecture - System architecture overview</li> <li>FAQ - Frequently asked questions</li> </ul> <p>Last Updated: 2026-02-07 Version: 3.0.0 (Fully Automated Docker Deployment)</p>"}]}