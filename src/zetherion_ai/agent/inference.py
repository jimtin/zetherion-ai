"""InferenceBroker - Smart multi-provider LLM dispatch.

Central class through which ALL LLM calls flow, enabling smart provider
selection based on task type, provider capabilities, and availability.
"""

from __future__ import annotations

import asyncio
import time
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

import anthropic
import httpx
import openai
from google import genai  # type: ignore[attr-defined]

from zetherion_ai.agent.providers import (
    Provider,
    TaskType,
    get_ollama_tier,
    get_provider_for_task,
)
from zetherion_ai.config import get_settings
from zetherion_ai.logging import get_logger
from zetherion_ai.models.pricing import get_cost

if TYPE_CHECKING:
    from zetherion_ai.costs.tracker import CostTracker as PersistentCostTracker
    from zetherion_ai.models.registry import ModelRegistry

log = get_logger("zetherion_ai.agent.inference")


@dataclass
class InferenceResult:
    """Result of an inference call."""

    content: str
    provider: Provider
    task_type: TaskType
    model: str
    input_tokens: int = 0
    output_tokens: int = 0
    latency_ms: float = 0.0
    estimated_cost_usd: float = 0.0


@dataclass
class ProviderHealth:
    """Health status of a provider."""

    available: bool
    last_check: float = 0.0
    error_message: str = ""


@dataclass
class CostTracker:
    """Tracks costs per provider."""

    total_calls: int = 0
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost_usd: float = 0.0
    by_task_type: dict[str, float] = field(default_factory=dict)


# Approximate cost per 1M tokens (input/output) for each provider
# These are estimates and should be updated periodically
COST_PER_MILLION_TOKENS: dict[Provider, tuple[float, float]] = {
    Provider.CLAUDE: (3.0, 15.0),  # Claude Sonnet 4.5
    Provider.OPENAI: (2.5, 10.0),  # GPT-4o
    Provider.GEMINI: (0.075, 0.30),  # Gemini Flash
    Provider.OLLAMA: (0.0, 0.0),  # Free (local)
}


class InferenceBroker:
    """Central LLM dispatch with smart provider selection.

    All LLM calls in Zetherion AI flow through this broker, which:
    - Selects the optimal provider for each task type
    - Handles fallback when providers are unavailable
    - Tracks costs and usage per provider
    - Prefers Ollama where possible to reduce API costs
    """

    def __init__(
        self,
        model_registry: ModelRegistry | None = None,
        cost_tracker: PersistentCostTracker | None = None,
    ) -> None:
        """Initialize the inference broker.

        Args:
            model_registry: Optional ModelRegistry for dynamic model resolution.
            cost_tracker: Optional CostTracker for persistent cost storage.
        """
        settings = get_settings()

        # Optional integrations (Phase 5B.1)
        self._model_registry = model_registry
        self._persistent_cost_tracker = cost_tracker

        # Provider availability
        self._available_providers: set[Provider] = set()
        self._provider_health: dict[Provider, ProviderHealth] = {}

        # Cost tracking
        self._cost_tracker: dict[Provider, CostTracker] = {p: CostTracker() for p in Provider}

        # Ollama configuration (uses generation container, not router container)
        self._ollama_model = settings.ollama_generation_model
        self._ollama_url = settings.ollama_url  # Generation container URL
        self._ollama_tier = get_ollama_tier(self._ollama_model)

        # Initialize provider clients
        self._claude_client: anthropic.AsyncAnthropic | None = None
        self._openai_client: openai.AsyncOpenAI | None = None
        self._gemini_client: genai.Client | None = None

        # Claude
        if settings.anthropic_api_key:
            self._claude_client = anthropic.AsyncAnthropic(
                api_key=settings.anthropic_api_key.get_secret_value()
            )
            self._claude_model = settings.claude_model
            self._available_providers.add(Provider.CLAUDE)

        # OpenAI
        if settings.openai_api_key:
            self._openai_client = openai.AsyncOpenAI(
                api_key=settings.openai_api_key.get_secret_value()
            )
            self._openai_model = settings.openai_model
            self._available_providers.add(Provider.OPENAI)

        # Gemini (always available if we have the API key)
        if settings.gemini_api_key:
            self._gemini_client = genai.Client(api_key=settings.gemini_api_key.get_secret_value())
            self._gemini_model = settings.router_model
            self._available_providers.add(Provider.GEMINI)

        # Ollama (check availability asynchronously later)
        # For now, assume available - health check will update this
        self._available_providers.add(Provider.OLLAMA)

        log.info(
            "inference_broker_initialized",
            available_providers=[p.value for p in self._available_providers],
            ollama_model=self._ollama_model,
            ollama_tier=self._ollama_tier.value,
        )

    async def infer(
        self,
        prompt: str,
        task_type: TaskType,
        system_prompt: str | None = None,
        messages: list[dict[str, str]] | None = None,
        max_tokens: int = 2048,
        temperature: float = 0.7,
    ) -> InferenceResult:
        """Make an inference call with smart provider selection.

        Args:
            prompt: The user's prompt/message.
            task_type: Type of task for provider selection.
            system_prompt: Optional system prompt.
            messages: Optional conversation history.
            max_tokens: Maximum tokens to generate.
            temperature: Sampling temperature.

        Returns:
            InferenceResult with the response and metadata.
        """
        start_time = time.time()

        # Get the optimal provider for this task
        provider = get_provider_for_task(
            task_type=task_type,
            ollama_model=self._ollama_model,
            available_providers=self._available_providers,
        )

        log.debug(
            "provider_selected",
            task_type=task_type.value,
            provider=provider.value,
        )

        # Make the inference call
        try:
            result = await self._call_provider(
                provider=provider,
                prompt=prompt,
                task_type=task_type,
                system_prompt=system_prompt,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
        except Exception as e:
            log.warning(
                "provider_call_failed",
                provider=provider.value,
                error=str(e),
            )
            # Try fallbacks
            result = await self._try_fallbacks(
                task_type=task_type,
                prompt=prompt,
                system_prompt=system_prompt,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                failed_provider=provider,
            )

        # Calculate latency
        result.latency_ms = (time.time() - start_time) * 1000

        # Estimate cost using pricing module
        cost_usd, cost_estimated = self._estimate_cost(
            provider=result.provider,
            model=result.model,
            input_tokens=result.input_tokens,
            output_tokens=result.output_tokens,
        )
        result.estimated_cost_usd = cost_usd

        # Track costs
        self._track_cost(result, cost_estimated=cost_estimated)

        log.info(
            "inference_complete",
            task_type=task_type.value,
            provider=result.provider.value,
            model=result.model,
            latency_ms=round(result.latency_ms, 2),
            input_tokens=result.input_tokens,
            output_tokens=result.output_tokens,
            cost_usd=round(result.estimated_cost_usd, 6),
        )

        return result

    async def _call_provider(
        self,
        provider: Provider,
        prompt: str,
        task_type: TaskType,
        system_prompt: str | None,
        messages: list[dict[str, str]] | None,
        max_tokens: int,
        temperature: float,
    ) -> InferenceResult:
        """Call a specific provider.

        Args:
            provider: The provider to call.
            prompt: The user's prompt.
            task_type: Task type for metadata.
            system_prompt: Optional system prompt.
            messages: Optional conversation history.
            max_tokens: Maximum tokens.
            temperature: Sampling temperature.

        Returns:
            InferenceResult from the provider.
        """
        match provider:
            case Provider.CLAUDE:
                return await self._call_claude(
                    prompt, task_type, system_prompt, messages, max_tokens, temperature
                )
            case Provider.OPENAI:
                return await self._call_openai(
                    prompt, task_type, system_prompt, messages, max_tokens, temperature
                )
            case Provider.GEMINI:
                return await self._call_gemini(
                    prompt, task_type, system_prompt, messages, max_tokens, temperature
                )
            case Provider.OLLAMA:
                return await self._call_ollama(
                    prompt, task_type, system_prompt, messages, max_tokens, temperature
                )
            case _:
                raise ValueError(f"Unknown provider: {provider}")

    async def _call_claude(
        self,
        prompt: str,
        task_type: TaskType,
        system_prompt: str | None,
        messages: list[dict[str, str]] | None,
        max_tokens: int,
        temperature: float,
    ) -> InferenceResult:
        """Call Claude API."""
        if not self._claude_client:
            raise RuntimeError("Claude client not initialized")

        # Build messages
        api_messages: list[dict[str, Any]] = []
        if messages:
            for msg in messages:
                api_messages.append({"role": msg["role"], "content": msg["content"]})
        api_messages.append({"role": "user", "content": prompt})

        response = await self._claude_client.messages.create(
            model=self._claude_model,
            max_tokens=max_tokens,
            system=system_prompt or "",
            messages=api_messages,  # type: ignore[arg-type]
        )

        return InferenceResult(
            content=response.content[0].text,  # type: ignore[union-attr]
            provider=Provider.CLAUDE,
            task_type=task_type,
            model=self._claude_model,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
        )

    async def _call_openai(
        self,
        prompt: str,
        task_type: TaskType,
        system_prompt: str | None,
        messages: list[dict[str, str]] | None,
        max_tokens: int,
        temperature: float,
    ) -> InferenceResult:
        """Call OpenAI API."""
        if not self._openai_client:
            raise RuntimeError("OpenAI client not initialized")

        # Build messages
        api_messages: list[dict[str, Any]] = []
        if system_prompt:
            api_messages.append({"role": "system", "content": system_prompt})
        if messages:
            for msg in messages:
                api_messages.append({"role": msg["role"], "content": msg["content"]})
        api_messages.append({"role": "user", "content": prompt})

        response = await self._openai_client.chat.completions.create(
            model=self._openai_model,
            messages=api_messages,  # type: ignore[arg-type]
            max_tokens=max_tokens,
            temperature=temperature,
        )

        return InferenceResult(
            content=response.choices[0].message.content or "",
            provider=Provider.OPENAI,
            task_type=task_type,
            model=self._openai_model,
            input_tokens=response.usage.prompt_tokens if response.usage else 0,
            output_tokens=response.usage.completion_tokens if response.usage else 0,
        )

    async def _call_gemini(
        self,
        prompt: str,
        task_type: TaskType,
        system_prompt: str | None,
        messages: list[dict[str, str]] | None,
        max_tokens: int,
        temperature: float,
    ) -> InferenceResult:
        """Call Gemini API."""
        if not self._gemini_client:
            raise RuntimeError("Gemini client not initialized")

        # Build content
        content = prompt
        if system_prompt:
            content = f"{system_prompt}\n\n{prompt}"

        # Wrap synchronous Gemini call in thread to avoid blocking event loop
        def _sync_generate() -> Any:
            return self._gemini_client.models.generate_content(  # type: ignore[union-attr]
                model=self._gemini_model,
                contents=content,
                config={
                    "temperature": temperature,
                    "max_output_tokens": max_tokens,
                },
            )

        response = await asyncio.to_thread(_sync_generate)

        # Gemini doesn't provide token counts in the same way
        return InferenceResult(
            content=response.text or "",
            provider=Provider.GEMINI,
            task_type=task_type,
            model=self._gemini_model,
            input_tokens=len(content.split()) * 2,  # Rough estimate
            output_tokens=len((response.text or "").split()) * 2,
        )

    async def _call_ollama(
        self,
        prompt: str,
        task_type: TaskType,
        system_prompt: str | None,
        messages: list[dict[str, str]] | None,
        max_tokens: int,
        temperature: float,
    ) -> InferenceResult:
        """Call Ollama API."""
        settings = get_settings()

        # Build messages for chat endpoint
        api_messages: list[dict[str, str]] = []
        if system_prompt:
            api_messages.append({"role": "system", "content": system_prompt})
        if messages:
            api_messages.extend(messages)
        api_messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient(timeout=settings.ollama_timeout) as client:
            response = await client.post(
                f"{self._ollama_url}/api/chat",
                json={
                    "model": self._ollama_model,
                    "messages": api_messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                    },
                },
            )
            response.raise_for_status()
            data = response.json()

        content = data.get("message", {}).get("content", "")

        return InferenceResult(
            content=content,
            provider=Provider.OLLAMA,
            task_type=task_type,
            model=self._ollama_model,
            input_tokens=data.get("prompt_eval_count", 0),
            output_tokens=data.get("eval_count", 0),
        )

    async def _try_fallbacks(
        self,
        task_type: TaskType,
        prompt: str,
        system_prompt: str | None,
        messages: list[dict[str, str]] | None,
        max_tokens: int,
        temperature: float,
        failed_provider: Provider,
    ) -> InferenceResult:
        """Try fallback providers when the primary fails."""
        # Get remaining available providers
        remaining = self._available_providers - {failed_provider}

        for fallback in [Provider.CLAUDE, Provider.OPENAI, Provider.GEMINI, Provider.OLLAMA]:
            if fallback in remaining:
                try:
                    log.info("trying_fallback", provider=fallback.value)
                    return await self._call_provider(
                        provider=fallback,
                        prompt=prompt,
                        task_type=task_type,
                        system_prompt=system_prompt,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                    )
                except Exception as e:
                    log.warning("fallback_failed", provider=fallback.value, error=str(e))
                    remaining.remove(fallback)

        # All providers failed
        raise RuntimeError("All providers failed")

    def _estimate_cost(
        self,
        provider: Provider,
        model: str,
        input_tokens: int,
        output_tokens: int,
    ) -> tuple[float, bool]:
        """Estimate the cost of an API call.

        Uses the pricing module for accurate cost calculation with fallback.

        Returns:
            Tuple of (cost_usd, is_estimated).
        """
        cost_result = get_cost(model, input_tokens, output_tokens)
        return cost_result.cost_usd, cost_result.estimated

    def _track_cost(self, result: InferenceResult, cost_estimated: bool = False) -> None:
        """Track costs for a completed inference.

        Args:
            result: The inference result to track.
            cost_estimated: Whether the cost was estimated (pricing unknown).
        """
        # In-memory tracking
        tracker = self._cost_tracker[result.provider]
        tracker.total_calls += 1
        tracker.total_input_tokens += result.input_tokens
        tracker.total_output_tokens += result.output_tokens
        tracker.total_cost_usd += result.estimated_cost_usd

        task_key = result.task_type.value
        tracker.by_task_type[task_key] = (
            tracker.by_task_type.get(task_key, 0.0) + result.estimated_cost_usd
        )

        # Persistent tracking via CostTracker (Phase 5B.1)
        if self._persistent_cost_tracker:
            self._persistent_cost_tracker.record(
                provider=result.provider.value,
                model=result.model,
                tokens_input=result.input_tokens,
                tokens_output=result.output_tokens,
                cost_usd=result.estimated_cost_usd,
                cost_estimated=cost_estimated,
                task_type=task_key,
                latency_ms=int(result.latency_ms) if result.latency_ms else None,
            )

    def get_cost_summary(self) -> dict[str, Any]:
        """Get a summary of costs by provider.

        Returns:
            Dictionary with cost breakdown per provider.
        """
        summary: dict[str, Any] = {}
        total_cost = 0.0

        for provider, tracker in self._cost_tracker.items():
            if tracker.total_calls > 0:
                summary[provider.value] = {
                    "calls": tracker.total_calls,
                    "input_tokens": tracker.total_input_tokens,
                    "output_tokens": tracker.total_output_tokens,
                    "cost_usd": round(tracker.total_cost_usd, 4),
                    "by_task_type": {k: round(v, 6) for k, v in tracker.by_task_type.items()},
                }
                total_cost += tracker.total_cost_usd

        summary["total_cost_usd"] = round(total_cost, 4)
        return summary

    async def health_check(self, provider: Provider) -> bool:
        """Check if a provider is healthy.

        Args:
            provider: The provider to check.

        Returns:
            True if healthy, False otherwise.
        """
        try:
            if provider == Provider.OLLAMA:
                async with httpx.AsyncClient(timeout=5) as client:
                    response = await client.get(f"{self._ollama_url}/api/tags")
                    return response.status_code == 200
            elif provider == Provider.GEMINI and self._gemini_client:
                # Wrap synchronous Gemini call to avoid blocking
                def _sync_health_check() -> bool:
                    self._gemini_client.models.generate_content(  # type: ignore[union-attr]
                        model=self._gemini_model,
                        contents="test",
                        config={"max_output_tokens": 5},
                    )
                    return True

                return await asyncio.to_thread(_sync_health_check)
            elif provider == Provider.CLAUDE and self._claude_client:
                await self._claude_client.messages.create(
                    model=self._claude_model,
                    max_tokens=5,
                    messages=[{"role": "user", "content": "test"}],
                )
                return True
            elif provider == Provider.OPENAI and self._openai_client:
                await self._openai_client.chat.completions.create(
                    model=self._openai_model,
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=5,
                )
                return True
        except Exception as e:
            log.warning("health_check_failed", provider=provider.value, error=str(e))
        return False

    @property
    def available_providers(self) -> set[Provider]:
        """Get the set of available providers."""
        return self._available_providers.copy()
